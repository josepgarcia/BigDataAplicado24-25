{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Big Data Aplicado curso: 24-25","text":"<p> UD00: Conceptos generales 1. ConceptosGenerales 2. bash 3. EntornosVirtualesPython 4. Bash Scripting 5. Bash Varios 6. Bash Ejercicios 7. Bash Chuleta 8. Entornos Virtuales Python 9. \u00bfC\u00f3mo modificar los apuntes?</p> <p>UD01: Hadoop 1. Instalaci\u00f3n Ubuntu Server 2. Descarga y configuraci\u00f3n de Hadoop 3. Ejercicios</p> <p>UD02: Cluster Hadoop pseudodistribuido. HDFS. 1. Instalaci\u00f3n 2. Comandos HDFS 3. Ejercicios HDFS 4. Administraci\u00f3n HDFS 5. Snapshots 6. HDFS desde Python 7. Cluster con docker</p> <p>UD03: Mapreduce. Yarn 1. 0.index 2. Configurar cluster YARN 3. Fases mapreduce 4. Contar palabras 5. Ejercicio ventas 6. Ejercicio temperatura -AULES-</p> <p>UD04: Sqoop. Hive.</p> <p>UD09: Extra 1. Cluster hadoop varios nodos</p>"},{"location":"_TODO/","title":"TODO","text":""},{"location":"_TODO/#spark","title":"SPARK","text":"<p>Spark SQL and Spark 3 using Scala Hands-On with Labs https://www.udemy.com/course/cca-175-spark-and-hadoop-developer-certification-scala/?couponCode=NEWYEARCAREER</p>"},{"location":"_TODO/#otras-herramientas","title":"Otras herramientas","text":"<p>HBASE https://www.udemy.com/course/big-data-hadoop-developer-course-with-handson/?couponCode=NEWYEARCAREER</p>"},{"location":"_TODO/#proyectos","title":"Proyectos","text":"<p>An\u00e1lisis tweets https://www.researchgate.net/publication/338198026_Twitter_data_analysis_using_Hadoop_ecosystems_and_apache_zeppelin</p>"},{"location":"_TODO/#no","title":"NO","text":"<p>Tot en JAVA https://www.udemy.com/course/hadoopinrealworld/</p>"},{"location":"_TODO/#no-incluits","title":"No incluits","text":"<p>https://www.udemy.com/course/projects-in-hadoop-and-big-data-learn-by-building-apps/?couponCode=NEWYEARCAREER</p> <p>https://www.udemy.com/course/learn-big-data-the-hadoop-ecosystem-masterclass/?couponCode=NEWYEARCAREER</p>"},{"location":"formato/","title":"Formato","text":"<p>\u270f\ufe0f EJERCICIOS</p> Soluci\u00f3n <p>$ hdfs dfsadmin -report       \"Under replicated blocks\" -&gt; Si configuramos como r\u00e9plica por ejemplo 3 y hay alg\u00fan bloque con menos. Si esto baja de un determinado porcentaje, hadoop se pone en modo seguro porque piensa que hay alg\u00fan problema.   $ hdfs fsck /   $ hdfs fsck /datos2   $ hdfs dfsadmin -printTopology   $ hdfs dfsadmin -listOpenFiles</p>"},{"location":"formato/#test","title":"Test","text":"<p>Ejercicios</p> <p>agregar:</p> <ul> <li>admonition</li> <li>pymdownx.details</li> </ul> <p>First, create a directory in your <code>docs</code> directory to hold the event pages:</p> <pre><code>$ mkdir docs/events\n</code></pre> <p>Then, add a file <code>.meta.yml</code> inside this new directory with settings for the page icon and a hot pink background color that will stand out on social media. Note that you can override the background image by setting it to <code>null</code> here since it is not visible anyway because of the opaque coloring.</p> <pre><code>---\nicon: material/calendar-plus\nsocial:\n  cards_layout_options:\n    background_image: null\n    background_color: \"#ff1493\"\n---\n</code></pre> <p>Now add a page within the <code>docs/events</code> directoy. It does not need to have any special content, just a top level header.</p> <p>To turn on the <code>default/variant</code> layout in <code>mkdocs.yml</code>, add the <code>cards_layout</code> option and also add the meta plugin:</p> <pre><code>plugins:\n  - meta\n  - social:\n      cards_layout: default/variant\n</code></pre> <p>After running <code>mkdocs build</code>, you can see that the social card at <code>site/assets/images/social/events/index.png</code> features the page icon.</p> <p>Title</p> <p>Contents</p>"},{"location":"formato/#admonition","title":"Admonition","text":"<p>Title -&gt; Para ejercicios</p> <p>Contents</p> <p>Title -&gt; PENDIENTE</p> <p>Contents</p> <p>Title -&gt; Para ejemplos</p> <p>Contents</p> <p>Title -&gt; Frases</p> <p>Contents</p> <p>Title -&gt; Para errores</p> <p>Contents</p> <p>Title -&gt; Info APP</p> <p>Contents</p> <p>Title</p> <p>Contents</p> <p>Title -&gt; Algo m\u00e1s de informaci\u00f3n</p> <p>Contents</p> <p>Title</p> <p>Contents</p> <p>Title</p> <p>Contents</p> <p>Title</p> <p>Contents</p> <p>Title</p> <p>Contents</p> <p>Title</p> <p>Contents</p>"},{"location":"SLIDES/UD03/","title":"Slide 1","text":""},{"location":"SLIDES/UD03/#slide-11","title":"Slide 1.1","text":"<p>A paragraph with some text and a link.</p>"},{"location":"SLIDES/UD03/#slide-2","title":"Slide 2","text":""},{"location":"SLIDES/UD03/#slide-21","title":"Slide 2.1","text":"<p>Sed vitae pellentesque ipsum. Pellentesque ante sem, bibendum non lacus in, facilisis dignissim neque. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec nec risus ligula. Vivamus dignissim et mauris pulvinar blandit.</p>"},{"location":"SLIDES/UD03/#slide-3","title":"Slide 3","text":""},{"location":"SLIDES/UD03/#slide-31","title":"Slide 3.1","text":"<p>Sed vitae pellentesque ipsum. Pellentesque ante sem, bibendum non lacus in, facilisis dignissim neque. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec nec risus ligula. Vivamus dignissim et mauris pulvinar blandit.</p>"},{"location":"UD00/1.ConceptosGenerales/","title":"Conceptos Generales","text":""},{"location":"UD00/1.ConceptosGenerales/#imprescindibles","title":"Imprescindibles","text":"<ul> <li> <p>C\u00f3mo utilizar Virtualbox</p> </li> <li> <p>Terminal Linux</p> </li> </ul>"},{"location":"UD00/1.ConceptosGenerales/#discos-duros","title":"Discos Duros","text":"<p>Tipos de discos duros</p> <ul> <li>HDD (Hard Drive Disk): Mec\u00e1nicos</li> <li>Unidad de estado s\u00f3lido o SSD<ul> <li>Conexi\u00f3n SATA o PCIe</li> <li>Conexi\u00f3n NVME m.2</li> </ul> </li> </ul> <p> </p>"},{"location":"UD00/1.ConceptosGenerales/#particiones","title":"Particiones","text":"<p>Una partici\u00f3n</p> <p></p> <p>Varias particiones</p> <p></p> <p>Tipos de particiones  Independientemente del sistema de archivos de una partici\u00f3n (FAT, NTFS, ext3, ext4, etc.), si se habla de un disco duro que use MBR, existen 3 tipos diferentes de particiones:</p> <p>Partici\u00f3n primaria. Son las divisiones primarias del disco. En un disco duro, pueden existir de una a cuatro particiones primarias o hasta tres primarias y una extendida. Depende de una tabla de particiones. Un disco duro f\u00edsico completamente formateado (por ejemplo, una unidad de disco duro externa USB nueva) consiste, en realidad, en una partici\u00f3n primaria que ocupa todo el espacio del disco y posee un sistema de archivos. Pr\u00e1cticamente, cualquier sistema operativo puede detectar este tipo de particiones primarias, y asignarles una unidad, siempre y cuando el sistema operativo reconozca su formato (sistema de archivos).</p> <p>Partici\u00f3n extendida. Tambi\u00e9n conocida como partici\u00f3n secundaria, es otro tipo de partici\u00f3n que act\u00faa como una partici\u00f3n primaria; sirve para contener m\u00faltiples unidades l\u00f3gicas en su interior. Fue ideada para romper la limitaci\u00f3n de 4 particiones primarias en un solo disco f\u00edsico. Solo puede existir una partici\u00f3n de este tipo por disco, y solo sirve para contener particiones l\u00f3gicas. Por lo tanto, es el \u00fanico tipo de partici\u00f3n que no soporta un sistema de archivos directamente.</p> <p>Partici\u00f3n l\u00f3gica. Ocupa una porci\u00f3n de la partici\u00f3n extendida o la totalidad de la misma, y se ha formateado con un tipo espec\u00edfico de sistema de archivos (FAT32, NTFS, ext3, ext4, etc.) y se le ha asignado una unidad, as\u00ed el sistema operativo reconoce las particiones l\u00f3gicas o su sistema de archivos. Se pueden tener un m\u00e1ximo de 23 particiones l\u00f3gicas en una partici\u00f3n extendida. Aunque algunos sistemas operativos pueden ser m\u00e1s restrictivos, como Linux que impone un m\u00e1ximo de 15, incluyendo las 4 primarias, en discos SCSI y en discos IDE 8963.</p> <p>El n\u00famero de particiones que puede crear en un disco b\u00e1sico depende del estilo de partici\u00f3n del disco:</p> <ul> <li>En los discos con registro de inicio maestro (MBR) se pueden crear hasta cuatro particiones primarias por disco o bien se pueden crear hasta tres particiones primarias y una partici\u00f3n extendida. Dentro de la partici\u00f3n extendida se pueden crear un n\u00famero ilimitado de unidades l\u00f3gicas.</li> <li>En los discos con tabla de particiones GUID (GPT) se pueden crear hasta 128 particiones primarias. Con GPT no existe la limitaci\u00f3n a cuatro particiones primarias por lo que no es necesario crear particiones extendidas ni unidades l\u00f3gicas.</li> </ul>"},{"location":"UD00/1.ConceptosGenerales/#programas-para-realizar-particiones","title":"Programas para realizar particiones","text":"<p>Gparted</p> <p></p> <p>cfdisk (consola) o fdisk</p> <p></p> <p>Gestor de disco Windows</p> <p></p> <p>Utilidad de discos MacOS</p> <p></p>"},{"location":"UD00/1.ConceptosGenerales/#sistemas-de-archivos","title":"Sistemas de archivos","text":"<p>Un sistema de archivos indexa toda la informaci\u00f3n de los datos en un dispositivo de almacenamiento, incluyendo el tama\u00f1o del Archivo, los atributos, la ubicaci\u00f3n y la jerarqu\u00eda en el directorio. El sistema de archivos tambi\u00e9n especifica la ruta a un archivo mediante la estructura de directorios con un formato.</p> <ul> <li>Sistema de archivos de Windows\u00a0- FAT, NTFS, exFAT</li> <li>macOS\u00a0- HFS, APFS, HFS+</li> <li>Linux -\u00a0EXT2/3/4, XFS, JFS, Btrfs</li> </ul> <p>Comparativa deferentes sistemas de ficheros.</p>"},{"location":"UD00/1.ConceptosGenerales/#configuracion-red-virtualbox","title":"Configuraci\u00f3n red VirtualBox","text":"VM \u2194 HOST VM1 \u2194 VM2 VM \u2192 INTERNET VM \u2190 INTERNET Solo anfitri\u00f3n SI SI NO NO Interna NO SI NO NO Adaptador puente SI SI SI SI NAT NO NO SI Reenv\u00edo de puertos Red NAT NO SI SI Reenv\u00edo de puertos <p>El modo que nos da m\u00e1s flexibilidad ser\u00eda el adaptador puente, mientras que el resto ser\u00edan m\u00e1s restrictivos.</p> <ul> <li>El adaptador puente hace que la m\u00e1quina virtual se conecte a la misma red que el anfitri\u00f3n, de tal forma que la MV se comportar\u00e1 como si fuera un PC m\u00e1s conectado a la red real. Nos permite conectar entre MV, desde el anfitri\u00f3n, y a Internet bidireccionalmente. Por contra, nos puede ocasionar problemas puesto que estar\u00e1 conectado a la red real (especialmente en caso de montar servidores).</li> <li>En modo solo anfitri\u00f3n podremos conectarnos desde el anfitri\u00f3n a nuestras m\u00e1quinas virtuales y viceversa, as\u00ed como conectar entre m\u00e1quinas virtuales. En todo caso, no tendremos por defecto conexi\u00f3n a internet (ni de salida ni de entrada)</li> <li>El modo NAT ser\u00eda todo lo contrario al modo anterior. Las m\u00e1quinas virtuales tendr\u00edan salida a internet, pero para poder conectar desde internet se tendr\u00edan que mapear puertos mediante NAT. No podr\u00edamos conectarnos entre diferentes m\u00e1quinas virtuales.</li> <li>Para poder conectar tambi\u00e9n entre m\u00e1quinas virtuales existe el modo red NAT, que a\u00f1ade a las caracter\u00edsticas del modo NAT.</li> <li>El modo interno ser\u00eda el m\u00e1s restringido, permitiendo \u00fanicamente conexi\u00f3n entre las m\u00e1quinas virtuales. No podr\u00edamos conectar desde el anfitri\u00f3n a las MV, ni tendr\u00edamos salida a Internet desde las MV.</li> </ul>"},{"location":"UD00/10.git/","title":"Uso de git","text":""},{"location":"UD00/10.git/#introduccion","title":"Introducci\u00f3n","text":"<p>Crear repositorios</p>"},{"location":"UD00/10.git/#submodulos","title":"Subm\u00f3dulos","text":""},{"location":"UD00/10.git/#anadir-submodulo-a-nuestro-proyecto","title":"A\u00f1adir subm\u00f3dulo a nuestro proyecto","text":"<p>C\u00f3mo incluir un repositorio dentro del nuestro. A crear un subm\u00f3dulo se \"copian\" los archivos dentro de nuestro proyecto y se crean unos archivos para indicar que esos archivos est\u00e1n asociados a otro repositorio:</p> <ol> <li> <p>A\u00f1adimos el nuevo subm\u00f3dulo: <code>git submodule add https://github.com/twbs/bootstrap vendor/bootstrap</code> Utilizamos la carpeta vendor para incluir c\u00f3digo que no es nuestro.</p> </li> <li> <p>Se han creado: <pre><code>ls vendor/bootstrap\n\ngit status\n    new file: .gitmodules     # archivo que asocia la ruta con la URL de github\n    new file: vendor/bootstrap    # Trata a esta carpta como un fichero\n\ngit add .\n\ngit commit -m \"Nuevo subm\u00f3dulo\"\n</code></pre></p> </li> </ol>"},{"location":"UD00/10.git/#como-actualizar-si-el-sumbodulo-tiene-alguna-actualizacion","title":"\u00bfC\u00f3mo actualizar si el sumb\u00f3dulo tiene alguna actualizaci\u00f3n?","text":"<p>https://www.youtube.com/watch?v=lBTykudEa_g</p> <p>Opci\u00f3n 1, podemos entrar en la carpeta correspondiente (vendor/bootstrap) y hacer un: <code>git pull</code></p> <p>Opci\u00f3n 2, si tenemos muchos subm\u00f3dulos <code>git submodule update --remote --recursive</code></p> <p>Opci\u00f3n 3, la m\u00e1s sencilla, pull + fetch + actualizaci\u00f3n de subm\u00f3dulos, lo deja todo al d\u00eda <code>git pull --recurse-submodules</code> Tambi\u00e9n funciona con el clone</p>"},{"location":"UD00/10.git/#inicializar-repositorio-que-tenga-un-submodulo","title":"Inicializar repositorio que tenga un submodulo","text":"<ol> <li>Lo clonamos</li> <li>Se crean las carpetas (dentro de vendor) pero est\u00e1n vac\u00edas.</li> <li>Debemos inicializar los m\u00f3dulos (registrarlos) <code>git subm\u00f3dules init</code></li> <li>Descargar los archivos del sumb\u00f3dulo <code>git submodule update</code></li> </ol>"},{"location":"UD00/2.bash/","title":"Terminal","text":"<p>La l\u00ednea de comandos de Linux, tambi\u00e9n conocida como terminal o shell, es una herramienta esencial en el mundo de la inform\u00e1tica y la administraci\u00f3n de sistemas. Ofrece un entorno de texto en el que los usuarios pueden interactuar con el sistema operativo mediante comandos escritos en lugar de utilizar una interfaz gr\u00e1fica de usuario.</p> <p>\u00bfPor qu\u00e9 aprender la l\u00ednea de comandos de Linux?</p> <ol> <li>Control Total: La l\u00ednea de comandos proporciona un control preciso sobre el sistema, permiti\u00e9ndote realizar tareas espec\u00edficas con gran flexibilidad.</li> <li>Automatizaci\u00f3n: Automatizar tareas repetitivas mediante scripts, lo que ahorra tiempo y reduce errores.</li> <li>Administraci\u00f3n del Sistema: Gestionar servidores y recursos de manera eficiente.</li> <li>Recuperaci\u00f3n y Resoluci\u00f3n de Problemas: En situaciones de recuperaci\u00f3n de datos o resoluci\u00f3n de problemas, la l\u00ednea de comandos es una herramienta valiosa.</li> </ol> <p>Conceptos B\u00e1sicos:</p> <ul> <li>Comandos: Son instrucciones espec\u00edficas que se ingresan en la l\u00ednea de comandos para realizar tareas, como crear archivos, mover carpetas o mostrar informaci\u00f3n del sistema.</li> <li>Argumentos: Son datos adicionales necesarios para que el comando funcione correctamente.</li> <li>Directorio Actual: Especifica el contexto de trabajo actual.</li> <li>Rutas: Se utilizan para especificar la ubicaci\u00f3n de archivos y directorios en el sistema.</li> </ul> <p>Primeros Pasos:</p> <ol> <li>Abrir la Terminal: En la mayor\u00eda de las distribuciones de Linux, puedes abrir la terminal desde el men\u00fa de aplicaciones o utilizando combinaciones de teclas como \"Ctrl + Alt + T\".</li> <li>Uso de comandos: Utiliza comandos como <code>ls</code> (listar), <code>echo</code> (imprimir por pantalla) y <code>pwd</code> (imprimir directorio de trabajo) para explorar el sistema de archivos.</li> <li>Ayuda y Manuales: <code>man ls</code> muestra el manual para el comando <code>ls</code>.</li> </ol> <pre><code>$ ls\n\n$ pwd\n\n$ echo \"HOLA\"\n\n$ man ls\n\n$ ls -la\n</code></pre> <p>Tipos de shell:</p> <p>Bash (Bourne Again Shell): Es el shell m\u00e1s com\u00fan en Linux y es ampliamente utilizado. Ofrece una amplia gama de caracter\u00edsticas, incluyendo autocompletado de comandos, historial de comandos, y scripts para automatizar tareas.</p> <p>Zsh (Z Shell): Es una extensi\u00f3n de Bash con caracter\u00edsticas adicionales, como sugerencias interactivas y personalizaci\u00f3n avanzada. Se ha vuelto popular entre los usuarios avanzados.</p> <p>Fish es conocido por su facilidad de uso y su sintaxis m\u00e1s intuitiva. Ofrece sugerencias de comandos en tiempo real y es ideal para principiantes.</p>"},{"location":"UD00/2.bash/#archivos-carpetas-y-enlaces","title":"Archivos, carpetas y enlaces","text":"<p>1. Archivos:</p> <ul> <li>Los archivos son unidades b\u00e1sicas de almacenamiento que contienen datos, programas o informaci\u00f3n.</li> <li>Pueden ser de diferentes tipos, como archivos de texto, binarios, de configuraci\u00f3n, etc.</li> <li>Los archivos se identifican por su nombre y extensi\u00f3n (opcional).</li> <li>Algunos comandos: <code>cat</code>, <code>touch</code>, <code>cp</code>, <code>mv</code>, <code>rm</code>, <code>head</code> y <code>tail</code>.</li> <li>Se pueden crear archivos o directorios \u201cocultos\u201d si su nombre empieza por punto (.), ejemplo: .config</li> </ul> <p>2. Carpetas (Directorios):</p> <ul> <li>Las carpetas, tambi\u00e9n llamadas directorios, son contenedores utilizados para organizar y agrupar archivos y otras carpetas.</li> <li>Se utilizan para crear una estructura jer\u00e1rquica en el sistema de archivos.</li> <li>Se utilizan comandos como: <code>mkdir</code>, <code>rmdir</code>, <code>cd</code> y <code>pwd</code>.</li> </ul> <p>3. Carpetas ESPECIALES:</p> <p>Todas las carpetas Linux tienen 2 carpetas \u201cespeciales\u201d en su interior.</p> <ul> <li>Carpeta punto (.) - Hace referencia a la carpeta actual (a ella misma)</li> <li>Carpeta punto punto (..) -  Hace referencia a la carpeta superior (carpeta \u201cpadre\u201d).</li> </ul> <p>4. Enlaces:</p> <ul> <li>Los enlaces son referencias o punteros a archivos o directorios existentes en el sistema de archivos.</li> <li>Hay dos tipos principales de enlaces en sistemas Linux: enlaces duros (hard links) y enlaces simb\u00f3licos (symbolic links o symlinks).</li> <li>Los enlaces duros apuntan al mismo contenido f\u00edsico que el archivo original y no pueden cruzar sistemas de archivos.</li> <li>Los enlaces simb\u00f3licos son m\u00e1s flexibles y pueden apuntar a archivos o directorios en cualquier ubicaci\u00f3n, incluso a trav\u00e9s de sistemas de archivos diferentes.</li> <li>Los enlaces permiten compartir recursos de manera eficiente y mantener la integridad de los datos.</li> </ul>"},{"location":"UD00/2.bash/#permisos","title":"Permisos","text":"<p>En Linux, cada fichero y carpeta tiene tres niveles de permisos: propietario, grupo y otros.</p> <p>El propietario es el usuario que cre\u00f3 el archivo o carpeta, el grupo es un conjunto de usuarios relacionados y \"otros\" incluye a todos los dem\u00e1s usuarios.</p> <p>Tipos:</p> <p>Lectura (r): Permite ver el contenido del archivo o listar el contenido de una carpeta.</p> <p>Escritura (w): Permite modificar o eliminar el archivo o carpeta.</p> <p>Ejecuci\u00f3n (x): Permite ejecutar un archivo o acceder a una carpeta.</p> <p>Representaci\u00f3n:</p> <p>Los permisos se representan con una cadena de nueve caracteres: rwxrwxrwx.</p> <p>Los primeros tres caracteres representan los permisos del propietario, los siguientes tres del grupo y los \u00faltimos tres para otros.</p> <p></p> <p></p> <p></p> <p>Permisos especiales</p> <ul> <li>Algunos archivos y carpetas tienen permisos especiales, como el bit SUID (Set User ID), que permite a un usuario ejecutar el archivo con los privilegios del propietario.</li> <li>El bit SGID (Set Group ID) hace que los archivos se ejecuten con los privilegios del grupo.</li> <li>El bit sticky (t) evita que otros usuarios eliminen o modifiquen archivos en una carpeta, a menos que sean propietarios o tengan permisos especiales.</li> </ul>"},{"location":"UD00/2.bash/#carpetas-del-sistema","title":"Carpetas del sistema","text":"<p>Estas carpetas son fundamentales para la organizaci\u00f3n y el funcionamiento del sistema. Dependiendo del Linux instalado algunas podr\u00edan no existir.</p> <ol> <li>/root:<ul> <li>Es el directorio de inicio del superusuario (root) y contiene los archivos de configuraci\u00f3n y datos del superusuario.</li> </ul> </li> <li>/home:<ul> <li>Contiene directorios personales de los usuarios regulares. Cada usuario tiene su propia carpeta con su nombre de usuario, donde pueden almacenar sus archivos y configuraciones personales.</li> </ul> </li> <li>/etc:<ul> <li>Archivos de configuraci\u00f3n del sistema y de aplicaciones. Aqu\u00ed se encuentran configuraciones importantes como /etc/passwd (informaci\u00f3n de usuarios), /etc/fstab (tablas de montaje de dispositivos), y muchos otros.</li> </ul> </li> <li>/bin y /sbin:<ul> <li>/bin almacena binarios (ejecutables) esenciales para el sistema, que son necesarios incluso en el modo de usuario \u00fanico.</li> <li>/sbin contiene binarios similares, pero est\u00e1n destinados a ser utilizados por el superusuario (root).</li> </ul> </li> <li>/usr:<ul> <li>/usr (abreviatura de \"Unix System Resources\") contiene programas, bibliotecas y archivos de datos que son utilizados por aplicaciones y usuarios.</li> <li>/usr/bin y /usr/sbin contienen binarios de programas instalados, mientras que /usr/lib contiene bibliotecas compartidas.</li> </ul> </li> <li>/var:<ul> <li>/var almacena datos variables, como archivos de registro (logs), correos electr\u00f3nicos y otros datos que cambian con el tiempo. Por ejemplo, /var/log contiene archivos de registro del sistema.</li> </ul> </li> <li>/tmp:<ul> <li>/tmp es un directorio temporal donde los programas y usuarios pueden crear archivos temporales. Los archivos aqu\u00ed se eliminan autom\u00e1ticamente despu\u00e9s de un tiempo o al reiniciar el sistema.</li> </ul> </li> <li>/dev:<ul> <li>/dev contiene archivos especiales que representan dispositivos en el sistema. Estos archivos se utilizan para interactuar con hardware y controladores de dispositivos.</li> </ul> </li> <li>/proc:<ul> <li>/proc es un sistema de archivos virtual que proporciona informaci\u00f3n en tiempo real sobre el sistema y los procesos en ejecuci\u00f3n. Se utiliza para acceder a informaci\u00f3n del kernel y configuraci\u00f3n din\u00e1mica.</li> </ul> </li> <li>/mnt y /media:<ul> <li>/mnt y /media son directorios utilizados para montar dispositivos de almacenamiento, como unidades USB o discos duros externos.</li> </ul> </li> <li>/opt:<ul> <li>/opt es el directorio donde se instalan aplicaciones y paquetes de software adicionales. Algunas aplicaciones de terceros se instalan en esta ubicaci\u00f3n.</li> </ul> </li> <li>/srv:<ul> <li>/srv se utiliza para almacenar datos de servicios proporcionados por el sistema, como sitios web o archivos compartidos a trav\u00e9s de la red.</li> </ul> </li> </ol>"},{"location":"UD00/2.bash/#rutas-absolutas-y-relativas","title":"Rutas absolutas y relativas","text":"<p>Hay dos tipos principales de rutas: rutas absolutas y rutas relativas.</p> <p>1. Rutas Absolutas:</p> <ul> <li>Siempre comienza con <code>/</code> y muestra la ubicaci\u00f3n exacta sin importar el directorio actual desde el que se est\u00e9 trabajando.</li> <li>Ejemplo de ruta absoluta: <code>/home/usuario/archivo.txt</code></li> </ul> <p>2. Rutas Relativas:</p> <ul> <li>No comienza con <code>/</code> y depende del directorio actual para determinar la ubicaci\u00f3n. (Es relativa al directorio actual)</li> <li>Ejemplo de ruta relativa: <code>../carpeta/archivo.txt</code></li> <li>Otro ejemplo: <code>carpeta/archivo.txt</code></li> </ul> <p>Ejemplo:</p> <p>Supongamos que est\u00e1s trabajando desde el directorio <code>/home/usuario/</code> y tienes la siguiente estructura de carpetas y archivos:</p> <pre><code>/home/usuario/\n    \u251c\u2500\u2500 documentos/\n    \u2502   \u251c\u2500\u2500 archivo1.txt\n    \u2502   \u2514\u2500\u2500 archivo2.txt\n    \u251c\u2500\u2500 fotos/\n    \u2502   \u251c\u2500\u2500 imagen1.jpg\n    \u2502   \u2514\u2500\u2500 imagen2.jpg\n    \u2514\u2500\u2500 musica/\n        \u251c\u2500\u2500 cancion.mp3\n        \u2514\u2500\u2500 lista_de_reproduccion.m3u\n</code></pre> <ul> <li>Para acceder al archivo <code>archivo1.txt</code> desde el directorio actual <code>/home/usuario/</code>, puedes utilizar una ruta relativa: <code>documentos/archivo1.txt</code>.</li> <li>Para acceder al mismo archivo desde cualquier ubicaci\u00f3n en el sistema, puedes utilizar una ruta absoluta: <code>/home/usuario/documentos/archivo1.txt</code>.</li> </ul> <p>Las rutas relativas son \u00fatiles cuando deseas trabajar dentro de un contexto espec\u00edfico, mientras que las rutas absolutas son necesarias cuando necesitas acceder a recursos desde cualquier ubicaci\u00f3n en el sistema.</p>"},{"location":"UD00/2.bash/#tips-terminal","title":"Tips terminal","text":"<p>Uso tabulador.</p> <p>Navegaci\u00f3n por flechas.</p>"},{"location":"UD00/2.bash/#editores-consola","title":"Editores consola","text":""},{"location":"UD00/2.bash/#nano","title":"nano","text":"<p>Nano es un editor de texto en la l\u00ednea de comandos que es especialmente adecuado para usuarios principiantes.</p> <pre><code>nano nombre_del_archivo # Abrir un archivo (o crear un archivo si no existe)\n</code></pre> <p>\u270f\ufe0f EJERCICIOS</p> <pre><code>1. Crear un archivo llamado \"abecedario.txt\" que contenga todas las letras del abecedario, una por l\u00ednea.\n2. Ejecuta el siguiente comando para tener una copia de seguridad del archivo:\n    cp abecedario.txt abecedario.bk\n</code></pre> <p>Navegaci\u00f3n:</p> <ul> <li>Utiliza las teclas de direcci\u00f3n (flechas) para mover el cursor por el texto.</li> <li>Puedes usar las teclas \"Av P\u00e1g\" y \"Re P\u00e1g\" para desplazarte r\u00e1pidamente hacia arriba y hacia abajo.</li> </ul> <p>Edici\u00f3n de Texto:</p> <ul> <li>Simplemente escribe o borra texto directamente en la ubicaci\u00f3n del cursor.</li> <li>Usa las teclas \"Insert\" o \"Ins\" para alternar entre los modos de inserci\u00f3n y reemplazo.</li> <li>Para copiar y pegar, selecciona el texto con el cursor y luego utiliza las teclas \"Ctrl + K\" para cortar y \"Ctrl + U\" para pegar.</li> </ul> <p>Guardar y Salir:</p> <ul> <li>Para guardar los cambios y salir, presiona \"Ctrl + O\" (te pedir\u00e1 confirmaci\u00f3n, presiona \"Enter\" para confirmar) y luego \"Ctrl + X\" para salir.</li> <li>Si deseas salir sin guardar los cambios, simplemente presiona \"Ctrl + X\" y confirma si es necesario.</li> </ul> <p>Buscar y Reemplazar:</p> <ul> <li>Presiona \"Ctrl + W\" para buscar texto en el archivo.</li> <li>Para buscar y reemplazar, presiona \"Ctrl + \\\" y sigue las instrucciones en la parte inferior de la pantalla.</li> </ul> <p>N\u00fameros de L\u00ednea:</p> <ul> <li>Puedes ver los n\u00fameros de l\u00ednea activando la opci\u00f3n \"Ctrl + _\" (Control + Shift + Barra Invertida).</li> </ul> <p>Guardar Copias de Seguridad:</p> <ul> <li>Para hacer copias de seguridad autom\u00e1ticas mientras editas, usa la opci\u00f3n \"Ctrl + O\" y agrega la opci\u00f3n \"-B\" al final del nombre del archivo.</li> </ul> <p>Resaltado de Sintaxis:</p> <ul> <li>Nano ofrece resaltado de sintaxis para varios lenguajes de programaci\u00f3n. Puedes habilitarlo con \"Ctrl + Y\" durante la edici\u00f3n.</li> </ul> <p>Configuraci\u00f3n Personalizada:</p> <ul> <li>Puedes personalizar Nano creando un archivo de configuraci\u00f3n en tu directorio de inicio llamado \".nanorc\". Aqu\u00ed puedes definir atajos de teclado y otras preferencias.</li> </ul> <p>Ayuda:</p> <ul> <li>Para acceder a la ayuda en l\u00ednea de Nano, presiona \"Ctrl + G\". Aqu\u00ed encontrar\u00e1s una lista de comandos y atajos \u00fatiles.</li> </ul>"},{"location":"UD00/2.bash/#vi-vim","title":"vi, vim","text":"<p>El editor de consola Vi es una herramienta poderosa y vers\u00e1til para editar archivos de texto en sistemas basados en Unix y Linux. Aunque puede tener una curva de aprendizaje pronunciada, dominar Vi es esencial para los usuarios avanzados y administradores de sistemas.</p> <pre><code>vi nombre_del_archivo # Abrir un archivo (o crear un archivo nuevo si no existe)\n</code></pre> <p>Modos de Vi:</p> <ul> <li>Modo Normal: Cuando abres Vi, est\u00e1s en el modo normal. En este modo, no puedes editar directamente el texto, pero puedes navegar por el archivo y ejecutar comandos.</li> <li>Modo de Inserci\u00f3n: Para editar el texto, debes cambiar al modo de inserci\u00f3n. Presiona \"i\" para ingresar al modo de inserci\u00f3n antes del cursor. Tambi\u00e9n puedes usar \"a\" para entrar en el modo de inserci\u00f3n despu\u00e9s del cursor.</li> <li>Modo de Comando: Puedes volver al modo normal en cualquier momento presionando la tecla \"Esc\".</li> </ul> <p>Guardar y Salir:</p> <ul> <li>Para guardar los cambios y salir, presiona \"Esc\" para asegurarte de estar en el modo normal, luego escribe <code>:wq</code> y presiona \"Enter\".</li> <li>Para salir sin guardar, utiliza <code>:q!</code>.</li> <li>Si deseas guardar pero no salir, utiliza <code>:w</code>.</li> </ul> <p>Navegaci\u00f3n:</p> <ul> <li>h, j, k, l: Estas teclas se utilizan en el modo normal para mover el cursor hacia la izquierda, abajo, arriba y derecha, respectivamente.</li> <li>G: Salta a la \u00faltima l\u00ednea del archivo.</li> <li>:n: Salta a la l\u00ednea \"n\" del archivo.</li> </ul> <p>B\u00fasqueda y Reemplazo:</p> <ul> <li>/texto: Busca \"texto\" hacia adelante en el archivo.</li> <li>?texto: Busca \"texto\" hacia atr\u00e1s en el archivo.</li> <li>:s/buscar/reemplazar/g: Reemplaza todas las ocurrencias de \"buscar\" con \"reemplazar\" en la l\u00ednea actual.</li> <li>:%s/buscar/reemplazar/g: Reemplaza todas las ocurrencias de \"buscar\" con \"reemplazar\" en todo el archivo.</li> </ul> <p>Copiar, Cortar y Pegar:</p> <ul> <li>yy: Copia (yanks) la l\u00ednea actual.</li> <li>dd: Corta (borra) la l\u00ednea actual.</li> <li>p: Pega el texto copiado o cortado despu\u00e9s del cursor.</li> </ul> <p>Desplazamiento R\u00e1pido:</p> <ul> <li>Ctrl + f: Desplazamiento hacia adelante una p\u00e1gina.</li> <li>Ctrl + b: Desplazamiento hacia atr\u00e1s una p\u00e1gina.</li> </ul> <p>Deshacer y Rehacer:</p> <ul> <li>u: Deshace la \u00faltima acci\u00f3n.</li> <li>Ctrl + r: Rehace la \u00faltima acci\u00f3n deshecha.</li> </ul>"},{"location":"UD00/2.bash/#comandos-basicos","title":"Comandos b\u00e1sicos","text":""},{"location":"UD00/2.bash/#listado-de-archivos","title":"Listado de archivos","text":"<p>pwd</p> <ul> <li>Descripci\u00f3n: (\"print working directory\") muestra la ruta completa del directorio en el que te encuentras actualmente en el sistema de archivos.</li> <li>Principales opciones:<ul> <li><code>P</code>: Muestra la ruta f\u00edsica real en lugar de la ruta simb\u00f3lica, si est\u00e1s en un enlace simb\u00f3lico.</li> <li><code>-help</code>: Muestra la ayuda y la informaci\u00f3n de uso del comando \"pwd\".</li> <li><code>-version</code>: Muestra la versi\u00f3n del comando \"pwd\".</li> </ul> </li> <li>Ejemplo de uso:</li> </ul> <pre><code>pwd # Muestra la carpeta actual, la carpeta donde est\u00e1 el usuario\n</code></pre> <p>ls</p> <ul> <li>Descripci\u00f3n: (list) Muestra una lista de archivos y directorios en la ubicaci\u00f3n especificada.</li> <li>Principales opciones:<ul> <li><code>l</code>: Muestra el contenido en formato largo, incluyendo detalles como permisos, propietario, grupo, tama\u00f1o, fecha de modificaci\u00f3n y nombre del archivo.</li> <li><code>a</code>: Muestra archivos ocultos (los que comienzan con un punto <code>.</code>).</li> <li><code>h</code>: Muestra tama\u00f1os de archivos en un formato legible por humanos (por ejemplo, KB, MB, GB).</li> <li><code>R</code>: Lista de manera recursiva, mostrando el contenido de subdirectorios.</li> </ul> </li> <li>Ejemplo de uso:</li> </ul> <pre><code>ls # Listar archivos y directorios en el directorio actual\nls -la # Listar archivos y directorios en formato largo y mostrar archivos ocultos\nls -R /ruta/al/directorio # Lista de manera recursiva\n</code></pre> <p>\u270f\ufe0f EJERCICIOS <pre><code>1. Listar todos los archivos del directorio /bin. \n2. Muestra todos los archivos y directorios de la carpeta actual (tambi\u00e9n los ficheros ocultos)\n3. Muestra solamente los directorios de la carpeta actual\n4. Muestra el contenido de la ra\u00edz del sistema\n5. Muestra el contenido de la carpeta actual utilizando rutas absolutas\n6. Listar todos los archivos del directorio /dev que empiecen por t.\n7. Listar todos los archivos del directorio /dev que empiecen por t y acaben en C1.\n8. Listar recursivamente el contenido de /usr\n9. Listar todos los archivos del directorio /etc que empiecen por t en orden inverso.\n10. Lista todos los archivos de tipo \"log\" del sistema\n11. Mostrar el d\u00eda y la hora actual.\n</code></pre></p>"},{"location":"UD00/2.bash/#leer-el-contenido-de-un-archivo","title":"Leer el contenido de un archivo","text":"<p>cat</p> <p>(concatenate) se utiliza para mostrar el contenido de archivos de texto en la terminal, pero tambi\u00e9n se utiliza para crear, concatenar y mostrar el contenido de archivos.</p> <ul> <li>Principales opciones:<ul> <li><code>n</code>: Numerar las l\u00edneas del archivo.</li> <li><code>E</code>: Mostrar un signo de d\u00f3lar (<code>$</code>) al final de cada l\u00ednea.</li> <li><code>A</code>: Equivalente a <code>vET</code> para mostrar n\u00fameros de l\u00ednea y signos de d\u00f3lar.</li> <li><code>b</code>: Numerar las l\u00edneas en blanco.</li> <li><code>s</code>: Suprimir l\u00edneas en blanco repetidas al mostrar el contenido.</li> </ul> </li> </ul> <pre><code>cat abecedario.txt #Mostrar el contenido de un archivo en la terminal\ncat -n abecedario.txt #Mostrar el contenido de un archivo con n\u00fameros de l\u00ednea\ncat abecedario.txt abecedario.txt #Concatenar dos archivos y mostrar el resultado \n</code></pre> <p>more (less)</p> <p>Visualizar el contenido de archivos de texto uno por uno en la terminal. Permite desplazarse hacia adelante y hacia atr\u00e1s a trav\u00e9s del contenido de un archivo, lo que lo hace \u00fatil para ver archivos largos sin sobrecargar la pantalla.</p> <ul> <li>Principales opciones:<ul> <li>Presiona la barra espaciadora para avanzar una p\u00e1gina.</li> <li>Presiona la tecla \"q\" para salir de <code>more</code> en cualquier momento.</li> <li>Para buscar texto, presiona la tecla \"/\" seguida del texto a buscar y luego presiona \"Enter\". Luego, usa \"n\" para buscar la siguiente coincidencia y \"N\" para buscar la anterior.</li> </ul> </li> </ul> <pre><code>more archivo.txt\n</code></pre> <p>head</p> <p>Mostrar las primeras l\u00edneas de un archivo de texto en la terminal. Por defecto, muestra las primeras 10 l\u00edneas.</p> <ul> <li>Principales opciones:<ul> <li><code>n N</code>: Muestra las primeras N l\u00edneas del archivo.</li> <li><code>c N</code>: Muestra los primeros N bytes del archivo en lugar de l\u00edneas.</li> </ul> </li> </ul> <pre><code>head abecedario.txt #Mostrar las primeras 10 l\u00edneas de un archivo de texto\nhead -n 5 abecedario.txt #Mostrar las primeras 5 l\u00edneas de un archivo de texto\nhead -c 100 abecedario.txt #Mostrar los primeros 100 bytes de un archivo\n</code></pre> <p>tail</p> <p>Mostrar las \u00faltimas l\u00edneas de un archivo de texto en la terminal. Por defecto, muestra las \u00faltimas 10 l\u00edneas.</p> <ul> <li>Principales opciones:<ul> <li><code>n N</code>: Muestra las \u00faltimas N l\u00edneas del archivo.</li> <li><code>f</code>: Muestra el contenido del archivo en tiempo real y se actualiza autom\u00e1ticamente cuando se agregan nuevas l\u00edneas al archivo (\u00fatil para ver archivos de registro en constante cambio).</li> </ul> </li> </ul> <pre><code>tail abecedario.txt #Mostrar las \u00faltimas 10 l\u00edneas de un archivo de texto\ntail -n 5 abecedario.txt #Mostrar las \u00faltimas 5 l\u00edneas de un archivo de texto\ntail -f abecedario.txt #Mostrar el contenido en tiempo real de un archivo de registro\n</code></pre>"},{"location":"UD00/2.bash/#gestion-de-archivos","title":"Gesti\u00f3n de archivos","text":"<p>touch</p> <p>Crear archivos vac\u00edos o actualizar la marca de tiempo de archivos existentes. Su funcionamiento b\u00e1sico es crear un archivo si no existe o actualizar la marca de tiempo del archivo si ya existe.</p> <ul> <li>Principales opciones:<ul> <li><code>c</code>: No crea un nuevo archivo si no existe.</li> <li><code>d</code>: Permite especificar una fecha y hora de marca de tiempo en lugar de usar la actual.</li> <li><code>t</code>: Se utiliza junto con la opci\u00f3n <code>d</code> para establecer una fecha y hora espec\u00edficas.</li> </ul> </li> </ul> <pre><code>touch miarchivo.txt # Crear un archivo vac\u00edo llamado \"miarchivo.txt\"\ntouch -c miarchivo.txt # Actualizar la marca de tiempo de un archivo existente\ntouch -d \"2023-10-02 14:30:00\" miarchivo.txt # Establecer una marca de tiempo personalizada\n\nstat miarchivo.txt\n</code></pre> <p>Operadores &gt; y &gt;&gt;</p> <p>Se utilizan en Bash para redirigir la salida est\u00e1ndar de un comando hacia un archivo en lugar de mostrarla en la pantalla.</p> <p><code>&gt;</code> (Redirecci\u00f3n de salida):</p> <p>Utilizado para redirigir la salida est\u00e1ndar de un comando hacia un archivo. Si el archivo ya existe, su contenido se sobrescribir\u00e1.</p> <pre><code>ls &gt; lista_archivos.txt\n</code></pre> <p><code>&gt;&gt;</code> (Redirecci\u00f3n de salida, modo anexar):</p> <p>Funcionamiento: Similar a <code>&gt;</code>, pero en lugar de sobrescribir el archivo, agrega la salida al final del archivo. Si el archivo no existe, se crea.</p> <pre><code>echo \"Texto adicional\" &gt;&gt; archivo_existente.txt\n</code></pre> <p>Operador | (tuber\u00eda, pipe) \u203c\ufe0f</p> <p>Redirigir la salida de un comando hacia la entrada de otro. Esto permite encadenar varios comandos juntos para realizar operaciones m\u00e1s complejas o procesar datos de manera eficiente.</p> <pre><code>ls -la | head -n 2\n</code></pre> <p>cp</p> <p>Copiar archivos o directorios de un lugar a otro en el sistema de archivos.</p> <ul> <li>Principales opciones:<ul> <li><code>r</code> o <code>R</code>: Copia directorios de manera recursiva.</li> <li><code>i</code>: Pide confirmaci\u00f3n antes de sobrescribir un archivo existente.</li> <li><code>u</code>: Copia solo cuando el archivo de origen sea m\u00e1s reciente que el archivo de destino o cuando el archivo de destino no exista.</li> <li><code>v</code>: Muestra un mensaje para cada archivo copiado, mostrando el progreso.</li> </ul> </li> </ul> <pre><code>cp abecedario.txt /ruta/destino/ # Copiar un archivo a otro directorio\ncp -r directorio_origen/ directorio_destino/ #Copiar un directorio y su contenido recursivamente\ncp -i abecedario.txt /ruta/destino/ #Copiar un archivo con confirmaci\u00f3n antes de sobrescribir\n</code></pre> <p>mv</p> <p>Mover o renombrar archivos y directorios en el sistema de archivos. Puede utilizarse para cambiar el nombre de un archivo o para cambiar su ubicaci\u00f3n en el sistema de archivos.</p> <ul> <li>Principales opciones:<ul> <li><code>i</code>: Pide confirmaci\u00f3n antes de sobrescribir un archivo existente en el destino.</li> <li><code>u</code>: Mueve solo cuando el archivo de origen sea m\u00e1s reciente que el archivo de destino o cuando el archivo de destino no exista.</li> <li><code>v</code>: Muestra un mensaje para cada operaci\u00f3n realizada, mostrando el progreso.</li> </ul> </li> </ul> <pre><code>mv abecedario.txt /ruta/destino/ #Mover un archivo a otro directorio\nmv abecedario.txt abecedario2.txt #Cambiar el nombre de un archivo\nmv directorio_origen/ directorio_destino/ # Mover un directorio y su contenido a otro lugar\nmv directorio_actual/ nuevo_nombre/ #Cambiar el nombre de un directorio\n</code></pre> <p>rm</p> <p>Eliminar archivos o directorios en el sistema de archivos.</p> <ul> <li>Principales opciones:<ul> <li><code>r</code> o <code>R</code>: Elimina directorios y su contenido de manera recursiva.</li> <li><code>i</code>: Pide confirmaci\u00f3n antes de eliminar cada archivo o directorio.</li> <li><code>f</code>: Forzar la eliminaci\u00f3n sin pedir confirmaci\u00f3n, \u00fatil para eliminar archivos sin interacci\u00f3n.</li> <li><code>v</code>: Muestra un mensaje para cada archivo o directorio eliminado, mostrando el progreso.</li> </ul> </li> </ul> <pre><code>rm abecedario.txt #Eliminar un archivo\nrm -r directorio/ #Eliminar un directorio y su contenido de manera recursiva\nrm -f abecedario.txt abecedario2.txt # Eliminar varios archivos sin confirmaci\u00f3n\nrm -i abecedario1.txt abecedario2.txt # Eliminar archivos con confirmaci\u00f3n\n</code></pre> <p>mkdir</p> <p>Crear directorios (carpetas) en el sistema de archivos.</p> <ul> <li>Principales opciones:<ul> <li><code>p</code>: Crea directorios padres necesarios de manera recursiva. Si un directorio padre no existe, lo crear\u00e1 autom\u00e1ticamente.</li> <li><code>m</code>: Establece permisos (modo) para el directorio creado.</li> </ul> </li> </ul> <pre><code>mkdir nombre_directorio #Crear un directorio en la ubicaci\u00f3n actual\nmkdir -m 755 nombre_directorio #Crear un directorio con permisos espec\u00edficos\nmkdir -p directorio_padre/directorio_hijo/subdirectorio # Crear directorios anidados de manera recursiva\n</code></pre> <p>ln</p> <p></p> <ul> <li>Descripci\u00f3n: El comando <code>ln</code> se utiliza para crear enlaces (links) entre archivos en sistemas Unix y Linux. Puede crear enlaces duros (hard links) o enlaces simb\u00f3licos (symbolic links o symlinks).</li> <li>Principales opciones:<ul> <li><code>s</code>: Crea un enlace simb\u00f3lico en lugar de un enlace duro. Los enlaces simb\u00f3licos son referencias a archivos o directorios y pueden apuntar a ubicaciones fuera del sistema de archivos actual.</li> <li><code>b</code>: Realiza una copia de seguridad de los archivos existentes antes de reemplazarlos.</li> <li><code>f</code>: Fuerza la creaci\u00f3n de enlaces, incluso si el archivo de destino ya existe.</li> </ul> </li> </ul> <pre><code>ln archivo_existente enlace_duro #Crear un enlace duro (hard link) para un archivo existente\nln -s /ruta/al/origen enlace_simbolico #Crear un enlace simb\u00f3lico (symlink) a un archivo o directorio\nln -b archivo_existente enlace_duro #Crear un enlace duro y realizar una copia de seguridad del archivo de destino si ya existe\n</code></pre>"},{"location":"UD00/2.bash/#ejercicios","title":"\u270f\ufe0f Ejercicios","text":"<pre><code>0. Crea el siguiente sistema de ficheros en tu home:\n\nPara crear los ficheros y que tengan algo de contenido puedes utilizar el comando:\necho $RANDOM &gt; fichero_a_crear.txt\n\n\u2514\u2500\u2500 home/\n  \u2514\u2500\u2500 tu_usuario/\n      \u2514\u2500\u2500 ejercicios/\n          \u251c\u2500\u2500 carpeta1/\n          \u2502   \u251c\u2500\u2500 archivo1.txt\n          \u2502   \u251c\u2500\u2500 archivo2.txt\n          \u2502   \u251c\u2500\u2500 subcarpeta1/\n          \u2502   \u2502   \u2514\u2500\u2500 archivo3.txt\n          \u2502   \u2514\u2500\u2500 subcarpeta2/\n          \u2502       \u2514\u2500\u2500 archivo4.txt\n          \u251c\u2500\u2500 carpeta2/\n          \u2502   \u251c\u2500\u2500 subcarpeta1/\n          \u2502   \u2502   \u2514\u2500\u2500 archivo5.txt\n          \u2502   \u2514\u2500\u2500 subcarpeta2/\n          \u2502       \u2514\u2500\u2500 archivo6.txt\n          \u2514\u2500\u2500 documentos/\n              \u251c\u2500\u2500 documento1.txt\n              \u2514\u2500\u2500 documento2.txt\n\n1. Vuelve a tu home (cd)\n\n**// Utilizando rutas relativas**\n2. Elimina el archivo \"archivo1.txt\"\n3. Borra la carpeta \"subcarpeta2\"\n4. Mueve el archivo \"archivo2.txt\" a la carpeta \"documentos\"\n5. Crea una carpeta llamada \"tmp/\" dentro de ejercicios.\n6. Copia la carpeta \"subcarpeta1\" a la carpeta \"tmp\"\n\n**// Utilizando rutas absolutas**\n7. Utiliza el comando rm para eliminar el archivo \"archivo5.txt\"\n8. Copia el archivo \"documento1.txt\" a la carpeta \"subcarpeta1\"\nNO-&gt;&gt;&gt;&gt; 9. Borra el archivo \"archivo5.txt\"\n10. Copia la carpeta \"carpeta1\" dentro de la carpeta \"tmp\"\n11. Borra la carpeta \"tmp/carpeta1\"\n\n**// Enlaces**\n12. Crea un enlace llamado \"enlace1\" dentro de ejercicios, al archivo \"archivo6.txt\"\n13. Muestra el contenido de \"enlace1\"\n14. Modifica el contenido de \"archivo6.txt\"\n15. Muestra el contenido de \"enlace1\"\n16. Borra \"archivo6.txt\"\n17. Realiza un ls -la para ver qu\u00e9 ha ocurrido\n18. Muestra el contenido de \"enlace1\"\n\n**Extra**\n1. Mu\u00e9vete a tu home (/home/tu_usuario) sin especificar la carpeta (utilizando un alias). \n        https://es.wikipedia.org/wiki/Virgulilla\n2. Cambia a la carpeta /tmp y regresa al directorio anterior.\n        https://www.ibm.com/docs/en/zos/2.2.0?topic=descriptions-cd-change-working-directory\n3. Ejecuta los siguientes comandos (dentro de tu home)\n    $   mkdir extra\n    $   cd extra\n    $   echo \"Hola\" &gt; 1\n    $   echo \"Hola\" &gt; 2\n    $   echo \"Hola\" &gt; -3\n    $   mkdir 4 5 6 \n    **** Escribe un comando que borre TODOS los archivos y directorios **** \n</code></pre>"},{"location":"UD00/2.bash/#division-de-archivos","title":"Divisi\u00f3n de archivos","text":"<p>cut</p> <p>Seleccionar y mostrar partes espec\u00edficas de l\u00edneas de texto en archivos o la entrada est\u00e1ndar. Es \u00fatil para dividir o filtrar datos basados en campos delimitados por separadores, como tabulaciones o comas.</p> <ul> <li>Principales opciones:<ul> <li><code>f N</code>: Especifica el n\u00famero de campo que deseas extraer (por ejemplo, <code>f 2</code> para el segundo campo).</li> <li><code>d DELIMITADOR</code>: Especifica el delimitador utilizado para separar campos en las l\u00edneas (por defecto, es la pesta\u00f1a).</li> </ul> </li> </ul> <pre><code>cut -f 1 -d ',' archivo.csv #Extraer el primer campo de un archivo CSV (coma como delimitador)\ncut -f 3 -d $'\\t' archivo.txt #Extraer el tercer campo de un archivo de texto tabulado\ncut -c 1-5 archivo.txt #Extraer los primeros cinco caracteres de cada l\u00ednea de un archivo\n</code></pre> <p>split</p> <p>Dividir archivos grandes en fragmentos m\u00e1s peque\u00f1os, lo que puede ser \u00fatil para manejar archivos extensos o para la transferencia m\u00e1s eficiente de datos.</p> <ul> <li>Principales opciones:<ul> <li><code>b N</code>: Divide el archivo en fragmentos de tama\u00f1o fijo especificado (por ejemplo, <code>b 1M</code> divide en fragmentos de 1 megabyte).</li> <li><code>l N</code>: Divide el archivo en fragmentos con un n\u00famero fijo de l\u00edneas (por ejemplo, <code>l 100</code> divide en fragmentos de 100 l\u00edneas).</li> <li><code>d</code>: Utiliza nombres de archivo num\u00e9ricos para los fragmentos (por ejemplo, <code>x00</code>, <code>x01</code>, <code>x02</code>, ...).</li> <li><code>a N</code>: Especifica el n\u00famero de caracteres a utilizar en los nombres de archivo num\u00e9ricos (por ejemplo, <code>a 3</code> para <code>x001</code>, <code>x002</code>, ...).</li> </ul> </li> </ul> <pre><code>split -b 1M archivo_grande.txt #Dividir un archivo en fragmentos de 1 megabyte\nsplit -l 100 archivo_grande.txt #Dividir un archivo en fragmentos con 100 l\u00edneas cada uno\nsplit -a 3 archivo_grande.txt #Dividir un archivo en fragmentos con nombres num\u00e9ricos de tres caracteres\n</code></pre>"},{"location":"UD00/2.bash/#ejercicios_1","title":"\u270f\ufe0f Ejercicios","text":"<pre><code>1. Crea un archivo llamado \"resultado.txt\" que contenga un listado largo (opci\u00f3n -la) de la carpeta /etc \n2. Muestra la columna de los permisos del archivo anterior.\n3. Muesta el nombre de los archivos de \"resultado.txt\". &lt;--- (tr)\n4. Muesta los nombres de los usuarios registrados en el sistema.\n\n---\n5. Crea un archivo que contenga:\n\nNombre,Edad,Profesi\u00f3n,Tel\u00e9fono,Ciudad,Pa\u00eds,Primer Apellido,Segundo Apellido\nJuan,30,Ingeniero,123-456-789,Madrid,Espa\u00f1a,P\u00e9rez,G\u00f3mez\nMar\u00eda,25,M\u00e9dica,987-654-321,Barcelona,Espa\u00f1a,Rodr\u00edguez,L\u00f3pez\nCarlos,35,Abogado,555-555-555,Valencia,Espa\u00f1a,Fern\u00e1ndez,Mart\u00ednez\nAna,28,Profesora,444-333-222,Sevilla,Espa\u00f1a,L\u00f3pez,S\u00e1nchez\nDavid,40,Arquitecto,111-222-333,Bilbao,Espa\u00f1a,Mart\u00ednez,Ruiz\nLaura,29,Periodista,777-888-999,Granada,Espa\u00f1a,Garc\u00eda,P\u00e9rez\nPedro,45,M\u00e9dico,555-123-456,Toledo,Espa\u00f1a,Garc\u00eda,Jim\u00e9nez\nIsabel,32,Ingeniera,333-666-999,Zaragoza,Espa\u00f1a,Torres,Rodr\u00edguez\nSergio,27,Dise\u00f1ador,666-999-444,Barcelona,Espa\u00f1a,Ruiz,Gonz\u00e1lez\nLuisa Gonz\u00e1lez,38,Psic\u00f3loga,222-444-666,Valencia,Espa\u00f1a,Gonz\u00e1lez,Ruiz\n\n6. Muestra el primer apellido de todos\n7. Muestra la profesi\u00f3n de todos\n8. Muestra el tel\u00e9fono de todos\n7. Muestra el Nombre y primer apellido de todos\n8. Mustra el Tel\u00e9fono de los 2 \u00faltimos\n9. Muestra todos los datos del primero\n10. Muestra todos los datos del \u00faltimo separados por espacios\n11. Muestra el segundo grupo de d\u00edgitos del tel\u00e9fono de Laura (888)\n12. Muestra unicamente los campos Tel\u00e9fono, ciudad y pa\u00eds de todos \n</code></pre>"},{"location":"UD00/2.bash/#busqueda-y-sustitucion-en-archivos","title":"B\u00fasqueda y sustituci\u00f3n en archivos","text":"<p>tr</p> <p>Toma una cadena de texto de entrada y realiza transformaciones en los caracteres seg\u00fan las especificaciones proporcionadas por el usuario. Puede utilizarse para reemplazar caracteres, eliminar caracteres, cambiar may\u00fasculas a min\u00fasculas o viceversa, entre otros.</p> <ul> <li>Principales opciones:<ul> <li><code>d</code>: Elimina los caracteres especificados en lugar de traducirlos.</li> <li><code>c</code>: Complementa el conjunto de caracteres especificados en lugar de traducirlos.</li> <li><code>s</code>: Sustituye secuencias repetidas de caracteres con un solo car\u00e1cter.</li> <li><code>u</code>: Unicode. Permite especificar rangos de caracteres Unicode para la traducci\u00f3n.</li> </ul> </li> </ul> <pre><code>cat archivo.txt | tr 'a-z,' 'A-Z ' # Cambia min\u00fasculas por may\u00fasculas\nls -la | tr -s ' ' # Sustituye m\u00faltiples espacios por uno\n</code></pre> <p>grep</p> <p>Buscar patrones de texto en archivos o en la entrada est\u00e1ndar. Se pueden buscar l\u00edneas que coincidan con un patr\u00f3n especificado en un archivo o una serie de archivos.</p> <ul> <li>Principales opciones:<ul> <li><code>i</code>: Realiza b\u00fasquedas insensibles a may\u00fasculas y min\u00fasculas.</li> <li><code>v</code>: Invierte la b\u00fasqueda para mostrar las l\u00edneas que NO coinciden con el patr\u00f3n.</li> <li><code>r</code> o <code>R</code>: Realiza b\u00fasquedas recursivas en directorios.</li> <li><code>l</code>: Muestra solo los nombres de los archivos que contienen coincidencias.</li> <li><code>n</code>: Muestra n\u00fameros de l\u00ednea junto con las coincidencias.</li> <li><code>e PATR\u00d3N</code>: Permite buscar m\u00faltiples patrones utilizando expresiones regulares.</li> </ul> </li> </ul> <pre><code>grep \"palabra\" archivo.txt #Buscar una palabra espec\u00edfica en un archivo\ngrep -i \"palabra\" archivo.txt #Buscar una palabra insensible a may\u00fasculas y min\u00fasculas en un archivo\ngrep -rl \"palabra\" /ruta/al/directorio/ #Buscar una palabra en todos los archivos de un directorio de manera recursiva y mostrar los nombres de los archivos que contienen coincidencias\ngrep -e \"patr\u00f3n1\" -e \"patr\u00f3n2\" archivo.txt #Buscar m\u00faltiples patrones utilizando expresiones regulares\n</code></pre> <p>sed</p> <p>(Stream Editor) Realizar transformaciones en el texto de entrada (ya sea desde un archivo o la entrada est\u00e1ndar) y escribir la salida en la pantalla o en un archivo. Es especialmente \u00fatil para la b\u00fasqueda y sustituci\u00f3n de texto, as\u00ed como para realizar otras ediciones en el contenido del archivo.</p> <ul> <li>Principales opciones:<ul> <li><code>e SCRIPT</code>: Permite especificar un script de edici\u00f3n de <code>sed</code> para realizar m\u00faltiples operaciones.</li> <li><code>i</code>: Modifica el archivo de entrada directamente (in-place) en lugar de mostrar la salida en la pantalla.</li> <li><code>n</code>: Suprime la salida predeterminada de <code>sed</code> y solo muestra las l\u00edneas modificadas seg\u00fan el script.</li> <li><code>r</code> o <code>E</code>: Habilita las expresiones regulares extendidas.</li> </ul> </li> </ul> <pre><code>sed 's/antiguo_texto/nuevo_texto/g' archivo.txt &gt; nuevo_archivo.txt #Sustituir una cadena de texto por otra en un archivo y guardar el resultado en un nuevo archivo\nsed -i '/^$/d' archivo.txt #Eliminar l\u00edneas vac\u00edas de un archivo y modificar el archivo en su lugar (in-place)\nsed -e 's/palabra1/reemplazo1/g' -e 's/palabra2/reemplazo2/g' archivo.txt #Utilizar un script de edici\u00f3n sed para realizar m\u00faltiples operaciones\nsed 's/ \\+/\\ /g' ls -la # reemplazar espacios\n</code></pre> <p>sed VS tr</p> <p>Si necesitas realizar ediciones complejas o transformaciones avanzadas en el texto, \"sed\" es la opci\u00f3n adecuada (permite el uso de expresiones regulares). Si solo necesitas realizar traducciones simples de caracteres o eliminar caracteres espec\u00edficos, \"tr\" es m\u00e1s adecuado debido a su simplicidad y velocidad.</p>"},{"location":"UD00/2.bash/#ejercicios_2","title":"\u270f\ufe0f Ejercicios","text":"<pre><code>Crea un archivo con el siguiente contenido (ejemplotexto.txt):\nLa camioneta es     vieja\nLa estUfa es nueva\nLa cesta de nueva\nLa camisa es vieja\nLa sudadera es nueva\n\n// Utiliza la opci\u00f3n -i para sobreescribir el fichero original\n1. Cambiar la U may\u00fascula por min\u00fascula.\n2. Reemplaza la palabra nueva por vieja.\n3. Reemplaza la palabra vieja por nueva solamente en la l\u00ednea 4 del archivo\n4. La primera l\u00ednea tiene muchos espacios antes de \"vieja\", sustituyelos por 1 espacio\n5. Sustituye todos los espacios por #\n6. Eliminar los saltos de l\u00ednea del archivo\n</code></pre> <pre><code># Soluciones\n\nsed 's/U/u/g' ejemplotexto.txt    \nsed -i '4 s/nueva/vieja/g' ejemplotexto.txt\nsed 's/ /#/g' ejemplotexto.txt\n\u2192 Mejora: sed 's/ \\+/#/g' ejemplotexto.txt\nsed -z 's/\\n/ /g' a.txt\n</code></pre>"},{"location":"UD00/2.bash/#ordenar","title":"Ordenar","text":"<p>sort</p> <p>Ordenar l\u00edneas de texto en un archivo o la entrada est\u00e1ndar. Puede ordenar l\u00edneas alfab\u00e9ticamente o num\u00e9ricamente, en orden ascendente o descendente.</p> <ul> <li>Principales opciones:<ul> <li><code>r</code>: Ordena en orden descendente (de mayor a menor).</li> <li><code>n</code>: Realiza una ordenaci\u00f3n num\u00e9rica en lugar de una ordenaci\u00f3n alfab\u00e9tica.</li> <li><code>u</code>: Elimina l\u00edneas duplicadas en la salida.</li> <li><code>k N[,M]</code>: Especifica un campo o rango de campos a considerar al ordenar, donde N y M son n\u00fameros de columna.</li> </ul> </li> </ul> <pre><code>sort archivo.txt #Ordenar l\u00edneas alfab\u00e9ticamente en orden ascendente\nsort -rn archivo.txt #Ordenar l\u00edneas num\u00e9ricamente en orden descendente\nsort -n -k 2 archivo.txt #Ordenar un archivo por el segundo campo (columna) num\u00e9rico\nsort -u archivo.txt &gt; archivo_sin_duplicados.txt #Eliminar l\u00edneas duplicadas en un archivo y guardar el resultado en un nuevo archivo\n</code></pre>"},{"location":"UD00/2.bash/#gestion-de-usuarios-y-grupos","title":"Gesti\u00f3n de usuarios y grupos","text":"<p>useradd *vs *adduser</p> <p>useradd es un comando que ejecuta un binario del sistema, mientras que adduser es un script en perl que utiliza el binario useradd.</p> <p>La mayor ventaja del comando adduser es que crea el directorio home (/home/usuario/) del usuario de manera autom\u00e1tica, cosa que no hace useradd (hay que usar la opci\u00f3n -m).</p> <p>userdel *vs *deluser</p> <p>Ambos comandos sirven para borrar usuarios. Y al igual que useradd y adduser: el comando userdel es un fichero binario, mientras que deluser es un script en perl que usa el binario userdel.</p> <p>groupadd</p> <p>Creaci\u00f3n de grupos</p> <pre><code>groupadd grupo1\n\n# comprueba el fichero /etc/group\n</code></pre> <p>usermod</p> <p>A\u00f1adir un usuario a un grupo</p> <pre><code>usermod -aG grupo1 usuario1\n</code></pre>"},{"location":"UD00/2.bash/#gestion-de-permisos","title":"Gesti\u00f3n de Permisos","text":"<p>chmod</p> <p>Cambiar los permisos (lectura, escritura, ejecuci\u00f3n) de archivos o carpetas en Linux.</p> <ul> <li>Principales Opciones:<ul> <li><code>R</code> (o <code>-recursive</code>): Aplica los cambios de permisos de manera recursiva a todos los archivos y carpetas dentro de una carpeta.</li> <li><code>+</code> (m\u00e1s) y `` (menos): Permite agregar (+) o quitar (-) permisos.</li> <li><code>u</code>, <code>g</code>, <code>o</code> y <code>a</code>: Representan al propietario (user), grupo (group), otros (others) y todos (all) respectivamente.</li> <li>Permisos en formato octal: Se pueden establecer permisos utilizando n\u00fameros octales, como 755 o 644.</li> </ul> </li> </ul> <pre><code># Cambiar los permisos de un archivo para que el propietario tenga permisos de lectura y escritura, el grupo tenga solo permisos de lectura y otros no tengan ning\u00fan permiso\nchmod u=rw,g=r,o= archivo.txt \n\n# Cambiar de manera recursiva los permisos de una carpeta y su contenido para que todos tengan permisos de lectura y escritura\nchmod -R a=rw carpeta/\n</code></pre> <p>Ejemplo con notaci\u00f3n octal:</p> <p>Supongamos que deseas establecer los siguientes permisos en un archivo llamado \"mi_archivo.txt\":</p> <p>El propietario debe tener permisos de lectura, escritura y ejecuci\u00f3n (7).</p> <p>El grupo debe tener permisos de lectura y ejecuci\u00f3n, pero no de escritura (5).</p> <p>Otros usuarios deben tener solo permisos de lectura (4).</p> <pre><code>chmod 755 mi_archivo.txt\n</code></pre> <p></p> <p>umask</p> <p>Establecer y mostrar la m\u00e1scara de creaci\u00f3n de archivos por defecto. La m\u00e1scara de creaci\u00f3n de archivos determina los permisos predeterminados que se asignar\u00e1n a los archivos y carpetas cuando se creen.</p> <ul> <li>Principales Opciones:<ul> <li>Sin opciones, el comando <code>umask</code> muestra la m\u00e1scara de creaci\u00f3n de archivos actual en notaci\u00f3n octal.</li> <li><code>S</code> (o <code>-symbolic</code>): Muestra la m\u00e1scara de creaci\u00f3n de archivos en notaci\u00f3n simb\u00f3lica (por ejemplo, \"u=rw,go=rx\").</li> <li>Puedes establecer una m\u00e1scara de creaci\u00f3n de archivos espec\u00edfica proporcionando un n\u00famero octal como argumento.</li> </ul> </li> </ul> <pre><code>umask #Mostrar la m\u00e1scara de creaci\u00f3n de archivos actual en notaci\u00f3n octal\numask 022 #Establecer una m\u00e1scara de creaci\u00f3n de archivos que permita al propietario tener permisos completos (lectura, escritura, ejecuci\u00f3n) y al grupo y otros tener solo permisos de lectura:\numask -S #Mostrar la m\u00e1scara de creaci\u00f3n de archivos actual en notaci\u00f3n simb\u00f3lica\n</code></pre> <p>chown</p> <p>Cambiar el propietario y el grupo de archivos y carpetas en sistemas Unix y Linux.</p> <ul> <li>Principales Opciones:<ul> <li><code>R</code> (o <code>-recursive</code>): Aplica los cambios de propietario y grupo de manera recursiva a todos los archivos y carpetas dentro de una carpeta.</li> <li><code>usuario:grupo</code> (o <code>usuario</code>) especifica el nuevo propietario y, opcionalmente, el nuevo grupo para los archivos y carpetas.</li> <li>Puedes utilizar el nombre de usuario o el identificador num\u00e9rico de usuario (UID) y el nombre de grupo o el identificador num\u00e9rico de grupo (GID) como argumentos.</li> </ul> </li> </ul> <pre><code># Cambiar el propietario y el grupo de un archivo llamado \"mi_archivo.txt\" a un usuario llamado \"nuevo_usuario\" y un grupo llamado \"nuevo_grupo\"\nchown nuevo_usuario:nuevo_grupo mi_archivo.txt\n\n# Cambiar de manera recursiva el propietario y el grupo de una carpeta llamada \"mi_carpeta\" y su contenido:\nchown -R nuevo_usuario:nuevo_grupo mi_carpeta/\n</code></pre> <p>chgrp</p> <p>Cambiar el grupo de archivos y carpetas sin cambiar el propietario.</p> <ul> <li>Principales Opciones:<ul> <li><code>R</code> (o <code>-recursive</code>): Aplica los cambios de grupo de manera recursiva a todos los archivos y carpetas dentro de una carpeta.</li> </ul> </li> </ul> <pre><code>chgrp nuevo_grupo mi_archivo.txt # Cambiar el grupo de un archivo llamado \"mi_archivo.txt\" a un grupo llamado \"nuevo_grupo\"\nchgrp -R nuevo_grupo mi_carpeta/ # Cambiar de manera recursiva el grupo de una carpeta llamada \"mi_carpeta\" y su contenido\n</code></pre>"},{"location":"UD00/2.bash/#ejercicios_3","title":"\u270f\ufe0f Ejercicios","text":"<pre><code>1. Crea un usuario utilizando useradd y comprueba si se ha creado su home.\n2. Borra un usuario utilizando deluser y comprueba si se ha borrado su home.\n3. Muestra las \u00faltimas l\u00edneas de /etc/passwd\n4. Crea un grupo grupo1\n5. A\u00f1ade a los dos usuarios creados a este grupo.\n6. Borra los grupos creados\n7. Borra los usuarios creados\n</code></pre>"},{"location":"UD00/2.bash/#ayuda","title":"Ayuda","text":"<p>man</p> <p>Acceder al sistema de p\u00e1ginas del manual en Unix y Linux. Proporciona documentaci\u00f3n detallada y ayuda sobre otros comandos y utilidades disponibles en el sistema.</p> <ul> <li>Principales opciones:<ul> <li><code>man NOMBRE_COMANDO</code>: Muestra la p\u00e1gina del manual de un comando espec\u00edfico. Reemplaza \"NOMBRE_COMANDO\" con el nombre del comando del que deseas obtener informaci\u00f3n.</li> <li><code>k PALABRA_CLAVE</code>: Busca comandos relacionados con una palabra clave en la descripci\u00f3n del comando.</li> <li><code>f NOMBRE_COMANDO</code>: Muestra una descripci\u00f3n breve del comando sin acceder a la p\u00e1gina completa del manual.</li> <li><code>man -k .</code>: Muestra una lista de todos los comandos disponibles en el sistema.</li> </ul> </li> </ul> <pre><code>man ls #Ver la p\u00e1gina del manual del comando ls\nman -f grep #Mostrar una descripci\u00f3n breve del comando grep\n</code></pre> <p>tldr</p> <p>https://github.com/tldr-pages/tldr</p> <p>Proporciona versiones simplificadas y m\u00e1s f\u00e1ciles de entender de las p\u00e1ginas del manual de otros comandos. Est\u00e1 dise\u00f1ado para brindar informaci\u00f3n r\u00e1pida y concisa sobre c\u00f3mo usar comandos comunes de Unix y Linux sin necesidad de leer las p\u00e1ginas completas del manual.</p> <ul> <li>Principales opciones:<ul> <li><code>tldr NOMBRE_COMANDO</code>: Muestra una descripci\u00f3n simplificada y ejemplos de uso para el comando especificado. Reemplaza \"NOMBRE_COMANDO\" con el nombre del comando que deseas consultar.</li> <li><code>tldr --list</code>: Lista todos los comandos disponibles en el sistema para los que se proporcionan descripciones en <code>tldr</code>.</li> </ul> </li> </ul> <pre><code>tldr ls # Ver una descripci\u00f3n simplificada del comando ls con ejemplos\ntldr --list # Listar todos los comandos disponibles en tldr\n</code></pre>"},{"location":"UD00/3.ssh-soluciones/","title":"Varios: ssh - scp - screen - sshfs","text":"<p>\u270f\ufe0f Ejercicios</p> <ol> <li> <p>Modifica la configuraci\u00f3n del servidor SSH para que se pueda acceder como root. <pre><code>vi /etc/ssh/sshd_config\n\nPermitRootLogin yes\n\nservice sshd restart\n</code></pre></p> </li> <li> <p>Modifica el puerto del servicio SSH para que escuche por el 2222. <pre><code>vi /etc/ssh/sshd_config\n\nPort 22\nPort 2222\n</code></pre></p> </li> <li> <p>Realiza las modificaciones necesarias para que al conectarte al ssh de tu m\u00e1quina virtual no te pida contrase\u00f1a.</p> </li> </ol> <pre><code>1- Como usuario hadoop:\nssh-keygen -t rsa\n(Enter a todo)\n\ncd .ssh\n\nssh-copy-id -i id_rsa.pub hadoop@localhost\n</code></pre> <p>A partir de este momento, podremos ejecutar como usuario hadoop el comando: <code>ssh root@localhost</code>  Y no ser\u00e1 necesario escribir ninguna contrase\u00f1a</p>"},{"location":"UD00/3.ssh/","title":"Varios: ssh - scp - screen - sshfs","text":""},{"location":"UD00/3.ssh/#ssh","title":"ssh","text":"<p>A la m\u00e1quina virtual podemos acceder de dos formas:</p> <ol> <li>Desde la misma m\u00e1quina virtual.</li> <li>A trav\u00e9s de ssh (debe estar el servicio activo).</li> </ol> <pre><code># Para comprobar que est\u00e1 activo el servicio, desde la m\u00e1quina virtual ejecutamos\n$ ssh localhost\n$ systemctl status ssh # otra manera de comprobarlo\n\n# Si el servicio no est\u00e1 activo lo activamos con (como root)\n/sbin/service ssh start\n\n# Para que se inicie siempre que reiniciemos\nsystemctl enable ssh\n\n# ssh nombre_de_usuario@ip_maquina\n$ ssh user@192.168.64.14\n\n# para salir de la conexi\u00f3n ssh (ctrl + d) o el comando\n$ exit\n\n# Intenta acceder a la m\u00e1quina virtual como root\n$ ssh root@192.168.64.14\n</code></pre> <p>\u270f\ufe0f Ejercicios</p> <pre><code>1. Modifica la configuraci\u00f3n del servidor SSH para que se pueda acceder como root.\nEl usuario hadoop ha de poder iniciar sesi\u00f3n como root sin contrase\u00f1a.\n\n2. Modifica el puerto del servicio SSH para que escuche por el 2222.\n</code></pre>"},{"location":"UD00/3.ssh/#scp","title":"scp","text":"<p>Se utiliza para copiar archivos y directorios de forma segura entre sistemas remotos a trav\u00e9s de SSH (Secure Shell). Es especialmente \u00fatil para transferir archivos de un sistema local a uno remoto o viceversa de manera segura y eficiente.</p> <p>Principales opciones:</p> <ul> <li><code>r</code>: Copia directorios y sus contenidos de manera recursiva.</li> <li><code>P</code>: Especifica el puerto SSH a utilizar.</li> <li><code>i</code>: Permite especificar un archivo de clave privada en lugar de la clave por defecto.</li> <li><code>v</code>: Ejecuta <code>scp</code> en modo verbose para obtener informaci\u00f3n detallada sobre la transferencia.</li> </ul> <pre><code># Copiar un archivo local a un servidor remoto\nscp archivo.txt usuario@servidor.com:/ruta/destino/\n\n# Copiar un archivo remoto a la m\u00e1quina local\nscp usuario@servidor.com:/ruta/archivo-remoto.txt /ruta/local/\n\n# Copiar un directorio y su contenido de manera recursiva\nscp -r directorio_local/ usuario@servidor.com:/ruta/destino/\n</code></pre>"},{"location":"UD00/3.ssh/#screen","title":"screen","text":"<p>Permite crear sesiones de terminal multiplexadas en sistemas Unix y Linux. Esto significa que puedes iniciar m\u00faltiples sesiones de terminal dentro de una sola ventana y alternar entre ellas. Adem\u00e1s, las sesiones de <code>screen</code> pueden ejecutarse en segundo plano, lo que permite desacoplarlas de la sesi\u00f3n de terminal actual.</p> <p>La principal ventaja de <code>screen</code> es que podemos:</p> <ul> <li>Conectarnos a un servidor ssh.</li> <li>Ejecutar un proceso largo.</li> <li>Salir del servidor ssh (el proceso continua ejecut\u00e1ndose).</li> <li>Volver a conectar al servidor ssh y recuperar la ejecuci\u00f3n anterior.</li> </ul> <p>Hold a session open on a remote server. Manage multiple windows with a single SSH connection.</p> <p>Start a new screen session: <code>screen</code></p> <p>Start a new named screen session: <code>screen -S session_name</code></p> <p>Start a new daemon and log the output to <code>screenlog.x</code>:  <code>screen -dmLS session_name command</code></p> <p>Show open screen sessions: <code>screen -ls</code></p> <p>Reattach to an open screen: <code>screen -r session_name</code></p> <p>Detach from inside a screen: <code>Ctrl + A, D</code></p> <p>Kill the current screen session: <code>Ctrl + A, K</code></p> <p>Kill a detached screen: <code>screen -X -S session_name quit</code></p> <p>M\u00e1s info: https://www.youtube.com/watch?v=_ZJiEX4rmN4</p> <pre><code>$ sudo apt install screen\n$ screen \u2013version\n</code></pre> <pre><code># Iniciamos\n$ screen\n\n# Alternativa: Tambi\u00e9n podemos iniciar una sesi\u00f3n y darle un nombre\n# usando la variable -S. Por ejemplo\n$ screen -S session1\n</code></pre> <p>Comandos para realizar la multiplexaci\u00f3n de la terminal. </p>"},{"location":"UD00/3.ssh/#ejercicio","title":"Ejercicio","text":"<pre><code>- Realiza las modificaciones necesarias para que al conectarte al ssh de tu m\u00e1quina virtual no te pida contrase\u00f1a.\n</code></pre>"},{"location":"UD00/3.ssh/#sshfs","title":"sshfs","text":"<p><code>sshfs</code> nos permite montar carpetas de m\u00e1quinas remotas a trav\u00e9s de ssh.</p> <p>https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh</p> <p>Instalaci\u00f3n en m\u00e1quina remota</p> <pre><code>apt update\n\napt install sshfs\n</code></pre> <p>Montar en local</p> <pre><code>sudo mkdir /mnt/remoto\n\nsudo sshfs -o allow_other,default_permissions sammy@your_other_server:~/ /mnt/droplet\n\nsudo umount /mnt/droplet\n</code></pre> <ul> <li><code>o</code>\u00a0precedes miscellaneous mount options (this is the same as when running the\u00a0<code>mount</code>\u00a0command normally for non-SSH disk mounts). In this case, you are using\u00a0<code>allow_other</code>\u00a0to allow other users to have access to this mount (so that it behaves like a normal disk mount, as\u00a0<code>sshfs</code>\u00a0prevents this by default), and\u00a0<code>default_permissions</code>\u00a0(so that it otherwise uses regular filesystem permissions).</li> <li><code>sammy@your_other_server:~/</code>\u00a0provides the full path to the remote directory, including the remote username,\u00a0<code>sammy</code>, the remote server,\u00a0<code>your_other_server</code>, and the path, in this case\u00a0<code>~/</code>\u00a0for the remote user\u2019s home directory. This uses the same syntax as SSH or SCP.</li> <li><code>/mnt/droplet</code>\u00a0is the path to the local directory being used as a mount point.</li> </ul>"},{"location":"UD00/3.ssh/#open-vpn","title":"Open VPN","text":"<p>Script que nos facilita la instalaci\u00f3n de una VPN</p> <p>https://github.com/Nyr/openvpn-install</p>"},{"location":"UD00/4.BashScripting/","title":"Creaci\u00f3n de scripts en bash","text":""},{"location":"UD00/4.BashScripting/#introduccion","title":"Introducci\u00f3n","text":"<p>Un script es un archivo de texto que contiene una serie de comandos y/o instrucciones que se pueden ejecutar de manera secuencial para realizar una tarea espec\u00edfica o automatizar una serie de tareas. Estos scripts generalmente est\u00e1n escritos en lenguajes de scripting como Bash, Python, Perl o incluso lenguajes de programaci\u00f3n como C o C++.</p> <p><code>copia_seguridad.sh</code></p> <pre><code>#!/bin/bash\n\n# Definir directorio de origen y destino\ndirectorios_origen=\"/ruta/del/directorio/a/copiar\"\ndirectorio_destino=\"/ruta/del/directorio/de/destino\"\n\n# Crear un nombre de archivo con marca de tiempo\nfecha=$(date +\"%Y%m%d%H%M%S\")\nnombre_archivo_backup=\"backup_$fecha.tar.gz\"\n\n# Crear una copia de seguridad comprimida con tar\ntar -czvf \"$directorio_destino/$nombre_archivo_backup\" \"$directorios_origen\"\n\n# Verificar si la operaci\u00f3n de copia de seguridad fue exitosa\nif [ $? -eq 0 ]; then\n  echo \"Copia de seguridad completada con \u00e9xito en $nombre_archivo_backup\"\nelse\n  echo \"Error al realizar la copia de seguridad\"\nfi\n</code></pre> <p>Para ejecutar un script, hemos de darle permisos de ejecuci\u00f3n:</p> <pre><code>chmod +x copia_seguridad.sh # le damos permisos de ejecuci\u00f3n\n./copia_seguridad.sh # lo ejecutamos\n</code></pre>"},{"location":"UD00/4.BashScripting/#binbash","title":"#!/bin/bash","text":"<p>Al iniciar un script ponemos en la primera l\u00ednea #!/bin/bash, \u00bfPor qu\u00e9?</p> <p>La l\u00ednea #!/bin/bash al principio de un script en Bash es conocida como \"shebang\" o \"hashbang\". </p> <p>Si ejecutas un script como <code>./nombre-del-scrip.extension</code>, el sistema buscar\u00e1 en la l\u00ednea superior del archivo para determinar el int\u00e9rprete,  (generalmente <code>/bin/bash</code>) y utiliza ese programa para interpretar y ejecutar el script. Esto es especialmente \u00fatil cuando tienes varios int\u00e9rpretes de comandos disponibles en tu sistema (por ejemplo, Bash, sh, zsh, etc.), y deseas asegurarte de que el script sea ejecutado con un int\u00e9rprete espec\u00edfico.</p> <p>Al ejecutar el script como <code>bash nombre_del_script.sh</code>, la primera l\u00ednea se ignorar\u00e1.</p> <pre><code>**Ejecuci\u00f3n de un script**\n$ bash script.sh  # Ejecuci\u00f3n modo 1\n\n$ ./script.sh # Ejecuci\u00f3n modo 2 falla no tiene permisos\nzsh: permission denied: ./script.sh\n$ chmod +x script.sh\n$ ./script.sh # OK\n</code></pre>"},{"location":"UD00/4.BashScripting/#caracteres-especiales","title":"Car\u00e1cteres especiales","text":"<p># Comentarios</p> <p>Las l\u00edneas que comienzan con # (con excepci\u00f3n del primer #!) son comentarios y no ser\u00e1n interpretadas</p> <pre><code># echo \"Esta linea no se imprime por pantalla\"\n</code></pre> <p>; Separador de comandos</p> <pre><code>echo \"hola\"; ls\n\necho \"HOLA\"; \"ADIOS\"\n\nif [ -f file.txt ]; then\n    echo \"SI\"\nfi; echo \"Comparaci\u00f3n finalizada\"\n</code></pre> <p>$ Mostrar el contenido de una variable</p> <pre><code>var1=5 # sin espacios\necho $var1 #5\n</code></pre> <p>| pipe</p> <p>Redirige la salida de un comando a la entrada del siguiente, encadena \u00f3rdenes y comandos.</p> <pre><code>ls -la | wc -l\n</code></pre> <p>Otros <pre><code>${ } \u2192 \u00a0permite referenciar, modificar, o manipular variables\n$? \u2192 Hace referencia al \u00faltimo comando ejecutado, si contiene un 0 la ejecuci\u00f3n ha sido correcta, si es distinto de 0 hubo alg\u00fan error en la ejecuci\u00f3n del comando anterior.\n$! \u2192 Contiene el ID del proceso del \u00faltimo proceso subordinado. Resulta \u00fatil cuando un shellscript necesita eliminar un proceso subordinado que ha iniciado previamente.\n$_ \u2192 Devuelve el argumento final del comando previo ejecutado\n$$ \u2192 Contiene el PID del proceso\n\\ \u2192 (escape) Hace que el shell interprete el car\u00e1cter de forma literal\n\\n \u2192 Nueva l\u00ednea\n\\t \u2192 Tabulador\n&gt; &amp;&gt; &gt;&amp; &gt;&gt; &lt; \u2192 Redirecci\u00f3n\n</code></pre></p> <pre><code>name=\"Mundo\"\necho \"Hola ${name}!\" -&gt; pa\necho \"${text:0:3}\" -&gt; extracci\u00f3n texto\necho \"${var:-ValorPorDefecto}\"\n</code></pre> <pre><code>#!/bin/bash\nname=\"Usuario\"\ndirectory=\"/home/${name}/documentos\"\necho \"Hola ${name}, tu directorio es: ${directory}\"\necho \"Longitud del nombre: ${#name}\"\necho \"Tu nombre en min\u00fasculas: ${name,,}\"\u00a0 # \"usuario\"\necho \"Primeras 3 letras de tu nombre: ${name:0:3}\"\n</code></pre> <pre><code>#!/bin/bash\nsleep 30 &amp;\npid=$!\necho \"El proceso en segundo plano tiene el PID: $pid\"\nps -p $pid\n</code></pre> <pre><code>cp archivo1.txt archivo2.txt\necho \"Archivo copiado: $_\"\n</code></pre> <pre><code>#!/bin/bash\ntouch ejemplo.txt\necho \"Archivo creado: $_\"\necho \"Hola Bash\" &gt;&gt; $_\necho \"Contenido agregado a: $_\"\ncat $_\n</code></pre>"},{"location":"UD00/4.BashScripting/#variables-y-argumentos","title":"Variables y argumentos","text":""},{"location":"UD00/4.BashScripting/#variables","title":"Variables","text":"<p>Se crean mediante una asignaci\u00f3n sin espacios. El shell es \u201ccase sensitive\u201d, sensible a may\u00fasculas y min\u00fasculas (a y A son distintas) Se accede a su valor anteponiendo $</p> <pre><code>$ a=\"hola\"\n$ b=20\n$ echo $A\n$ echo $a $b\n$ echo $a \\$b\n</code></pre> <p>Variables y comillas Comillas simples, los car\u00e1cteres especiales no se interpretan. Comillas dobles, permiten la interpretaci\u00f3n de car\u00e1cteres especiales como \u201c$\u201d (permiten mostrar el valor de una variable\u201d Comillas inversas: permiten asignar el resultado de una ejecuci\u00f3n de una orden (subshell), igual que con $( )</p> <pre><code>$ a=20; b='Variable: $a'\n$ echo $b\n---\n$ a=30; b=\"Variable: $a\"\n$ echo $b\n---\nd=`date`\necho $d\n</code></pre>"},{"location":"UD00/4.BashScripting/#variables-predefinidas","title":"Variables predefinidas","text":"<p>Crea y ejecuta el siguiente script (variables_predefinidas.sh) <pre><code>echo $HOME\necho $PATH\necho $PWD\necho $RANDOM\necho $LOGNAME\necho $UID\necho $HOSTNAME\n</code></pre></p>"},{"location":"UD00/4.BashScripting/#ejercicio","title":"Ejercicio","text":"<pre><code>Busca un comando que muestre todas las variables predefinidas\n</code></pre>"},{"location":"UD00/4.BashScripting/#parametros-argumentos","title":"Par\u00e1metros - argumentos","text":"<p>Al ejecutar un script podemos acceder a cada uno de los argumentos de la llamada del mismo </p>"},{"location":"UD00/4.BashScripting/#ejercicio_1","title":"Ejercicio","text":"<p>Copia y ejecuta el siguiente script <code>argumentos.sh</code> <pre><code>#!/bin/bash\necho '$_' $_\necho '$0' $0\necho '$1' $1\necho '$2' $2\necho '$3' $3\necho '$*' $*\necho '$@' $@\necho '$#' $#\necho '$_' $_\necho '$$' $$\necho '$?' $?\n</code></pre></p> <p>Ejecuta el script utilizando diferentes argumentos: <code>./argumentos.sh hola</code> <code>./argumentos.sh hola adios</code> <code>./argumentos.sh hola 123 asdf</code></p>"},{"location":"UD00/4.BashScripting/#condicionales","title":"Condicionales","text":"<p>Permiten decidir entre una o varias opciones seg\u00fan una condici\u00f3n l\u00f3gica. <pre><code>if [ 1 = 1]; then # Se suele poner el ; as\u00ed el then no queda en la l\u00ednea de abajo\n    echo \"Son iguales\"\nfi\n##########################################\nif [ 2 = 1]; then\n    echo \"Son iguales\"\nelse\n    echo \"Son distintos\"\nfi\n</code></pre></p> <p>Hay varias formas de especificar la condici\u00f3n l\u00f3gica Instrucci\u00f3n de test b\u00e1sica <code>[]</code> \u2192 Instrucciones simples <pre><code>if [$# -lt 3] &amp;&amp; [-a \"$2\"]; then\n</code></pre></p> <p>Instrucci\u00f3n de test extendida <code>[[]]</code> \u2192 Permite combinar varias expresiones, previene la separaci\u00f3n de palabras y la expansi\u00f3n de comodines <pre><code>if [[ $# -lt 3 &amp;&amp; -a $2 ]]; then ...\n</code></pre></p>"},{"location":"UD00/4.BashScripting/#operadores-condicionales","title":"Operadores condicionales","text":"<p>Enteros </p> <p>Ficheros </p> <p>Cadenas </p> <p>Podemos utilizar &lt; y &gt; dentro del constructor [[./4.BashScripting]]</p>"},{"location":"UD00/4.BashScripting/#operadores-logicos","title":"Operadores l\u00f3gicos","text":"<p> Dentro del constructor <code>[[]]</code> se pueden utilizar &amp;&amp; y ||</p>"},{"location":"UD00/4.BashScripting/#condicional-multiple-case","title":"Condicional m\u00faltiple case","text":"<p>Copia y ejecuta el siguiente script: <pre><code>echo \"Est\u00e1s seguro (s/n)? [n]\"\nread resp\ncase ${resp} in\n    [sS]) echo \"En fin, t\u00fa lo has querido...\" ;;\n    n|N) echo \"Estupendo, menos trabajo!\";;\n    *) echo \"Voy a considerar eso como un no!\";;\nesac\n</code></pre></p>"},{"location":"UD00/4.BashScripting/#ejercicios","title":"Ejercicios","text":"<pre><code>1. Crea un script al que se le pasen 2 argumentos (n\u00fameros, no validar) y nos diga cu\u00e1l es el mayor.\n2. Modifica el script para en vez de pasar 2 argumentos nos los pida. (comando read)\n3. Crea un script al que se le pase un argumento que sea un nombre de fichero y compruebe  si existe.\n4. Crea un script al que se le pase un argumento que sea un nombre de directorio y  compruebe si existe.\n5. Crea un script al que se le pasen 2 argumentos, un fichero y un directorio y compruebe si existen (utilizando operadores l\u00f3gicos). Respuestas: Los 2 existen, Los 2 no existen.\n6. Modifica el script que hemos utilizado en el punto 3.3 para que tambi\u00e9n acepte como respuesta \u201csi\u201d \u201cSI\u201d \u201cno\u201d \u201cNO\u201d y otras variaciones\n</code></pre>"},{"location":"UD00/4.BashScripting/#bucles","title":"Bucles","text":"<p>Iteran mientras o hasta que se cumple una condici\u00f3n. La instrucci\u00f3n de test o expresi\u00f3n l\u00f3gica se expresa de la misma manera que con las condiciones, por lo que podemos hacer test de \"si existe un archivo y es ejecutable\" o \"si el archivo existe y es un enlace simb\u00f3lico\" por ejemplo.</p>"},{"location":"UD00/4.BashScripting/#while-until","title":"While - until","text":"<pre><code>while [ expresi\u00f3n\u00a0]; do\n    echo \"Comandos a ejecutar\"\ndone\n\nuntil [ expresi\u00f3n ]; do\n    echo \"Comandos a ejecutar\"\ndone\n</code></pre> <pre><code>#!/bin/bash\nvar0=0\nLIMIT=10\n\nwhile [ $var0 -lt $LIMIT ]; do\n  echo -n $var0\n  let var0++\ndone\n</code></pre>"},{"location":"UD00/4.BashScripting/#tipo-for","title":"Tipo for","text":"<p>Opci\u00f3n 1, iterar sobre una lista</p> <pre><code>for item in &lt;lista&gt;; do\n    echo \"Comandos a ejecutar\"\ndone\n</code></pre> <pre><code>for i in 1 4 6 7; do\n    echo $i\ndone\n\nfor d in $(ls); do\n    echo $d\ndone\n</code></pre>"},{"location":"UD00/4.BashScripting/#ejercicios_1","title":"Ejercicios","text":"<pre><code>1. Crea un script que nos pida 10 n\u00fameros y los sume.\n2. Crea un script que imprima y sume 50 n\u00fameros aleatorios.\n3. Crea un script que nos muestre los n\u00fameros pares del 1 al 100.\n</code></pre>"},{"location":"UD00/4.BashScripting/#utilidades-de-los-bucles","title":"Utilidades de los bucles","text":"<p>Gracias a que los bucles pueden recorrer listas, podemos ejecutarlos para recorrer la salida de otros comandos:</p> <pre><code>## Recorrer los ficheros y directorios de la carpeta actual\nfor i in $(ls); do\n  echo $i\ndone\n\n----------------------\n## Recorrer los par\u00e1metros que se le pasan.\nfor i in \"$@\"; do\n   if ! [ -d ./$i ]; then\n       echo \"Directorio \\\"$i\\\" no existe en este lugar\"\n       exit 2\n   fi\ndone\n</code></pre>"},{"location":"UD00/4.BashScripting/#ejercicios_2","title":"Ejercicios","text":"<pre><code>1. Crea un script que verifique si hay **directorios** en la carpeta actual desde donde se ejecuta el script (no hay que considerar a punto y punto punto como directorio).\n\n2. Crea un script que muestre el nombre de usuario de todos los usuarios del sistema que tengan una shell v\u00e1lida. (hay que comprobar que la shell existe)\nSi un usuario tiene como shel (/bin/xxxx) no lo mostrar\u00e1.\n</code></pre>"},{"location":"UD00/4.BashScripting/#funciones","title":"Funciones","text":"<pre><code>#!/bin/bash\n# funcion-error arg1 arg2 (imprime el mayor entre 2 valores)\nfunction error () {\n    msg=$1 # Pase de par\u00e1metros (posicional) echo $msg\n    exit 1\n}\nif [ ! $1 ] || [ ! $2 ] then\n    error \"Error! Uso correcto: mayor arg1 arg2\"\nfi\nif [ $1 -gt $2 ]; then echo $1; else echo $2 \nfi\n</code></pre> <pre><code>#!/bin/bash\n# funcion-mayor arg1 arg2 (imprime el mayor entre 2 valores)\nfunction mayor () {\n    x=$1; y=$2 # Pase de par\u00e1metros (posicional) \n    if [ $x -gt $y ]; then\n        echo $x \n    else\n        echo $y \n    fi\n    return 0 # Retorno de la orden, no valor de salida \n}\n\nresultado=$(mayor $1 $2) # Se invoca como una orden para recuperar el valor de retorno\necho $resultado\n</code></pre>"},{"location":"UD00/4.BashScripting/#otros","title":"Otros","text":""},{"location":"UD00/4.BashScripting/#obtener-valores-pipeline-desde-script","title":"Obtener valores pipeline desde script","text":"<p>https://stackoverflow.com/questions/19408649/pipe-input-into-a-script/46726373#46726373 Podemos recoger lo que nos llega de una tuber\u00eda (PIPE) en un script bash: <pre><code>printf \"stuff\\nmore stuff\\n\" &gt; test.txt\n\ncat test.txt | ./testPipe.sh\n</code></pre></p> <pre><code>#!/bin/bash\n# Check to see if a pipe exists on stdin.\n# Filename testPipe.sh\nif [ -p /dev/stdin ]; then\n        echo \"Data was piped to this script!\"\n        # If we want to read the input line by line\n        while IFS= read line; do\n                echo \"Line: ${line}\"\n        done\n        # Or if we want to simply grab all the data, we can simply use cat instead\n        # cat\nelse\n        echo \"No input was found on stdin, skipping!\"\n        # Checking to ensure a filename was specified and that it exists\n        if [ -f \"$1\" ]; then\n                echo \"Filename specified: ${1}\"\n                echo \"Doing things now..\"\n        else\n                echo \"No input given!\"\n        fi\nfi\n</code></pre>"},{"location":"UD00/4.BashScripting/#comandos-avanzados","title":"Comandos avanzados","text":""},{"location":"UD00/4.BashScripting/#grep","title":"grep","text":"<p>Ya no se utiliza egrep Busca palabras o patrones en el contenido de uno o varios ficheros y vuelca por su salida las l\u00edneas que contienen dichas palabras o patrones.</p> <pre><code>grep [-inHv] patr\u00f3n [ficheros(s)]\n\n-i ignora la diferencia entre may\u00fasculas y min\u00fasculas \n-n imprime la l\u00ednea del fichero\n-H imprime el nombre del fichero\n-v b\u00fasqueda inversa (l\u00edneas que no contienen el patr\u00f3n)\n\n**tldr**\n- Search for a pattern within a file:\n    grep \"search_pattern\" path/to/file\n\n- Search for an exact string (disables regular expressions):\n    grep --fixed-strings \"exact_string\" path/to/file\n\n- Search for a pattern in all files recursively in a directory, showing line numbers of matches, ignoring binary files:\n    grep --recursive --line-number --binary-files=without-match \"search_pattern\" path/to/directory\n\n- Use extended regular expressions (supports `?`, `+`, `{}`, `()` and `|`), in case-insensitive mode:\n    grep --extended-regexp --ignore-case \"search_pattern\" path/to/file\n\n- Print 3 lines of context around, before, or after each match:\n    grep --context|before-context|after-context=3 \"search_pattern\" path/to/file\n\n- Print file name and line number for each match with color output:\n    grep --with-filename --line-number --color=always \"search_pattern\" path/to/file\n\n- Search for lines matching a pattern, printing only the matched text:\n    grep --only-matching \"search_pattern\" path/to/file\n\n- Search `stdin` for lines that do not match a pattern:\n    cat path/to/file | grep --invert-match \"search_pattern\"\n</code></pre> <p>Ejemplos:</p> <pre><code>- Busca la palabra \u201cnombre\u201d en los ficheros con extensi\u00f3n txt: \n$ grep -Hn nombre *.txt\n\n- Busca las l\u00edneas que comienzan por \u201ca\u201d o \u201cc\u201d may\u00fasculas o min\u00fasculas: \n$ grep -iHn ^[ac] *.txt\n\n- Busca l\u00edneas que terminan por una coma: \n$ grep -Hn ,$ *.txt\n\n- Busca l\u00edneas que no contienen un punto: \n$ grep -Hnv '\\.' *.txt\n</code></pre> <p>El car\u00e1cter \u201c.\u201d tiene un significado especial para el shell y para grep. Las comillas simples evitan la interpretaci\u00f3n por el shell, y la \u201c\\\u201d por grep.</p>"},{"location":"UD00/4.BashScripting/#find","title":"find","text":"<p>Permite explorar el sistema de archivos - Habitualmente se utiliza para localizar aquellos ficheros o directorios que cumplen con una condici\u00f3n. - Adicionalmente, pueden ejecutar acciones sobre ellos.</p> <p>Es \u00fatil en combinaci\u00f3n con bucles for o while.</p> <pre><code>**tldr**\n\n- Find files by extension:\n    find root_path -name '*.txt'\n\n- Find files matching multiple path/name patterns:\n    find root_path -path '**/path/**/*.ext' -or -name '*pattern*'\n\n- Find directories matching a given name, in case-insensitive mode:\n    find root_path -type d -iname '*lib*'\n\n- Find files matching a given pattern, excluding specific paths:\n    find root_path -name '*.py' -not -path '*/site-packages/*'\n\n- Find files matching a given size range, limiting the recursive depth to \"1\":\n    find root_path -maxdepth 1 -size +500k -size -10M\n\n- Run a command for each file (use `{}` within the command to access the filename):\n    find root_path -name '*.txt' -exec wc -l {} \\;\n\n- Find files modified in the last 7 days:\n    find root_path -daystart -mtime -7\n\n- Find empty (0 byte) files and delete them:\n    find root_path -type f -empty -delete\n</code></pre> <p>Ejemplos:</p> <pre><code>find . -type f -print\n\nfind . -name \\*.txt -exec basename {} \\;\n\n------- OTROS -------\n\nfind . -empty -type d\nfind . -empty -type f\nfind . -size +10M\n  * c: bytes, k:Kb, M:Mb, G:Gb\nfind . -type f -mmin -5\nfind . -perm 777\nfind . -maxdepth 1 -type f\n\nhttps://ciberninjas.com/15-ejemplos-del-comando-find-en-linux/\n</code></pre> <p>Ejecuta el siguente script <code>find-bucles.sh</code></p> <pre><code>#!/bin/bash\n# find-bucles.sh, 3 versiones alternativas\ndir=$HOME\n\necho \"Versi\u00f3n 1 ----------------------------\"\nfind $dir -name \\*.txt -exec grep -inH nombre {} \\;\n\necho \"Versi\u00f3n 2 ----------------------------\" \nfind $dir -name \\*.txt  -print | while read f do\n       grep -inH nombre $f\ndone\n\necho \"Versi\u00f3n 3 ----------------------------\" \noldifs=$IFS\nIFS=$'\\n'\nfor f in $(find $dir -name \\*.txt  -maxdepth 1 -print) do\n       grep -inH nombre $f\ndone\nIFS=$oldifs\n</code></pre> <p>Contesta a las siguientes preguntas: <pre><code>1. \u00bfQu\u00e9 diferencia hay en cada una?\n2. \u00bfPara qu\u00e9 se ha utilizado \u201cIFS\u201d?\n</code></pre></p>"},{"location":"UD00/4.BashScripting/#ejercicios_3","title":"Ejercicios","text":"<p>Creaci\u00f3n de ficheros y directorios para hacer las pr\u00e1cticas, ejecuta cada uno de los siguientes comandos para preparar el entorno para los ejercicios:</p> <pre><code>mkdir ~/find_grep\ncd ~/find_grep\n\nwget https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt\nmv el_quijote.txt ..\n\nmkdir dir{uno,2,tres,4,5}\nls\n\nsplit -n 50 -d ../el_quijote.txt quijote\nfor f in ./quijote??; do mv $f diruno/\"$f.txt\"; done\n\nsplit -n 20 -d ../el_quijote.txt el_qui\nfor f in ./el_qui??; do mv $f dir2/\"$f.doc\"; done\n\nwget https://gutenberg.org/cache/epub/84/pg84.txt -O ../Frankenstein.txt\n\nsplit -n 60 -d ../Frankenstein.txt Frankenstein.libro.completo.\nfor f in Frankenstein.libro.completo.??; do mv $f dirtres/\"$f.txt.avi\"; done\n\n## Dir 4\nCorta el libro de Frankenstein en fragmentos de 1000 l\u00edneas, mu\u00e9velos a la carpeta \ndir4,\nel nombre de archivo ser\u00e1: \"The_modern_Prometheus\" seguido de la numeraci\u00f3n \nque pone split\nCambia o pon la extensi\u00f3n .docx\n\ntouch sin_contenido.txt\ndd if=/dev/urandom of=fich20mb bs=1M count=20\n</code></pre> <p>EJERCICIOS find - grep</p> <pre><code>#### FIND\n1. Busca el archivo Frankenstein.libro.completo.31.txt.avi\n2. Buscar el archivo QUIJOTE14.txt sin importar si est\u00e1 escrito en may\u00fasculas o min\u00fasculas\n3. Busca todos los archivos que terminen en .doc o en .txt\n4. Busca directorios vac\u00edos\n5. Busca si existe alg\u00fan fichero vac\u00edo\n6. Busca si existe alg\u00fan fichero de m\u00e1s de 10 Mb\n7. Ejecuta el comando `touch archivo.txt`\n     Busca los archivos que se han modificado en los \u00faltimos 5 minutos\n8. Ejecuta el comando `chmod 777 dirtres/Frankenstein.libro.completo.18.txt.avi`\n     Busca alg\u00fan archivo con permisos 777\n9. Busca los archivos del directorio actual\n10. Ejecuta:\n        mkdir dir6\n      cd dir6\n    touch archivo{0..100}\n    ls\n    A\u00f1ade la extensi\u00f3n .txt a todos estos archivos\n\n#### GREP\n\n#### FIND + GREP\n</code></pre>"},{"location":"UD00/4.BashScripting/#resumen","title":"Resumen","text":""},{"location":"UD00/4a.BashVarios/","title":"Bash Varios","text":""},{"location":"UD00/4a.BashVarios/#comando-sudo","title":"Comando: sudo","text":""},{"location":"UD00/4a.BashVarios/#definicion-uso-tldr","title":"Definici\u00f3n, uso, tldr","text":""},{"location":"UD00/4a.BashVarios/#ejemplos","title":"Ejemplos","text":"<p>Otras cosas que explicar \u00bfC\u00f3mo permitir a un usuario que ejecute sudo? \u00bfQu\u00e9 significa !! ?</p>"},{"location":"UD00/4a.BashVarios/#uso-de-tuberias-pipes","title":"Uso de tuber\u00edas (pipes)","text":""},{"location":"UD00/4b.BashEjercicios-soluciones/","title":"Ejercicios Bash","text":""},{"location":"UD00/4b.BashEjercicios-soluciones/#ejercicio-1-solucion","title":"Ejercicio 1 Soluci\u00f3n","text":"<p>Falta revisar. ```bash     #!/bin/bash     if [ $# -ne 2 ]; then         echo \"Error parametros\"         exit 1     fi     # Comprobar puedo leer fichero     if [[ ! -r $1 || ! -r $2 ]]; then         echo 'e'     fi     #no funciona en mac, problema espacios al principio     #LINEAS_UNO=$( wc -l $1 | cut -d \" \" -f 1)     LINEAS_UNO=$( cat $1 | wc -l | tr -d \" \")     LINEAS_DOS=$( cat $2 | wc -l | tr -d \" \")     CAR_UNO=$( cat $1 | wc -c | tr -d \" \")     CAR_DOS=$( cat $2 | wc -c | tr -d \" \")</p> <pre><code>if [[ $LINEAS_UNO -gt $LINEAS_DOS ]]; then\n    echo \"$1 ($LINEAS_UNO) tiene m\u00e1s l\u00edneas que $2 ($LINEAS_DOS)\"\nelif [[ $LINEAS_UNO -lt $LINEAS_DOS ]]; then\n    echo \"$1 ($LINEAS_UNO) tiene menos l\u00edneas que $2 ($LINEAS_DOS)\"\nelse\n    echo \"$1 ($LINEAS_UNO) tiene las mismas l\u00edneas que $2 ($LINEAS_DOS)\"\nfi\n\nexit 0\n```\n</code></pre>"},{"location":"UD00/4b.BashEjercicios-soluciones/#ejercicio-2-solucion","title":"Ejercicio 2 Soluci\u00f3n","text":""},{"location":"UD00/4b.BashEjercicios-soluciones/#ejercicio-3-solucion","title":"Ejercicio 3 Soluci\u00f3n","text":"<p>```bash     #!/bin/bash</p> <pre><code>echo -e \"Nombre a buscar:\"\nread usuario\n\negrep ${usuario}: /etc/passwd &amp;&gt;/dev/null\n```\n</code></pre>"},{"location":"UD00/4b.BashEjercicios-soluciones/#ejercicio-4-solucion","title":"Ejercicio 4 Soluci\u00f3n","text":""},{"location":"UD00/4b.BashEjercicios-soluciones/#ejercicio-5-solucion","title":"Ejercicio 5 Soluci\u00f3n","text":""},{"location":"UD00/4b.BashEjercicios-soluciones/#ejercicio-6-solucion","title":"Ejercicio 6 Soluci\u00f3n","text":""},{"location":"UD00/4b.BashEjercicios-soluciones/#ejercicio-7-solucion","title":"Ejercicio 7 Soluci\u00f3n","text":""},{"location":"UD00/4b.BashEjercicios-soluciones/#ejercicio-8-solucion","title":"Ejercicio 8 Soluci\u00f3n","text":""},{"location":"UD00/4b.BashEjercicios-soluciones/#ejercicio-9-solucion","title":"Ejercicio 9 Soluci\u00f3n","text":""},{"location":"UD00/4b.BashEjercicios-soluciones/#ejercicio-10-solucion","title":"Ejercicio 10 Soluci\u00f3n","text":""},{"location":"UD00/4b.BashEjercicios/","title":"Ejercicios Bash","text":""},{"location":"UD00/4b.BashEjercicios/#ejercicio-1","title":"Ejercicio 1","text":"<p>Crear un programa al que se le pasen 2 ficheros de texto y diga cu\u00e1l es el que tiene m\u00e1s l\u00edneas.</p> <ul> <li>Debe asegurarse que se le pasan 2 argumentos, que son ficheros de texto y que existen.</li> <li>PISTA: comando <code>wc</code></li> </ul> <pre><code>#### Lo ejecutaremos de la siguiente manera:\n\n./cuenta_lineas.sh fichero1 fichero2\n</code></pre> <p>1b. Modifica el programa para que, si no le pasamos ning\u00fan par\u00e1metro nos pida el nombre de los 2 ficheros.</p>"},{"location":"UD00/4b.BashEjercicios/#ejercicio-2","title":"Ejercicio 2","text":"<p>Realiza un script que lea un valor entero que representa una nota hasta que se escriba la palabra SALIR.</p> <ul> <li>compruebe si est\u00e1 en el rango adecuado (0 a 10)</li> <li>Si lo est\u00e1, debe imprimirlo en modo texto (suspendido, suficiente, notable, sobresaliente...)</li> <li>Si no lo est\u00e1, debe mostrar un mensaje de error.</li> <li>Si no se introduce ning\u00fan n\u00famero, el script debe asumir un valor por defecto de 0.</li> </ul>"},{"location":"UD00/4b.BashEjercicios/#ejercicio-3","title":"Ejercicio 3","text":"<p>Crea un script que pregunte por un nombre de usuario y que devuelva por pantalla si existe o no en el sistema.</p>"},{"location":"UD00/4b.BashEjercicios/#ejercicio-4","title":"Ejercicio 4","text":"<p>Crear una tarea peri\u00f3dica para que cada 1 minutos, cree un archivo en /tmp/informe El nombre del archivo ser\u00e1, anyo-mes-dia_hora:minutos:segundos.txt En el archivo habr\u00e1 informaci\u00f3n del sistema en ese momento:  - total procesos en ejecuci\u00f3n, memoria disponible, almacenamiento, total de usuarios logueados, etc... Tambi\u00e9n contendr\u00e1 informaci\u00f3n sobre si hay determinados procesos en marcha en ese momento:  - sshd  - systemd  - top</p>"},{"location":"UD00/4b.BashEjercicios/#ejercicio-5","title":"Ejercicio 5","text":"<p>Lee una secuencia de l\u00edneas de su entrada enviadas desde una tuber\u00eda mediante <code>cat fichero</code> - Divide las l\u00edneas en palabras - Elimina los posibles signos de puntuaci\u00f3n (comas, puntos, exclamaciones) - Muestra cada una de las palabras</p>"},{"location":"UD00/4b.BashEjercicios/#ejercicio-6","title":"Ejercicio 6","text":"<p>Realiza una copia de seguridad de los archivos de $HOME</p> <ul> <li>La copia debe hacerse en /tmp/$HOME/</li> <li>Si es la primera vez que se hace el backup, deben copiarse todos los directorios y archivos</li> <li>Pero si ya hay una copia previa, s\u00f3lo deben copiarse los directorios y archivos que sean nuevos (o modificados) desde la \u00faltima copia. Para ello, utiliza un fichero oculto en /tmp/$HOME, que act\u00fae de marca temporal.</li> </ul> <p>Sugerencia: Se sugiere crear primero los directorios (mkdir) y luego copiar los archivos (cp). En ambos casos, estudia la utilizaci\u00f3n de la opci\u00f3n -p</p>"},{"location":"UD00/4b.BashEjercicios/#ejercicio-7","title":"Ejercicio 7","text":"<p>Implementa un script que imprima un listado de los usuarios que tienen procesos en ejecuci\u00f3n, indicando para cada uno:</p> <ul> <li>La cantidad de procesos que esta\u0301 ejecutando</li> <li>La suma de sus consumos actuales de CPU (en %)</li> <li>La suma de sus consumos actuales de memoria (en %)</li> </ul> <p>Sugerencia: Se sugiere utilizar la orden ps para averiguar la informaci\u00f3n de base.</p>"},{"location":"UD00/4b.BashEjercicios/#ejercicio-8","title":"Ejercicio 8","text":"<p>Implementa un script que se ejecute cada 5 minutos y compruebe que siguen ejecut\u00e1ndose los procesos necesarios para hadoop y yarn.  Si al ejecutarse no encuentra alguno de los procesos necesarios ha de dejar un registro (log) Si al ejecutarse encuentra todos los procesos necesarios tambi\u00e9n ha de indicarlo en el log. Pistas:    - Ejecutar un script cada X minutos o segundos\u2026 utilizaremos \"cron\"        - https://nksistemas.com/crear-un-script-simple-y-ejecutarlo-con-cron-crontab-en-tu-linux/      - Para ayudarnos a generar el cron correctamente: https://crontab.guru/    - Procesos activos en el sistema <code>ps -aux</code> , o para hadoop <code>jps</code>    - <code>bash -c \"exec -a &lt;MyProcessName&gt; &lt;Command&gt;\"</code></p> <p>Ejemplo de log creado: <pre><code>  lun 11 dic 2023 18:10:00 CET     Ejecuci\u00f3n correcta [OK]\n  lun 11 dic 2023 18:15:00 CET     Error NodeManager  [ERROR]\n  lun 11 dic 2023 18:15:00 CET     Error SecondaryNameNode  [ERROR]\n  lun 11 dic 2023 18:15:00 CET     Error ResourceManager  [ERROR]\n  lun 11 dic 2023 18:20:00 CET     Ejecuci\u00f3n correcta [OK]\n</code></pre></p>"},{"location":"UD00/4b.BashEjercicios/#ejercicio-9","title":"Ejercicio 9","text":"<p>Mandar los logs del script anterior por telegram. 1. Crear bot con BotFather Obtener token HTTP API 6482272093:AAEWWA3hZ0O6t4tC4TCpP5ewPVSjpQNL0jj 2. Entrar en el bot Start bot 3. Obtener chatID a trav\u00e9s de una URL     https://api.telegram.org/bot${token}/getUpdates 4. Obtener chatID https://ungineer.github.io/chatid.html 251175583 5. Mandar mensaje a trav\u00e9s de bash: <pre><code>API_TOKEN=\"&lt;your_api_token&gt;\"\nCHAT_ID=\"&lt;your_chat_id&gt;\"\n\n# Set the message text\nMESSAGE=\"This is a test message\"\n\n# Use the curl command to send the message\ncurl -s -X POST https://api.telegram.org/bot$API_TOKEN/sendMessage -d chat_id=$CHAT_ID -d text=\"$MESSAGE\"\n</code></pre></p>"},{"location":"UD00/4b.BashEjercicios/#ejercicio-10","title":"Ejercicio 10","text":"<p>Un sistema inform\u00e1tico almacena en varios ficheros \u201clog\u201d los usuarios y la fecha de conexi\u00f3n (usuario:mes:dia).</p> <p>Hay que hacer un script al que se le puedan pasan 2 par\u00e1metros, el primero un nombre de usuario y el segundo un fichero log y comprobar\u00e1 (mostrando por pantalla) cu\u00e1ntos accesos ha realizado el usuario el mes actual.</p> <p>Control de errores:</p> <ul> <li>2 par\u00e1metros requeridos</li> <li>El fichero de logs no existe</li> </ul> <p>Ejemplo datos.log</p> <pre><code>pedro:ENE:21\nana:JUN:13\njuan:ABR:16\nroberto:JUN:01\njuan:MAR:13\npedro:FEB:01\nlucas:ENE:07\npedro:JUN:15\nroberto:JUN:02\nana:JUN:13\nlucas:ENE:07\njuan:JUN:16\npedro:MAY:01\njuan:MAR:13\n</code></pre> <p>Ejemplos de ejecuci\u00f3n <pre><code>## Teniendo en cuenta que el mes actual es ENERO\n./conexiones.sh ana datos.log\n\nEl usuario ana se ha conectado este mes 0 veces.\n./conexiones.sh pedro datos.log\n\nEl usuario pedro se ha conectado este mes 1 vez.\n</code></pre></p>"},{"location":"UD00/4c.BashChuleta/","title":"Chuleta bash","text":""},{"location":"UD00/4c.BashChuleta/#variables","title":"Variables","text":"<pre><code>A=10\necho $A\necho ${A}DIOS\necho ${#A} // devuelve 2 (longitud)\nunset A\n\nb=\u201cHOLA\u201d; echo ${#b) -&gt; longitud de la variable (4)\n\nlocal A -&gt; variable local dentro de una funci\u00f3n\n\nreadonly A \nA=20\n     -bash: A: readonly variable\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#variables-de-entorno","title":"Variables de entorno","text":"<p> Para ver todas las variables que est\u00e1n definidas se puede utilizar el comando <code>env</code></p>"},{"location":"UD00/4c.BashChuleta/#ejecucion-scripts","title":"Ejecuci\u00f3n scripts","text":""},{"location":"UD00/4c.BashChuleta/#permisos","title":"Permisos","text":"<pre><code>chmod u+x nombre_script.sh\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#parametros-argumentos","title":"Par\u00e1metros, argumentos","text":"<p> <pre><code>$0 -&gt; nombre script\n$# -&gt; n\u00famero de par\u00e1metros\n$* -&gt; todos los par\u00e1metros vistos como una palabra\n$@ -&gt; todos los par\u00e1metros posicionales, cada uno est\u00e1 entrecomillado\n</code></pre></p>"},{"location":"UD00/4c.BashChuleta/#redirecciones","title":"Redirecciones","text":"<pre><code>ls 2&gt; fichero    ls 2&gt;/dev/null -&gt; Redirecci\u00f3n salida error\nls 2&gt;&amp;1 -&gt; Redirecci\u00f3n salida error a est\u00e1ndard (muestra el error, como si no estuviese el 2)\nls /aa &amp;&gt; fich -&gt; todas las salidas a fich \nls /aa &amp;&gt; /dev/null -&gt; Ocultar todas las salidas, error y OK\nls /saa &lt; fich // redirecci\u00f3n entrada\nls /saaa &amp;&gt;fich // redirecci\u00f3n salida est\u00e1ndar y errores\n\nORDEN DE EVALUACI\u00d3N\nRedirecci\u00f3nes E/S\nSustituci\u00f3n, expansi\u00f3n variables\nSustituci\u00f3n, expansi\u00f3n nombres ficheros\n\ncmd &lt; fichero -&gt; coge como entrada standard un fichero\n\ncat &lt;&lt; end\n   Escriure el que vols\u2026\nend\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#entrada-salida-de-datos","title":"Entrada Salida de datos","text":""},{"location":"UD00/4c.BashChuleta/#entrada","title":"Entrada","text":"<pre><code>echo \u00adn \"Escriu DNI i lletra separats per un espai\" \nread DNI LLETRA\n\nread -p \"Type something: \" text   // -p mostrar prompt -e linea entera\nType something: josep garcia  // se guarda en $text\n\nread -p \"PASS\" -s VAR  // s-&gt; silence\nPASS: // no se muestran caracteres que se escriben, guarda en VAR\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#control-de-flujo","title":"Control de flujo","text":""},{"location":"UD00/4c.BashChuleta/#bucles","title":"Bucles","text":"<pre><code>seq valor_inicio   valor_fin\nseq valor_inicio   incremento valor_fin\n\nfor i in $(seq 1 10); do echo $i; done #1 2 3 4 5 6 7 8 9 10\nfor i in $(seq 1 2 10); do echo $i; done #1 3 5 6 9\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#filtros","title":"Filtros","text":""},{"location":"UD00/4c.BashChuleta/#tr","title":"tr","text":"<pre><code>TR (significa: translate o transliterate)\n-s sustituye conjunto car\u00e1cteres repetidos por uno solo\n-c sustituye car\u00e1cteres espec\u00edficos: tr -c \u201c[a-z]\u201d x -&gt; traduce a x\n-d borra: tr -d [a-z]\ncat datos_demo.txt | tr -s \" \" | cut -f6 -d \"#\"\n\n# Cambia espacios por \\n\necho \u201ca b c d e\u201d | tr \u2018 \u2019 \u2018\\n\u2018 \n\n# BORRA la aparici\u00f3n de localhost en el fich\n# cat /etc/hosts | tr -d \u2018localhost'\n\n# Deja un solo :\n# echo \u201cAA::::::::B\u201d | tr -s \u201c:\u201d -&gt; AA:B\n\n# convertir a min\u00fasculas\necho \"EaaaEM\" | tr '[:upper:]' \u2018[:lower:]\u2019\necho \"EaaaEM\" | tr \"a-z\" \"A-Z\"\n#\ny=\"this Is A test\"\necho \"${y^^}\" -&gt; THIS IS A TEST\necho \"${y,,}\" -&gt; this is a test\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#sed","title":"sed","text":"<pre><code>sed \u2019s/a_sustitutir/a_sustituto/g\u2019 fich.txt\n\n** SED reemplaza cadenas, TR car\u00e1cteres\necho \"good good\" | sed 's/good/bad/g'\nbad bad\necho \"good good\" | tr \u2019good\u2019 \u2018x'\nxxxx xxxx\n\nObtener l\u00ednea 2: sed -n 2l fich.txt\nObtener todos menos 12 y 18: sed 12,18d fich.txt\nBorrar l\u00edneas en blanco: sed \u2019s/^$ /d\u2019 fich.txt\nNo mostrar l\u00edneas que tengan hola: sed \u2018/hola/d\u2019 fich.txt\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#cut","title":"cut","text":"<pre><code>CUT\n-d car\u00e1cter delimitador\n-b,-c,-f corta por bytes, car\u00e1cteres o campos\n\ncut -f6- -d \u201c#\u201d -&gt; saca la 6 y la siguiente\ncut -f1-6 -d \u201c#\" -&gt; de la 1 a la 6\ncut -f1,6 -d \u201c#\" -&gt; la 1 y la 6\n\n# cat datos_demo.txt | cut -f1-3 -d \u201c#\"\n1#josep#garcia\n2#ana#martin\n3#olivia#garcia\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#head","title":"head","text":"<pre><code>HEAD -&gt; Muestra 10 primeras lineas\nHead -n2 fich.txt -&gt; muestra 2 primeras l\u00edneas\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#tail","title":"tail","text":"<pre><code>Muestra las 10 \u00faltimas l\u00edneas\nTail -n3 fich.txt -&gt; muestra las 3 \u00faltimas\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#sort","title":"sort","text":"<pre><code>SORT -&gt; ordenar\n-f no distingue May\u00fas/min\u00fas\n-b (ignora blancos al principio l\u00ednea)\n-r (orden inverso)\n\nFitxer:\naa aabb:x\naaa abb:y\naaa aaa:z\n\ncat fich | sort -k3n -t\u201d:\u201d\nn-&gt; compara con campo num\u00e9rico -&gt;&gt; orden num\u00e9rico\nK3 -&gt; seg\u00fan tercer campo (campos seg\u00fan espacios)\n-t\u201d:\u201d -&gt; campos delimitados por dos puntos\n</code></pre> <pre><code># cat ordenar.txt | sort -k1\naa aabb:x\naaa aaa:z\naaa abb:y\n# cat ordenar.txt | sort -k2\naaa aaa:z\naa aabb:x\naaa abb:y\n# cat ordenar.txt | sort -k2 -t\":\"\naa aabb:x\naaa abb:y\naaa aaa:z\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#uniq","title":"uniq","text":"<p>Descarta repeticiones de l\u00ednea sucesivas</p> <pre><code>Fitxer:\naaa\nasf\naaa\nasfasdf\nasdff\naaa\naaa\n\n# cat unicos.txt | uniq\naaa\nasf\naaa\nasfasdf\nasdff\naaa\n</code></pre> <pre><code># cat unicos.txt | uniq -c -&gt; cuenta ocurrencias\n      1 aaa\n      1 asf\n      1 aaa\n      1 asfasdf\n      1 asdff\n      2 aaa\n\n# cat unicos.txt | uniq -u -&gt; muestra l\u00edneas \u00fanicas\naaa\nasf\naaa\nasfasdf\nasdff\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#operaciones-con-ficheros","title":"Operaciones con ficheros","text":""},{"location":"UD00/4c.BashChuleta/#comprimir","title":"Comprimir","text":"<pre><code>gzip fichero.txt -&gt; fichero.txt.gz\n\nbzip bigfile -&gt; bigfile.bz2\n\ntar cfz bigfile.tgz bigfile\n\nzip ./bigfile.zip bigfile\n\ntar -czvf archive.tar.gz /home/ubuntu --exclude=*.mp4\n\n#Varios directorios i ficheros\ntar -czvf archive.tar.gz /home/ubuntu/Downloads /usr/local/stuff /home/ubuntu/Documents/notes.txt\n</code></pre>"},{"location":"UD00/4c.BashChuleta/#descomprimir","title":"Descomprimir","text":"<pre><code>tar xf bigfile.tgz\ntar -xzvf archive.tar.gz\n\nunzip bigfile.zip\ngunzip bigfile.gz\nbunzip2 bigfile.gz2\n</code></pre>"},{"location":"UD00/7.docker/","title":"Repaso docker","text":""},{"location":"UD00/7.docker/#imprescindibles","title":"Imprescindibles","text":"<ul> <li>\u00bfQu\u00e9 es docker? Imagen vs contenedor </li> <li>Curso de docker</li> <li>Creaci\u00f3n de im\u00e1genes</li> </ul>"},{"location":"UD00/7.docker/#arquitectura","title":"Arquitectura","text":""},{"location":"UD00/7.docker/#instalacion-en-ubuntu","title":"Instalaci\u00f3n en Ubuntu","text":"<p>https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-20-04-es</p> <p>Requisitos previos:</p> <ul> <li>apt-transport-https:\u00a0permite que el administrador de paquetes transfiera datos a trav\u00e9s de https</li> <li>ca-certificates:\u00a0permite que el navegador web y el sistema verifiquen los certificados de seguridad</li> <li>curl:\u00a0transfiere datos (similar a wget)</li> <li>software-properties-common:\u00a0agrega scripts para administrar el software</li> </ul> <pre><code>sudo apt-get install  curl apt-transport-https ca-certificates software-properties-common\n</code></pre> <p>Agregamos repositorio</p> <pre><code># Primero clave GPG\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n\nsudo apt update\n\nsudo apt install docker-ce\n\nsudo systemctl status docker\n</code></pre> <p>Por defecto, el comando docker solo puede ser ejecutado por el usuario root o un usuario del grupo docker, que se crea autom\u00e1ticamente durante el proceso de instalaci\u00f3n de Docker.</p> <p>Para evitar escribir sudo al ejecutar el comando docker, agregue su nombre de usuario al grupo docker:</p> <pre><code>sudo usermod -aG docker ${USER}\n\n# Cerramos y abrimos sesi\u00f3n de nuevo o ejecutamos\nsu - ${USER}\n\n# Confirmamos los grupos de nuestro usuario\nid -nG\n</code></pre>"},{"location":"UD00/7.docker/#comandos-basicos","title":"Comandos b\u00e1sicos","text":""},{"location":"UD00/7.docker/#gestion-de-imagenes","title":"Gesti\u00f3n de imagenes","text":"<pre><code>docker image\ndocker history\ndocker inspect\ndocker save/load\ndocker rmi\n</code></pre>"},{"location":"UD00/7.docker/#gestion-de-contenedores","title":"Gesti\u00f3n de contenedores","text":"<pre><code>docker attach\ndocker exec\ndocker inspect\ndocker kill\ndocker logs\ndocker pause/unpause\ndocker port\ndocker ps\ndocker rename\ndocker start/stop/restart\ndocker rm\ndocker run\ndocker stats\ndocker top\ndocker update\n</code></pre>"},{"location":"UD00/7.docker/#ejemplo","title":"Ejemplo","text":"<pre><code># Ver los contenedores que tenemos\ndocker ps\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\n# Ver las imagenes que tenemos\ndocker images\nREPOSITORY                             TAG       IMAGE ID       CREATED        SIZE\n\n# Crear un contenedor con una imagen b\u00e1sica de debian\n# Como no tenemos ninguna imagen de debian, la descarga y la ejecuta\ndocker run debian\n# Intentamos ver el contenedor en ejecuci\u00f3n, no aparece nada porque ya se ha cerrado\ndocker ps\n# Podemos verlo con\ndocker ps -a\nCONTAINER ID   IMAGE     COMMAND       CREATED              STATUS                          PORTS     NAMES\n09b14daab800   debian    \"bash\"        2 seconds ago        Exited (0) 1 second ago                   pensive_wozniak\n\n# Ejecutar un comando en un contenedor\ndocker run debian /bin/echo \"Hello World\"\nHello World\n\n# Informaci\u00f3n\ndocker inspect debian\n</code></pre>"},{"location":"UD00/7.docker/#crear-contenedor-interactivo-y-con-nombre","title":"Crear contenedor interactivo y con nombre.","text":"<p>https://jolthgs.wordpress.com/2019/09/25/create-a-debian-container-in-docker-for-development/</p> <p>Para que docker no se invente un nombre como \u201cpensive_wozniak\u201d (comando anterior) podemos definir el nombre que queremos.</p> <p>Utilizaremos una de las im\u00e1genes de: https://hub.docker.com/_/debian/tags</p> <pre><code># Obtenemos la imagen, en el apartado aterior la hemos ejecutado directamente con \"run\", esto\n# la obtiene impl\u00edcitamente. En este caso la vamos a descargar.\ndocker pull debian:10-slim\n\n# --name\n# -h hostname que tendr\u00e1 el contenedor\n# -e codificaci\u00f3n de caracteres\n# -it modo interactivo\n# /bin/bash -l  la shell que se ejecutar\u00e1\ndocker run --name debian-mini -h equipo1 -e LANG=C.UTF-8 -it debian:10-slim /bin/bash -l\n\n# Una vez dentro del contenedor podemos actualizarlo e instalar los paquetes que creamos necesarios\napt update &amp;&amp; apt upgrade --yes &amp;&amp; apt install sudo locales --yes\n# Configurar timezone\ndpkg-reconfigure tzdata\n\n# Vamos a nuestra home y creamos un archivo\ncd\necho \"hola\" &gt; prueba.txt\n\n# Salimos del contenedor\nexit (o control + d)\n\n---\ndocker ps\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\ndocker ps -a\nCONTAINER ID   IMAGE            COMMAND          CREATED          STATUS                      PORTS     NAMES\n97d8dc048093   debian:10-slim   \"/bin/bash -l\"   2 minutes ago    Exited (0) 24 seconds ago             debian-mini\n</code></pre> <p>Una vez personalizado el contenedor podremos  </p>"},{"location":"UD00/7.docker/#imagenes","title":"Imagenes","text":"<p>Es un instalador donde podemos incorporar nuestra aplicaci\u00f3n. Es el punto de inicio para crear contenedores. Hay im\u00e1genes oficiales de por ejemplo Ubuntu, Apache, etc, que fueron creadas por sus creadores oficiales.</p> <p>P\u00e1gina oficial para im\u00e1genes: https://hub.docker.com </p> <p>Vamos a utilizar la siguiente imagen para pruebas:</p> <p>https://hub.docker.com/_/hello-world</p> <p>Para ejecutar este contenedor \u201chello-word\u201d escribimos en la terminal:</p> <pre><code>docker run hello-world\n</code></pre> <p>Una vez ejecutado, ya dispondremos de la imagen descargada, podemos ver todas las im\u00e1genes que tenemos descargadas con:</p> <pre><code>docker images\n\nREPOSITORY                              TAG           IMAGE ID       CREATED        SIZE\nhello-world                             latest        ee301c921b8a   9 months ago   9.14kB\n</code></pre> <p>Desde la p\u00e1gina web de docker hub, podemos ver diferentes versiones de la misma imagen en la pesta\u00f1a \u201cTAGS\u201d</p> <p></p> <p>Podemos descargar una imagen espec\u00edfica y ejecutarla:</p> <pre><code>docker run hello-world:linux\n\ndocker images\nREPOSITORY                              TAG           IMAGE ID       CREATED        SIZE\nhello-world                             latest        ee301c921b8a   9 months ago   9.14kB\nhello-world                             linux         ee301c921b8a   9 months ago   9.14kB\n\ndocker run hello-world:linux\n</code></pre> <p>Para eliminar una imagen utilizamos el par\u00e1metro <code>rmi</code> por ejemplo:</p> <pre><code>docker pull alpine\n\ndocker images\nREPOSITORY                              TAG           IMAGE ID       CREATED        SIZE\nalpine                                  latest        ace17d5d883e   3 weeks ago    7.73MB\nhello-world                             latest        ee301c921b8a   9 months ago   9.14kB\nhello-world                             linux         ee301c921b8a   9 months ago   9.14kB\n\n# Eliminamos, opci\u00f3n 1\ndocker rmi ace17d5d883e\n\n# Eliminamos, opci\u00f3n 2\ndocker rmi alpine\n\n# Eliminamos, opci\u00f3n 3\ndocker rmi ace1\n</code></pre> <p>Tambi\u00e9n se pueden buscar im\u00e1genes desde consola.</p> <pre><code>docker search ubuntu\n\nNAME                             DESCRIPTION                                     STARS     OFFICIAL\nubuntu                           Ubuntu is a Debian-based Linux operating sys\u2026   16888     [OK]\nwebsphere-liberty                WebSphere Liberty multi-architecture images \u2026   298       [OK]\nopen-liberty                     Open Liberty multi-architecture images based\u2026   64        [OK]\nneurodebian                      NeuroDebian provides neuroscience research s\u2026   106       [OK]\nubuntu-debootstrap               DEPRECATED; use \"ubuntu\" instead                52        [OK]\nubuntu-upstart                   DEPRECATED, as is Upstart (find other proces\u2026   115       [OK]\nubuntu/nginx                     Nginx, a high-performance reverse proxy &amp; we\u2026   112\nubuntu/squid                     Squid is a caching proxy for the Web. Long-t\u2026   83\nubuntu/cortex                    Cortex provides storage for Prometheus. Long\u2026   4\nubuntu/prometheus                Prometheus is a systems and service monitori\u2026   56\nubuntu/apache2                   Apache, a secure &amp; extensible open-source HT\u2026   70\n...\n</code></pre>"},{"location":"UD00/7.docker/#volumenes","title":"Vol\u00famenes","text":"<p>Los vol\u00famenes sirven para almacenar informaci\u00f3n de manera persistente en uno o varios contenedores. Es \u00fatil para que los archivos ya est\u00e9n integrados en el propio contenedor y podamos disponer de dichos archivos en diferentes contenedores diferentes.</p> <p>Tambi\u00e9n nos permiten compartir archivos con el contenedor. Modificarlos en local y que se modifiquen en el contenedor.</p>"},{"location":"UD00/7.docker/#operaciones-con-volumenes","title":"Operaciones con vol\u00famenes","text":"<pre><code># Ver disponibles\ndocker volume ls\nDRIVER    VOLUME NAME\n\n# Creamos un volumen\ndocker volume create almacen\nalmacen\n\ndocker volume ls\nDRIVER    VOLUME NAME\nlocal     almacen\n\ndocker volume inspect almacen\n[\n    {\n        \"CreatedAt\": \"2024-02-20T11:10:58Z\",\n        \"Driver\": \"local\",\n        \"Labels\": null,\n        \"Mountpoint\": \"/var/lib/docker/volumes/almacen/_data\",\n        \"Name\": \"almacen\",\n        \"Options\": null,\n        \"Scope\": \"local\"\n    }\n]\n\n# Lo borramos\ndocker volume rm almacen\n</code></pre>"},{"location":"UD00/7.docker/#ejemplo-compartir-volumenes-con-host","title":"Ejemplo: compartir vol\u00famenes con host","text":"<pre><code># Creamos un punto de montaje\ndocker run --rm -it -v /tmp/puntomontaje:/home ubuntu\n</code></pre>"},{"location":"UD00/7.docker/#ejemplo-compartir-volumenes-con-contenedores","title":"Ejemplo: compartir vol\u00famenes con contenedores","text":"<p>Vamos a crear un volumen para compartir archivos entre nuestro sistema de ficheros local y 2 contenedores (ubuntu y fedora).</p> <pre><code>docker volume create almacen\n\n# Descargamos la imagen de ubuntu\ndocker pull ubuntu\n# Descargamos la imagen de fedora\ndocker pull fedora\n\ndocker images\nREPOSITORY                             TAG       IMAGE ID       CREATED        SIZE\nubuntu                                 latest    a50ab9f16797   7 days ago     69.2MB\nfedora                                 latest    46243415778a   2 months ago   259MB\n\n## Creamos un contenedor, modo interactivo\ndocker run --rm -it -v almacen:/home ubuntu\n# dentro del contenedor\ncd /home\ntouch prueba.txt\nexit # -&gt; Salimos del contenedor\n\n## Creamos otro contenedor, modo interactivo\ndocker run --rm -it -v almacen:/home fedora\n# dentro del contenedor\ncd /home\nls -&gt; existe el archivo prueba.txt\n</code></pre>"},{"location":"UD00/7.docker/#compartir-volumenes-con-contenedores-y-en-local","title":"Compartir volumenes con contenedores y en local","text":"<pre><code>$ docker volume ls\nDRIVER    VOLUME NAME\nlocal     almacen\n</code></pre>"},{"location":"UD00/7.docker/#docker-compose-vs-dockerfile","title":"docker-compose vs DockerFile","text":"<p>https://blog.elhacker.net/2022/01/gestion-contenedores-dockerfile-y-docker-compose.html</p>"},{"location":"UD00/7.docker/#diferencias","title":"Diferencias","text":""},{"location":"UD00/7.docker/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose es una herramienta que permite simplificar el uso de Docker. A partir de archivos YAML es mas sencillo crear contenedores, conectarlos, habilitar puertos, vol\u00famenes, etc.</p> <p>Con Compose puedes crear diferentes contenedores y al mismo tiempo, en cada contenedor, diferentes servicios, unirlos a un vol\u00famen com\u00fan, iniciarlos y apagarlos, etc. Es un componente fundamental para poder construir aplicaciones y microservicios</p> <p>Par\u00e1metros docker-compose.yml</p> <ul> <li>\u201cversion\u00a0\u20183\u2019: Los archivos docker-compose.yml son versionados, lo que significa que es muy importante indicar la versi\u00f3n de las instrucciones que queremos darle. A medida de que Docker evoluciona, habr\u00e1 nuevas versiones, pero de todos modos, siempre hay compatibilidad hacia atr\u00e1s, al indicar la versi\u00f3n</li> <li>\u201cbuild\u00a0.\u201d: Se utiliza para indicar donde est\u00e1 el Dockerfile que queremos utilizar para crear el contenedor. Al definier \u201c.\u201d autom\u00e1ticamente considerar\u00e1 el Dockerfile existente en directorio actual.</li> <li>\u201ccommand\u201d: Una vez creado el contenedor, aqui lanzamos el comando que permite ejecutar Jekyll, en modo servidor. El comando \u201c\u2013host 0.0.0.0\u201d sirve para mapear el contenedor al sistema operativo host</li> <li>\u201cports\u201d: mapeamos los puertos locales, por ejemplo 4000 (webserver jekyll) y 35729 (livereload) al servidor host. Esto permite que accediendo a Localhost:4000 podamos probar el sitio generador por Jekyll</li> <li>\u201cvolumes\u201d: lo que hacemos es mapear el directorio local se mapee directamente con el /directoriox, lugar donde hemos creado la aplicaci\u00f3n. De este modo, cualquier cambio en el directorio local en el host, se har\u00e1 de inmediato en el contenedor.</li> </ul> <p></p> <p>Ejemplo, creaci\u00f3n contenedor con wordpress:</p> <pre><code>mkdir /tmp/wp\ncd /tmp/wp\n\nvi docker-compose.yml\n</code></pre> <pre><code>version: '3' # Utilizamos la versi\u00f3n 3\n\n## Nos saltamos la secci\u00f3n network \n\nservices:\n    db:\n        image: mariadb:10.3.9\n        volumes:\n            - data:/var/lib/mysql\n        environment:\n            - MYSQL_ROOT_PASSWORD=secret\n            - MYSQL_DATABASE=wordpress\n            - MYSQL_USER=manager\n            - MYSQL_PASSWORD=secret\n                ## Equivalente a\n                ## docker run -d --name wordpress-db \\\n        ## --mount source=wordpress-db,target=/var/lib/mysql \\\n        ## -e MYSQL_ROOT_PASSWORD=secret \\\n        ## -e MYSQL_DATABASE=wordpress \\\n        ## -e MYSQL_USER=manager \\\n        ## -e MYSQL_PASSWORD=secret mariadb:10.3.9\n\n    web:\n        image: wordpress:4.9.8\n        depends_on:\n            - db\n        volumes:\n            - ./target:/var/www/html\n        environment:\n            - WORDPRESS_DB_USER=manager\n            - WORDPRESS_DB_PASSWORD=secret\n            - WORDPRESS_DB_HOST=db\n        ports:\n            - 8080:80\n                ## Equivalente a\n                ## docker run -d --name wordpress \\\n            ## --link wordpress-db:mysql \\\n            ## --mount type=bind,source=\"$(pwd)\"/target,target=/var/www/html \\\n            ## -e WORDPRESS_DB_USER=manager \\\n            ## -e WORDPRESS_DB_PASSWORD=secret \\\n            ## -p 8080:80 \\\n            ## wordpress:4.9.8\n\nvolumes:\n    data: # creci\u00f3n de volumen, compose a\u00f1ade un prefijo por lo que se llamar\u00e1 worpdress_data\n</code></pre> <pre><code># Levantamos la aplicaci\u00f3n\ndocker-compose up -d\n# El par\u00e1metro -d es similar al de docker run: nos permite levantar los servicios en segundo plano.\n\ndocker-compose ps\n## docker-compose ps solo muestra informaci\u00f3n de los servicios que se define en docker-compose.yaml, mientras que docker muestra todos.\n\n# Detener servicios\ndocker-compose stop\n\n# Borrar\ndocker-compose down\n\n# Borrar vol\u00famenes\ndocker-compose down -v\n</code></pre> <p>Cuando creamos contenedores con\u00a0<code>docker</code>\u00a0sin indicar un nombre, por defecto asigna uno aleatorio; mientras que en\u00a0Compose\u00a0el prefijo es el nombre del directorio y el sufijo el nombre del servicio:\u00a0wordpress*_db*_1. El n\u00famero indica el n\u00famero de instancia. Es posible levantar m\u00e1s de una instancia de un mismo servicio.</p> <p>Equivalencia de par\u00e1metros</p> par\u00e1metro\u00a0Docker par\u00e1metro\u00a0Composer --link depends_on --mount volumes -e environment -p,--publish ports image <p>Si reiniciamos el ordenador, los contenedores estar\u00e1n detenidos (stop), podremos reiniciarlos con\u00a0<code>docker start</code>\u00a0o\u00a0<code>docker-compose start</code>. Este es el comportamiento predeterminado y el que nos interesa en un entorno de desarrollo.</p> <p>Sin embargo, en otros entornos, o para casos concretos, igual queremos que un contenedor tenga el mismo estado en el que estaba antes de reiniciar la m\u00e1quina (iniciado o parado).</p> <p>Para eso usaremos el par\u00e1metro\u00a0<code>restart</code>. En el caso de la base de datos de nuestro ejemplo, la configuraci\u00f3n quedar\u00eda como:</p> <pre><code>services:\n    db:\n        image: mariadb:10.3.9\n        **restart: unless-stopped**\n        volumes:\n            - data:/var/lib/mysql\n        environment:\n            - MYSQL_ROOT_PASSWORD=secret\n            - MYSQL_DATABASE=wordpress\n            - MYSQL_USER=manager\n            - MYSQL_PASSWORD=secret\n</code></pre> <p>Otros valores son:\u00a0<code>no</code>\u00a0(por defecto),\u00a0<code>always</code>\u00a0y\u00a0<code>on-failure</code>.</p>"},{"location":"UD00/7.docker/#dockerfile","title":"DockerFile","text":""},{"location":"UD00/7.docker/#ejemplo-basico","title":"Ejemplo b\u00e1sico","text":"<p>Nos ubicamos en la carpeta donde vayamos a trabajar con la imagen, por ejemplo voy a crear un directorio llamado docker-images.</p> <p>Y creamos una archivo Dockerfile con un editor de texto.</p> <pre><code>FROM ubuntu\nRUN apt update\nRUN apt install python 3 -y\nRUN apt install netris -y\n</code></pre> <p>Creamos la imagen seg\u00fan las \u00f3rdenes anteriores:</p> <pre><code>docker build -t python-ubuntu .\n</code></pre> <p>Hay un error en el Dockerfile, corregir y volver a ejecutar el build.</p> <p>Ya tenemos una imagen de ubuntu pero con python3 instalado:</p> <pre><code>docker run -it python-ubuntu\n\npython3\n\nPython 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre> <p>Una vez salimos podemos ver todas las \u201ccapas\u201d de la imagen:</p> <pre><code>docker history -H python-ubuntu\n\nIMAGE          CREATED         CREATED BY                                      SIZE      COMMENT\n78732fd773c4   2 minutes ago   RUN /bin/sh -c apt install netris # buildkit    1MB       buildkit.dockerfile.v0\n&lt;missing&gt;      2 minutes ago   RUN /bin/sh -c apt install python3 -y # buil\u2026   29.5MB    buildkit.dockerfile.v0\n&lt;missing&gt;      3 minutes ago   RUN /bin/sh -c apt update # buildkit            45.1MB    buildkit.dockerfile.v0\n&lt;missing&gt;      3 weeks ago     /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B\n&lt;missing&gt;      3 weeks ago     /bin/sh -c #(nop) ADD file:8d91b8bd386e0cc34\u2026   69.2MB\n&lt;missing&gt;      3 weeks ago     /bin/sh -c #(nop)  LABEL org.opencontainers.\u2026   0B\n&lt;missing&gt;      3 weeks ago     /bin/sh -c #(nop)  LABEL org.opencontainers.\u2026   0B\n&lt;missing&gt;      3 weeks ago     /bin/sh -c #(nop)  ARG LAUNCHPAD_BUILD_ARCH     0B\n&lt;missing&gt;      3 weeks ago     /bin/sh -c #(nop)  ARG RELEASE                  0B\n</code></pre>"},{"location":"UD00/7.docker/#creacion-de-imagenes-propias","title":"Creaci\u00f3n de im\u00e1genes propias","text":"<ul> <li>Para construir una imagen, se crea un\u00a0<code>Dockerfile</code>\u00a0con las instrucciones que especifican lo que va a ir en el entorno, dentro del contenedor (redes, vol\u00famenes, puertos al exterior, archivos que se incluyen.</li> <li>Indica c\u00f3mo y con qu\u00e9 construir la imagen.</li> <li>Podemos utilizar la imagen en tantos contenedores como queramos.</li> </ul> <p>El DockerFile nos permitir\u00e1 definir las funciones b\u00e1sicas del contenedor.</p> <p>Todo Dockerfile debe terminar en un comando CMD o en un ENTRYPOINT, pero en este caso, no lo utilizamos, ya que lanzaremos un comando directamente desde la receta de Docker Compose. Es decir, este Dockerfile se utiliza solamente para construir el contenedor y configurarlo. No es autoejecutable.</p>"},{"location":"UD00/7.docker/#from","title":"FROM","text":"<p>Imagen del sistema operativo donde va a correr el contenedor.</p> <p>** Las versiones \u201cAlpine linux\u201d ocupan muy poco espacio.</p>"},{"location":"UD00/7.docker/#run","title":"RUN","text":"<p>El comando\u00a0RUN\u00a0se ejecuta cuando se est\u00e1 construyendo una imagen personalizada para realizar una acci\u00f3n, creando una capa nueva. Este comando tiene el siguiente formato:</p> <p>RUN\u00a0comando </p> <p>RUN\u00a0[\u201cejecutable\u201d, \u201cparametro1\u201d, \u2026] </p> <p>Ejemplo en windows:</p> <p>RUN\u00a0[\u201cPowershell\u201d, \u201cGet-Services\u201d, \u201c*\u201d] </p>"},{"location":"UD00/7.docker/#copy","title":"COPY","text":"<p>Sirve para copiar archivos desde nuestra m\u00e1quina al contenedor. Podemos pasar un documento de texto de la m\u00e1quina anfitri\u00f3n al conenedor de python-ubuntu.</p> <pre><code>FROM ....\nRUN ....\nRUN ....\nCOPY prueba.txt /\n</code></pre> <p>Volvemos a construir la imagen y accedemos a ella para buscar el archivo.</p>"},{"location":"UD00/7.docker/#env","title":"ENV","text":"<p>Podemos crear una variable y enviarla a nuestro contenedor, en mi caso por ejemplo voy a definir una variable llamada contenido que va a ir dirigida a un bloc de notas que est\u00e1 dentro del contenedor:</p> <pre><code>....\nENV NUEVO_PATH /etc\n</code></pre> <p>Una vez regenerado la imagen y dentro del contenedor:</p> <pre><code>echo $NUEVO_PATH\n/etc\n</code></pre> <p>Podemos combinar RUN con ENV</p> <pre><code>ENV NUEVO_PATH /etc\nRUN echo $NUEVO_PATH &gt; /prueba.txt\n</code></pre>"},{"location":"UD00/7.docker/#workdir","title":"WORKDIR","text":"<p>Nos situamos en un directorio determinado, nos puede ayudar en la copia de ficheros.</p> <pre><code>.....\nWORKDIR /home\nCOPY prueba.txt .  # Lo copia en /home\n</code></pre>"},{"location":"UD00/7.docker/#expose","title":"EXPOSE","text":"<p>Permite exponer los puertos que queramos</p>"},{"location":"UD00/7.docker/#label","title":"LABEL","text":"<p>Creamos etiquetas, por ejemplo:</p> <pre><code>FROM ubuntu\nLABEL version=1.0\nLABEL autor=JosepGarcia\n.....\n</code></pre>"},{"location":"UD00/7.docker/#user","title":"USER","text":"<p>Sirve para establecer el usuario, debe existir. (Por defecto se utiliza root).</p> <pre><code>....\nRUN echo $(whoami) &gt; /tmp/usuarioantes.txt\n\nRUN useradd -m josepgarcia\nUSER josepgarcia\nWORKDIR /home/josepgarcia\nRUN echo $(whoami) &gt; /tmp/usuarioahora.txt\n</code></pre>"},{"location":"UD00/7.docker/#cmd","title":"CMD","text":"<p>Ejecuta comandos una vez se ha inicializado el conenedor (RUN se utiliza para crear la imagen de un contenedor).</p> <pre><code>## Ejecutamos el comando top cuando se inicie el contenedor\n.....\nCMD top\n</code></pre>"},{"location":"UD00/7.docker/#ignore","title":"IGNORE","text":"<p>Sirve para ignorar aquello que tengamos en nuestro directorio actual.</p> <p>Por ejemplo:</p> <p>Creamos una imagen que copie todo nuestro directorio actual al contenedor.</p> <pre><code>ls\n\uf308 Dockerfile  \uf15c prueba.txt\n\nvi Dockerfile\n\n...\nCOPY . /tmp\n...\n</code></pre> <p>Ahora le decimos que copie todo menos el archivo Dockerfile, para ello creamos un fichero llamado <code>.dockerignore</code></p> <pre><code>vi .dockerignore\n\nDockerfile\n</code></pre> <p>Ejercicio1.</p> <p>Modificar Dockerfile para que el usuario creado anteriormente pueda ejectuar sudo.</p>"},{"location":"UD00/7.docker/#guardar-estado-de-los-contenedores","title":"Guardar estado de los contenedores","text":"<p>https://www.baeldung.com/ops/docker-save-container-state</p> <p>PENDIENTE</p> <p>Pendiente a partir de este punto</p>"},{"location":"UD00/7.docker/#entrypoint","title":"ENTRYPOINT","text":"<p>Este comando se ejecuta cuando se quiere ejecutar un ejecutable en el contenedor en su arranque. Los ejemplos tipo de su uso, son cuando se quiere levantar un servidor web, una base de datos, etc \u2026.</p> <p>Este comando tiene dos sintaxis:</p> <p>ENTRYPOINT\u00a0[\u201ccomando\u201d, \u201cParametro1\u201d, \u201cParametro2\u201d, \u2026] </p> <p>Esta es la forma recomendada.</p> <p>ENTRYPOINT\u00a0comando parametro1 parametro2 </p> <p>Con esta forma el comando se ejecuta en la Shell del contenedor.</p> <p>Utilizaci\u00f3n de varios comandos de forma conjunta</p> <p>Como se ha comentado anteriormente el comando CMD se puede utilizar para pasar par\u00e1metros al comando ENRYPOINT. Una posible forma de realizarlo es:</p> <p>ENRYPOINT\u00a0[\u201cPowershell\u201d, \u201cGet-Services\u201d]CMD\u00a0[\u201cMySql] </p> <p>En el ejemplo, se est\u00e1 invocando al comando Get-Services para recuperar informaci\u00f3n de los servicios Windows y como en el comando CMD se est\u00e1 indicando el servicio en concreto del que se quiere recuperar la informaci\u00f3n que en este caso es del servicio Windows de MySql.</p> <p>Nota: cuando los comandos o par\u00e1metros son pasados entre corchetes siempre van entre comillas. Esto es porque el comando correspondiente lo interpreta como una cadena JSON.</p>"},{"location":"UD00/7.docker/#5-casos-de-uso","title":"5. Casos de uso","text":""},{"location":"UD00/7.docker/#compatibilidad-de-codigo-entre-diferentes-versiones-de-un-lenguaje","title":"Compatibilidad de c\u00f3digo entre diferentes versiones de un lenguaje","text":"<pre><code>mkdir /tmp/php\ncd /tmp/php\n</code></pre> <p>Crear el siguiente archivo (test.php)</p> <pre><code>&lt;?php\n// Funciona bien en php5 ya que list hace la asignaci\u00f3n desde el \u00faltimo al primero\n// En PHP 5, list() asigna los valores empezando desde el par\u00e1metro m\u00e1s a la derecha. En PHP 7, list() empieza desde el par\u00e1metro m\u00e1s a la izquierda.\n// https://www.php.net/manual/es/function.list.php\n$info = array('cafe\u00edna','marr\u00f3n', 'caf\u00e9');\n\n// Enumerar todas las variables\nlist($datos[], $datos[], $datos[]) = $info;\necho \"El $datos[0] es $datos[1] y la $datos[2] lo hace especial.\\n\";\n</code></pre> <p>A continuaci\u00f3n vamos a crear dos contenedores que sirva este c\u00f3digo usando im\u00e1genes distintas , para cada versi\u00f3n de PHP y usando puertos distintos para acceder a cada versi\u00f3n de la aplicaci\u00f3n:</p> <pre><code>docker run -d -p 8081:80 --name php56 -v /tmp/php:/var/www/html:ro php:5.6-apache\n## Accedemos al contenedor: docker exec -it eb326ffd1b66 /bin/bash\n## Ejecutamos test.php\n\n**docker run -d -p 8082:80 --name php74 -v /tmp/php:/var/www/html:ro php:7.4-apache**\n## Ejecutamos test.php desde web\n## http://localhost:8082/test.php\n</code></pre>"},{"location":"UD00/7.docker/#crear-una-imagen-de-un-repositorio-de-github","title":"Crear una imagen de un repositorio de github","text":"<p>Ejemplo repositorio:</p> <p>https://github.com/k4m4/kickthemout</p> <pre><code>FROM ubuntu:focal\n\nRUN apt update -y &amp;&amp; apt upgrade -y &amp;&amp; apt install python3 -y\nRUN apt install -y git\nRUN apt install -y python3-pip\nRUN git clone https://github.com/k4m4/kickthemout.git\n\nWORKDIR /kickthemout\n\nRUN pip3 install -r requirements.txt\n\nCMD python3 kickthemout.py\n</code></pre>"},{"location":"UD00/7.docker/#aplicacion-de-python-dentro-de-un-contenedor","title":"Aplicaci\u00f3n de python dentro de un contenedor","text":""},{"location":"UD00/7.docker/#crear-una-imagen-personalizada-pendiente","title":"Crear una imagen personalizada  \u2014 PENDIENTE \u2014","text":"<p>https://jolthgs.wordpress.com/2019/09/25/create-a-debian-container-in-docker-for-development/</p> <p>Para crear una imagen personalizada utilizaremos el contenedor que hab\u00edamos creado en el punto 1, con una debian actualizada y con un fichero de texto prueba.txt</p> <pre><code># Iniciamos el contenedor\ndocker start debian-mini\n\ndocker ps\nCONTAINER ID   IMAGE            COMMAND          CREATED          STATUS         PORTS     NAMES\n97d8dc048093   debian:10-slim   \"/bin/bash -l\"   10 minutes ago   Up 2 seconds             debian-mini\n\n# Entramos en el contenedor iniciado\ndocker exec -it debian-mini bash\n\n# Instalamos paquetes\napt install netris sl ninvaders\n\n# \u00bfD\u00f3nde est\u00e1n?\nfind / -name ninvaders\n\nls /usr/games\n\n# Salimos\ncontrol + d\n\n# Nuestra imagen\ndocker image ls\nREPOSITORY                             TAG       IMAGE ID       CREATED        SIZE\ndebian                                 10-slim   6016bddc4bad   9 days ago     63.5MB\n</code></pre>"},{"location":"UD00/7.docker/#otro-nombre-para-dockerfile","title":"Otro nombre para Dockerfile","text":"<pre><code>docker build -t test -f **otronombre** .\n</code></pre>"},{"location":"UD00/7.docker/#6-copias-de-seguridad","title":"6. Copias de seguridad","text":""},{"location":"UD00/7.docker/#copias-de-contenedores","title":"Copias de contenedores","text":"<p>Ya est\u00e9n encendidos o apagados, podemos realizar respaldos de seguridad de los contenedores. Utilizando la opci\u00f3n \u201cexport\u201d empaquetar\u00e1 el contenido, generando un fichero con extensi\u00f3n \u201c.tar\u201d de la siguiente manera:</p> <p>docker export -o fichero-resultante.tar nombre-contenedor </p> <p>o</p> <p>docker export nombre-contenedor &gt; fichero-resultante.tar </p>"},{"location":"UD00/7.docker/#restauracion-de-copias-de-seguridad-de-contenedores","title":"Restauraci\u00f3n de copias de seguridad de contenedores","text":"<p>Hay que tener en cuenta, antes de nada, que no es posible restaurar el contenedor directamente, de forma autom\u00e1tica. En cambio, s\u00ed podemos crear una imagen, a partir de un respaldo de un contenedor, mediante el par\u00e1metro \u201cimport\u201d de la siguiente manera:</p> <p>docker import fichero-backup.tar nombre-nueva-imagen </p>"},{"location":"UD00/7.docker/#copias-de-imagenes","title":"Copias de im\u00e1genes","text":"<p>Aunque no tiene mucho sentido por que se bajan muy r\u00e1pido, tambi\u00e9n tenemos la posibilidad de realizar copias de seguridad de im\u00e1genes. El proceso se realiza al utilizar el par\u00e1metro \u2018save\u2018, que empaquetar\u00e1 el contenido y generar\u00e1 un fichero con extensi\u00f3n \u201ctar\u201c, as\u00ed:</p> <p>docker save nombre_imagen &gt; imagen.tar </p> <p>o</p> <p>docker save -o imagen.tar nombre_imagen </p>"},{"location":"UD00/7.docker/#restaurar-copias-de-seguridad-de-imagenes","title":"Restaurar copias de seguridad de im\u00e1genes","text":"<p>Con el par\u00e1metro \u2018load\u2019, podemos restaurar copias de seguridad en formato \u2018.tar\u2019 y de esta manera recuperar la imagen.</p> <p>docker load -i fichero.tar </p>"},{"location":"UD00/7.docker/#7-contenedores-ejemplo","title":"7. Contenedores ejemplo","text":"<p>netdata</p> <p>Nginx Proxy Manager - acceso https con certificados v\u00e1lidos para mi red local, no la tengo abierta al exterior.</p> <p>Jellyfin - multimedia</p> <p>Nextcloud - nube</p> <p>Syncserver - sincronizar datos de firefox entre dispositivos (este est\u00e1 absoleto, publicaron otra herramienta pero hay poca informaci\u00f3n y a\u00fan no he conseguido hacerla funcionar)</p> <p>Vaultwarden - contrase\u00f1as</p> <p>Camera.ui - Para ver las c\u00e1maras e integrarlas en la casa de Apple.</p> <p>Homebridge - para poder usar dispositivos no compatibles en la casa de Apple</p> <p>HomeAssistant - ahora mismo la uso poco porque uso m\u00e1s la de Apple, pero ser\u00eda el equivalente en software libre</p> <p>Portainer - para gestionar los contenedores</p> <p>Photoprism - para ver las fotos</p> <p>Homepage - como p\u00e1gina de inicio con los contenedores y dem\u00e1s aplicaciones instaladas en el nas</p> <p>soulseek - para m\u00fasica</p> <p>youtubedl - para bajar musica de youtube</p> <p>glances - para ver informacion del estado del servidor v\u00eda web</p> <p>linkding - para guardar enlaces</p> <p>acestream - para ver canales en jellyfin</p> <p>gitbucket - para listas de acestream y de pihole</p> <p>libreddit - reedit sin publicidad (aunque en modo s\u00f3lo lectura, no se puede enviar contenido)</p> <p>invidious - para ver youtube</p> <p>uptime kuma - para ver si alguno de los contenedores o aplicaciones se caen</p> <p>searxng - un metabuscador para tener google, duckdns y otros en un mismo sitio</p> <p>watchtower - para actualizar autom\u00e1ticamente algunos contenedores</p>"},{"location":"UD00/7.docker/#8-ejercicios","title":"8. Ejercicios","text":""},{"location":"UD00/7.docker/#python","title":"Python","text":"<p>Crea 2 contenedores, uno con python 3.11 y otro con python 3.9</p> <pre><code>docker run -d --name python3.11 -v /tmp/php:/app python:3.11\n</code></pre>"},{"location":"UD00/8.EntornosVirtualesPython/","title":"Entornos virtuales python","text":""},{"location":"UD00/8.EntornosVirtualesPython/#entorno-por-defecto","title":"Entorno por defecto","text":"<pre><code>$ pip list\nPackage                   Version\n------------------------- ------------\naiohttp                   3.9.1\naiosignal                 1.3.1\naltair                    5.2.0\naltgraph                  0.17.2\nannotated-types           0.6.0\nanyio                     4.2.0\nappnope                   0.1.3\nasttokens                 2.4.1\n...\n...\n...\n</code></pre> <pre><code>$ mkdir entornos\n$ cd entornos\n\n$ pip install faker #instalamos en global\n$ pip list | grep Faker\nFaker                     20.1.0\n\n# Tenemos instalada la versi\u00f3n 20.1.0 en global\n</code></pre>"},{"location":"UD00/8.EntornosVirtualesPython/#creacion-entorno-virtual","title":"Creaci\u00f3n entorno virtual","text":"<pre><code>$ python3 -m venv entorno_virtual\n$ ls\nentorno_virtual\n</code></pre> <p>Lo activamos</p> <pre><code>$ source entorno_virtual/bin/activate\n\n**(entorno_virtual)** &lt;\u25b8&gt; ~/W/py/entornos\n</code></pre> <p>Mostramos los paquetes instalados en este entorno</p> <pre><code>$ pip list\nPackage Version\n------- -------\npip     24.0\n(entorno_virtual)\n</code></pre> <p>Instalamos otra versi\u00f3n de faker</p> <pre><code>$ pip install faker==25.2\n\n$ pip list\n</code></pre> <p>Salimos del entorno virtual</p> <pre><code>$ deactivate\n</code></pre> <p>Volvemos a entrar al entorno y congelamos los paquetes utilizados</p> <pre><code>$ source entorno_virtual/bin/activate\n\n**(entorno_virtual)** &lt;\u25b8&gt; ~/W/py/entornos\n\n$ pip freeze &gt; requirements.txt\n\n$ cat requirements.txt\n</code></pre> <p>Creamos un script con python.</p> <pre><code>'''test.py'''\nfrom faker import Faker\nfake = Faker()\n\nprint(fake.name())\n</code></pre>"},{"location":"UD00/8.EntornosVirtualesPython/#cual-es-la-utilidad-de-requirementstxt","title":"\u00bfCu\u00e1l es la utilidad de requirements.txt?","text":"<p>Ahora podemos compartir nuestro proyecto (github u otro ordenador), subiendo \u00fanicamente los archivos requirements.txt y el script que hemos creado (test.py).</p> <p>Para simular esta acci\u00f3n, vamos a borrar nuestro entorno.</p> <pre><code>$ deactivate\n\n$ rm -rf entorno_virtual\n</code></pre> <p>Restauramos el entorno que ten\u00edamos</p> <pre><code>$ python3 -m venv entorno2\n\n$ source entorno2/bin/activate # Lo activamos\n(entorno2)\n\n# Instalamos las dependencias que ten\u00edamos\n$ pip install -r requirements.txt\n</code></pre> <p>Cuando creamos un entorno virtual con venv solemos llamar a la carpeta del entorno \u201cvenv\u201d</p> <pre><code>$ python3 -m venv venv\n</code></pre> <p>Al utilizar git, podemos crear un archivo .gitignore que contenga entre otras cosas:</p> <pre><code># Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n</code></pre> <p>Ejemplo .gitignore para python:</p> <p>https://raw.githubusercontent.com/github/gitignore/main/Python.gitignore</p>"},{"location":"UD00/9.ModificarApuntes/","title":"Realizar pull request","text":"<p>Para a\u00f1adir una correcci\u00f3n o mejora a los apuntes, nos vamos a la parte superior de la p\u00e1gina que queremos editar.</p> <p></p> <p>A continuaci\u00f3n nos dirige a github, hay que hacer un fork del repositorio:  Ahora ya podemos editar el archivo, una vez finalizado click en \"commit changes\":  Describimos los cambios realizados (importante)  Creamos una pull request </p>"},{"location":"UD00/9.ModificarApuntes/#como-aprobar-un-pull-request","title":"\u00bfC\u00f3mo aprobar un pull request?","text":""},{"location":"UD00/_TODO/","title":"PENDENT","text":"<p>Contenedors M\u00e0quina virtual per a executar bash i fer tots ah\u00ed + nano  + vim + bash (pr\u00e0ctica per a instalar ssh)</p> <p>Hardware - Grupos de discos LVM (ubuntu)</p> <p>Big data Scripts - https://markobigdata.com/2018/04/25/bash-script-for-creating-new-user-in-hadoop-and-ambari-views/</p>"},{"location":"UD01/1.instalacionubuntuserver/","title":"Instalaci\u00f3n Ubuntu server. Configuraci\u00f3n entorno.","text":""},{"location":"UD01/1.instalacionubuntuserver/#1-descargar-e-instalar","title":"1. Descargar e instalar","text":"<p>https://ubuntu.com/download/server#manual-install</p> <p>LTS: The latest LTS version of Ubuntu, for desktop PCs and laptops. LTS stands for long-term support \u2014 which means five years of free security and maintenance updates</p> <p></p>"},{"location":"UD01/1.instalacionubuntuserver/#2-configuracion-del-entorno","title":"2. Configuraci\u00f3n del entorno","text":"<p>Una vez instalada la m\u00e1quina ubuntu pasamos a la configuraci\u00f3n de la misma.</p>  \ud83d\udca1 **INFO:** Para editar los ficheros utilizar\u00e9 el editor de consola vi o vim, puedes utilizar cualquier otro (como nano)."},{"location":"UD01/1.instalacionubuntuserver/#21-preparar-el-sistema","title":"2.1. Preparar el sistema","text":"<p>a) Podemos desactivar el entorno gr\u00e1fico al iniciar sesi\u00f3n ejecutando el siguiente comando como root:</p> <p>Si hemos instalado Ubuntu Server no es necesario, no tiene entorno gr\u00e1fico.</p> <pre><code>/sbin/systemctl set-default multi-user.target\n\n/sbin/sysctl set-default multi-user.target\n</code></pre> <p>b) Crear un usuario llamado hadoop, ese usuario lo utilizaremos para instalar hadoop.</p> <pre><code>adduser hadoop\n# Si no encuentra el comando /sbin/adduser\n# $PATH\n# No te olvides del password\n\ncat /etc/password # Mostrar\u00e1 el usuario creado\nls -la /home/ # Aparecer\u00e1 la \"home\" del usuarios\n</code></pre> <p>c) Poner el teclado en espa\u00f1ol, en caso de que la instalaci\u00f3n nos haya dejado un layout ingl\u00e9s (Si te funciona la \u00d1 salta este paso)</p> <pre><code>sudo apt-get install console-data\n\n# localectl set-keymap es\n</code></pre> <p>d) Comprobamos que ssh est\u00e9 funcionando, en caso contrario hay que instalarlo y configurarlo.</p> <pre><code>ssh localhost\n</code></pre>  \u26a0\ufe0f Pendiente, ssh sin contrase\u00f1a   <p>e) Para conocer nuestra IP</p> <p><pre><code>## Opci\u00f3n 1\n$ hostname -I\n\n## Opci\u00f3n 2\n$ ip addr\n\n## Opci\u00f3n 3\n# en /sbin/ifconfig ???\n$ apt install net-tools\n$ sudo ifconfig\n</code></pre> </p>"},{"location":"UD01/1.instalacionubuntuserver/#22-instalar-paquetes-en-debian","title":"2.2. Instalar paquetes en debian","text":"<p>Como root o utilizando sudo</p> <pre><code># Actualizar \u00edndice de paquetes (para ver si hay nuevas versiones)\nsudo apt update\n\n# Instalar un paquete o programa\nsudo apt install [package_name]\n\n# Borrar un paquete\napt remove [package_name]\n\n# Borrar un paquete y ficheros de configuraci\u00f3n\napt purge [package_name]\n\n# Actualizar paquetes a la \u00faltima versi\u00f3n\napt upgrade\n\n# B\u00fasqueda de paquetes\napt search [text_to_search]\n\n# \"Arreglar\" paquetes rotos\napt -f install\n</code></pre> <p>M\u00e1s info en:</p> <p>https://www.2daygeek.com/debian-ubuntu-apt-command-guide/</p>"},{"location":"UD01/1.instalacionubuntuserver/#23-instalar-java","title":"2.3. Instalar Java","text":"<p>https://hadoop.apache.org/release/3.4.0.html</p>"},{"location":"UD01/1.instalacionubuntuserver/#231-openjdk","title":"2.3.1 openjdk","text":"<p>OpenJDK es la versi\u00f3n libre de la plataforma de desarrollo Java bajo concepto de lenguaje orientado a objetos.</p> <ul> <li>Es opensource</li> <li>Implementaci\u00f3n de referencia</li> <li>Detr\u00e1s hay empresas como: IBM, Apple, SAP, Mac, Azul, Intel, RedHat etc.</li> <li>\u2026</li> </ul> <pre><code># Actualizar paquetes\napt update\n# Instalamos la versi\u00f3n openjdk-11 (compatible con hadoop, la 17 da problemas)\napt install openjdk-11-jdk\n\n# Comprobamos el path de instalaci\u00f3n de la aplicaci\u00f3n\n# Este path lo utilizaremos en el punto posterior (.bashrc)\nls -la /usr/lib/jvm/java-11-openjdk-amd64\n\njava -version\njavac -version\n# Estos comandos nos devuelven informaci\u00f3n sobre la versi\u00f3n de java que hemos instalado\n</code></pre>  \u26a0\ufe0f **ERROR - ERROR - ERROR - ERROR - ERROR - ERROR - ERROR - ERROR**  La  versi\u00f3n 17 de openjdk da error con `webdfs` (Lo veremos en la UD02). Utilizar java 11: [https://cfdownload.adobe.com/pub/adobe/coldfusion/java/java11/java11020/jdk-11.0.20_linux-x64_bin.deb](https://cfdownload.adobe.com/pub/adobe/coldfusion/java/java11/java11020/jdk-11.0.20_linux-x64_bin.deb)"},{"location":"UD01/1.instalacionubuntuserver/#24-bashrc","title":"2.4.  .bashrc","text":"<p>Se ejecuta cada vez que se inicia sesi\u00f3n.</p> <p>Contiene una serie de configuraciones para la sesi\u00f3n de terminal. Esto incluye configurar o habilitar: colorear, completar, historial de shell, alias de comando y m\u00e1s.</p> <pre><code>**Ejercicio**\n\nEdita el archivo .bashrc de tu usuario, a\u00f1ade un saludo y una variable, por ejemplo:\n\necho \"*** Bienvenido A $HOSTNAME ***\"\nNUMERO=50\n\nSal de la sesi\u00f3n y vuelve a entrar, \u00bfAparece el saldo? \u00bfPuedes acceder a la variable?\n</code></pre> <p>Para poder utilizar JAVA tenemos que crear la variable JAVA_HOME y a\u00f1adir su path a PATH.</p> <p>A\u00f1adimos diferentes variables necesarias para utilizar java a .bashrc</p> <p>Accedemos al sistema con el usuario hadoop que hemos creado anteriormente.</p> <pre><code>## ACCEDEMOS COMO HADOOP\nid \n\n# Volvemos a casa\ncd\n\n# Editamos el archivo\nnano .bashrc\n\n# A\u00f1adimos al final...\n\nexport JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\nexport PATH=$PATH:/usr/lib/jvm/java-17-openjdk-amd64/bin\n</code></pre> <p>export permite que la variable est\u00e9 disponible para subprocesos de la shell en ejecuci\u00f3n. Pone la variable en el ambiente para que otros procesos puedan hacer uso de estas.</p> <p>La variable PATH contiene una lista de directorios separados por dos puntos. Estos son los directorios en los que el shell busca el comando que el usuario escribe desde el teclado.</p> <pre><code>**Ejercicio**\n\nVamos a borrar la variable path para la sesi\u00f3n actual, cuando reiniciemos la sesi\u00f3n continuar\u00e1 igual\n\n1. Ejecutamos el comando ls -la\n2. echo $PATH\n3. Modificamos el valor de PATH\n        PATH=\n4. Ejecutamos de nuevo el comando ls -la\n5. Modificamos el valor de PATH\n        PATH=/bin/\n6. echo $PATH\n7. ls -la\n</code></pre>"},{"location":"UD01/2.descargaconfighadoop/","title":"Descarga y configuraci\u00f3n Hadoop.","text":""},{"location":"UD01/2.descargaconfighadoop/#descarga-instalacion-de-hadoop","title":"Descarga + \u201cinstalaci\u00f3n\u201d de hadoop","text":"<p>Informaci\u00f3n y descargas:</p> <p>https://hadoop.apache.org/</p>"},{"location":"UD01/2.descargaconfighadoop/#_1","title":"Hadoop","text":"\u2139\ufe0f Tutorial digitalocean [https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-in-stand-alone-mode-on-ubuntu-20-04](https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-in-stand-alone-mode-on-ubuntu-20-04)   <p>Descargamos Hadoop (Binary download)</p> <p>https://hadoop.apache.org/releases.html</p> <p></p> <ul> <li> <p>Copiamos la URL con la \u00faltima versi\u00f3n, \u00bfQu\u00e9 version descargar?</p> <p>Instalaciones por defecto</p> <p>https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz</p> <p>Instalaci\u00f3n en un MAC m1 o m2 (procesador ARM)</p> <p>https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6-aarch64.tar.gz</p> <p> \u26a0\ufe0f Pendiente, verificar archivo descargado con la firma <p>Desde ssh a la m\u00e1quina virtual, como root:</p> <pre><code>cd /opt\n\n# Descargamos\nwget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n\n# Descomprimimos\ntar -zxvf hadoop-3.3.6.tar.gz\nls -la\n\n# Creamos un enlace para trabajar de manera m\u00e1s c\u00f3moda\nln -s hadoop-3.3.6 hadoop\nls -la\n\n# Cambiamos el propietario y grupo de la carpeta y enlace creado\n# En el documento anterior creamos el usuario hadoop\nchown -R hadoop:hadoop hadoop-3.3.6 hadoop\n\n# Entramos a la carpeta descomprimida (a trav\u00e9s del enlace creado)\ncd hadoop\n</code></pre> <p>Accedemos ahora a la m\u00e1quina con usuario hadoop y modificamos de nuevo el archivo .bashrc, le a\u00f1adimos las siguientes l\u00edneas:</p> <pre><code># HADOOP_HOME sin barra final\nexport HADOOP_HOME=/opt/hadoop\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\nexport PATH=$PATH:$HADOOP_HOME/bin\nexport PATH=$PATH:$HADOOP_HOME/sbin\n</code></pre> <p>#### NO A\u00d1ADIR DE MOMENTO ESTAS VARIABLES</p> <p>export HADOOP_MAPRED_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\" export HADOOP_STREAMING=$HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar export HADOOP_LOG_DIR=$HADOOP_HOME/logs export PDSH_RCMD_TYPE=ssh</p> <p>Recargamos .bashrc saliendo y volviendo a entrar o mediante:</p> <pre><code>source .bashrc\n</code></pre> <p>Si todo ha ido bien ya podremos ejecutar hadoop:</p> <pre><code>hadoop -h\n\nhadoop version\n</code></pre>"},{"location":"UD01/2.descargaconfighadoop/#directorios-dentro-de-hadoop","title":"Directorios dentro de Hadoop","text":"<pre><code>/opt/hadoop/bin -&gt; \"ejecutables\"\n    hadoop, hdfs, mapred, yar\n\n/opt/hadoop/etc -&gt; ficheros de configuracion xml\n\n/opt/hadoop/lib -&gt; librer\u00edas nativas\n\n/opt/hadoop/sbin -&gt; scripts, utilidades que me permiten entre otras cosas arrancar y parar hadoop\n\n/opt/hadoop/share -&gt; paquetes, ejemplos ...\n</code></pre>"},{"location":"UD01/2.descargaconfighadoop/#comprobar-que-hadoop-funciona","title":"Comprobar que hadoop funciona","text":"<p>Para comprobar que <code>mapreduce</code> funciona, vamos a ejecutar unos ejemplos que podemos encontrar dentro de share.</p> <pre><code>$ pwd\n/opt/hadoop/share/hadoop/mapreduce\n\n# Podemos ver los ejemplos que se pueden ejecutar ejecutando sobre el paquete de ejemplos mapreduce\n$ jar tf hadoop-mapreduce-examples-3.3.6.jar\n# Nos sale un listado de todos los comandos u opciones de este paquete\n\n#### Vamos a ejecutar por ejemplo: \n#### org/apache/hadoop/examples/Grep.class\n\n# Preparamos los directorios donde estar\u00e1n los datos de entrada (en /tmp) y copiamos datos de entrada \"demo\",\n# por ejemplo ficheros de configuraci\u00f3n de hadoop\n$ mkdir /tmp/entrada\n$ cd /tmp/entrada\n$ cp /opt/hadoop/etc/hadoop/*.xml .\n\n## Vamos a buscar (tal y como hace grep), las palabras que contengan un determiando texto, en este caso\n## palabras que contengan el texto kms\n$ cd /opt/hadoop/share/hadoop/mapreduce/\n\n## Invocamos al paquete de ejemplos, comando grep\n$ hadoop jar hadoop-mapreduce-examples-3.3.6.jar grep /tmp/entrada /tmp/salida/ 'kms'\n\n## Hace una serie de procesos mapper y reducer (lo veremos m\u00e1s adelante)\n$ cd /tmp/salida/\n$ ls\npart-r-00000  _SUCCESS\n\n# _SUCCESS -&gt; ha sido satisfactorio\n\n# \u00bfCu\u00e1ntas veces ha encontrado la palabra kms?\n$ cat part-r-00000\n9   kms\n\n-------------------------------------------------\n# Se trata de un programa b\u00e1sico inicial para comprobar que funciona, podemos hacer lo mismo desde bash:\ncd /tmp/entrada\negrep kms * | wc -l\n</code></pre>"},{"location":"UD01/2.descargaconfighadoop/#posibles-errores","title":"Posibles Errores","text":"<p>ERROR 1 El directorio /tmp/salida ya existe, dar\u00e1 error si no se borra</p> <p>ERROR 2 Cannot execute /home/debian/hadoop-3.2.3//libexec/hadoop-config.sh En .bashrc hemos a\u00f1adido una barra al final de HADOOP_HOME Modificar archivo, guardar cambios y luego ejecutar: source .bashrc (para que los cambios se apliquen a la sesi\u00f3n) ERROR: JAVA_HOME /usr/lib/jvm/java-17-openjdk-amd64 does not exist La variable de sesi\u00f3n JAVA_HOME no es correcta, revisa la carpeta de java</p> <p>ERROR 3 ERROR: JAVA_HOME /usr/lib/jvm/java-17-openjdk-amd64 does not exist La variable de sesi\u00f3n JAVA_HOME no es correcta, revisa la carpeta de java</p>"},{"location":"UD01/3.ejercicios/","title":"Hadoop, ejercicios.","text":""},{"location":"UD01/3.ejercicios/#ejemplos-de-mapreduce","title":"Ejemplos de mapreduce:","text":"<p>El fichero de ejemplos hadoop-mapreduce-examples, contiene los siguientes:</p> aggregatewordcount Cuenta las palabras de los archivos de entrada. aggregatewordhist Calcula el histograma de las palabras de los archivos de entrada. <code>bbp</code> Uusa una f\u00f3rmula Bailey-Borwein-Plouffe para calcular los d\u00edgitos exactos de Pi. dbcount Cuenta los registros de vistas de p\u00e1gina almacenados en una base de datos. distbbp Usa una f\u00f3rmula de tipo BBP para calcular los bits exactos de Pi. grep Cuenta las coincidencias de una expresi\u00f3n regular en la entrada. join Realiza una uni\u00f3n de conjuntos de datos ordenados con particiones equiparables. multifilewc Cuenta las palabras de varios archivos. pentomino Programa para la colocaci\u00f3n de mosaicos con el fin de encontrar soluciones a problemas de pentomin\u00f3. pi Calcula Pi mediante un m\u00e9todo cuasi Monte Carlo. randomtextwriter Escribe 10\u00a0GB de datos de texto aleatorios por nodo. <code>randomwriter</code> Escribe 10\u00a0GB de datos aleatorios por nodo. <code>secondarysort</code> Define una ordenaci\u00f3n secundaria para la fase de reducci\u00f3n. sort Ordena los datos escritos por el escritor aleatorio. sudoku un solucionador de sudokus. 1 par\u00e1metro de entrada, el archivo que representa el sudoku a resolver. teragen genera datos para la ordenaci\u00f3n de terabytes (terasort). terasort ejecuta la ordenaci\u00f3n de terabytes (terasort). teravalidate comprueba los resultados de la ordenaci\u00f3n de terabytes (terasort). wordcount Cuenta las palabras de los archivos de entrada. 2 par\u00e1metros en la llamada: - Ruta al fichero del que se quiere contar - Directorio de salida, donde crear\u00e1 los ficheros de salida <code>wordmean</code> Cuenta la longitud media de las palabras de los archivos de entrada. <code>wordmedian</code> Cuenta la mediana de las palabras de los archivos de entrada. wordstandarddeviation Cuenta la desviaci\u00f3n est\u00e1ndar de la longitud de las palabras de los archivos de entrada."},{"location":"UD01/3.ejercicios/#ejercicios","title":"Ejercicios","text":"<pre><code>**1. Cuenta las veces que aparece la palabra \"allowed\" en los ficheros de configuraci\u00f3n de hadoop.**\n** Los ficheros de configuraci\u00f3n de hadoop son archivos XML, busca todos los archivos y copialos \nen una carpeta, por ejemplo:\n/tmp/ejercicio1\n\n**2. Resuelve el siguiente sudoku:**\n8 5 ? 3 9 ? ? ? ?\n? ? 2 ? ? ? ? ? ?\n? ? 6 ? 1 ? ? ? 2\n? ? 4 ? ? 3 ? 5 9\n? ? 8 9 ? 1 4 ? ?\n3 2 ? 4 ? ? 8 ? ?\n9 ? ? ? 8 ? 5 ? ?\n? ? ? ? ? ? 2 ? ?\n? ? ? ? 4 5 ? 7 8\n\nPista: jar tf hadoop-mapreduce-examples-3.3.6.jar | grep -i sudoku\n\n**3. Cuenta las palabras del libro \"Romeo y Julieta\"**\nhttps://raw.githubusercontent.com/lynnlangit/learning-hadoop-and-spark/master/0b-Example-Datasets/shakespeare-davinci/romeo.txt\n\n** Ejemplo:\nhttps://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Source_Code\n</code></pre>"},{"location":"UD01/3.ejercicios/#posibles-errores","title":"Posibles errores","text":"<p>ERROR 1 El directorio /tmp/salida ya existe, dar\u00e1 error si no se borra</p> <p>ERROR 2 Cannot execute /home/debian/hadoop-3.2.3//libexec/hadoop-config.sh En .bashrc hemos a\u00f1adido una barra al final de HADOOP_HOME Modificar archivo, guardar cambios y luego ejecutar: source .bashrc (para que los cambios se apliquen a la sesi\u00f3n) ERROR: JAVA_HOME /usr/lib/jvm/java-17-openjdk-amd64 does not exist La variable de sesi\u00f3n JAVA_HOME no es correcta, revisa la carpeta de java</p> <p>ERROR 3 ERROR: JAVA_HOME /usr/lib/jvm/java-17-openjdk-amd64 does not exist La variable de sesi\u00f3n JAVA_HOME no es correcta, revisa la carpeta de java</p>"},{"location":"UD01/3.ejerciciosSOL/","title":"UD01 3b. Hadoop, ejercicios (soluciones).","text":""},{"location":"UD01/3.ejerciciosSOL/#1-ejemplos-de-mapreduce","title":"1. Ejemplos de mapreduce:","text":"<p>El fichero de ejemplos hadoop-mapreduce-examples, contiene los siguientes:</p> aggregatewordcount Cuenta las palabras de los archivos de entrada. aggregatewordhist Calcula el histograma de las palabras de los archivos de entrada. <code>bbp</code> Uusa una f\u00f3rmula Bailey-Borwein-Plouffe para calcular los d\u00edgitos exactos de Pi. dbcount Cuenta los registros de vistas de p\u00e1gina almacenados en una base de datos. distbbp Usa una f\u00f3rmula de tipo BBP para calcular los bits exactos de Pi. grep Cuenta las coincidencias de una expresi\u00f3n regular en la entrada. join Realiza una uni\u00f3n de conjuntos de datos ordenados con particiones equiparables. multifilewc Cuenta las palabras de varios archivos. pentomino Programa para la colocaci\u00f3n de mosaicos con el fin de encontrar soluciones a problemas de pentomin\u00f3. pi Calcula Pi mediante un m\u00e9todo cuasi Monte Carlo. randomtextwriter Escribe 10\u00a0GB de datos de texto aleatorios por nodo. <code>randomwriter</code> Escribe 10\u00a0GB de datos aleatorios por nodo. <code>secondarysort</code> Define una ordenaci\u00f3n secundaria para la fase de reducci\u00f3n. sort Ordena los datos escritos por el escritor aleatorio. sudoku un solucionador de sudokus. 1 par\u00e1metro de entrada, el archivo que representa el sudoku a resolver. teragen genera datos para la ordenaci\u00f3n de terabytes (terasort). terasort ejecuta la ordenaci\u00f3n de terabytes (terasort). teravalidate comprueba los resultados de la ordenaci\u00f3n de terabytes (terasort). wordcount Cuenta las palabras de los archivos de entrada. 2 par\u00e1metros en la llamada: - Ruta al fichero del que se quiere contar - Directorio de salida, donde crear\u00e1 los ficheros de salida <code>wordmean</code> Cuenta la longitud media de las palabras de los archivos de entrada. <code>wordmedian</code> Cuenta la mediana de las palabras de los archivos de entrada. wordstandarddeviation Cuenta la desviaci\u00f3n est\u00e1ndar de la longitud de las palabras de los archivos de entrada."},{"location":"UD01/3.ejerciciosSOL/#2-ejercicios","title":"2. Ejercicios","text":"<pre><code>1. Cuenta las veces que aparece la palabra \"allowed\" en los ficheros de configuraci\u00f3n de hadoop.\n\n2. Resuelve el siguiente sudoku:\n8 5 ? 3 9 ? ? ? ?\n? ? 2 ? ? ? ? ? ?\n? ? 6 ? 1 ? ? ? 2\n? ? 4 ? ? 3 ? 5 9\n? ? 8 9 ? 1 4 ? ?\n3 2 ? 4 ? ? 8 ? ?\n9 ? ? ? 8 ? 5 ? ?\n? ? ? ? ? ? 2 ? ?\n? ? ? ? 4 5 ? 7 8\n\nPista: jar tf hadoop-mapreduce-examples-3.3.6.jar | grep -i sudoku\n\n3. Cuenta las palabras del libro \"Romeo y Julieta\"\nhttps://raw.githubusercontent.com/lynnlangit/learning-hadoop-and-spark/master/0b-Example-Datasets/shakespeare-davinci/romeo.txt\n\nEjemplo:\nhttps://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Source_Code\n</code></pre> <ul> <li> <p>Soluciones</p> <pre><code>**2. Sudoku**\n$ pwd\n/opt/hadoop/share/hadoop/mapreduce\n$ mkdir /tmp/sudoku/entrada\n$ wget https://raw.githubusercontent.com/naver/hadoop/master/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/puzzle1.dta -P /tmp/sudoku/entrada/\n**SOLUCION**\n$ hadoop jar hadoop-mapreduce-examples-3.3.6.jar sudoku /tmp/input/puzzle1.dta\n\n**3. Cuenta palabras**\n$ hadoop jar hadoop-mapreduce-examples-3.3.6.jar wordcount /tmp/input/romeo.txt /tmp/salidaromeo\n</code></pre> </li> </ul>"},{"location":"UD01/3.ejerciciosSOL/#3-errores","title":"3. Errores","text":"<p>ERROR 1 Cannot execute /home/debian/hadoop-3.2.3//libexec/hadoop-config.sh En .bashrc hemos a\u00f1adido una barra al final de HADOOP_HOME Modificar archivo, guardar cambios y luego ejecutar: source .bashrc (para que los cambios se apliquen a la sesi\u00f3n) ERROR: JAVA_HOME /usr/lib/jvm/java-17-openjdk-amd64 does not exist La variable de sesi\u00f3n JAVA_HOME no es correcta, revisa la carpeta de java</p> <p>ERROR 2 ERROR: JAVA_HOME /usr/lib/jvm/java-17-openjdk-amd64 does not exist La variable de sesi\u00f3n JAVA_HOME no es correcta, revisa la carpeta de java</p>"},{"location":"UD01/_TODO/","title":"TODO","text":"<ul> <li> <p>Instalar tot en dockers</p> </li> <li> <p>Crear un container b\u00e0sic en la instalaci\u00f3 de hadoop + DFS (sense yarn)</p> </li> </ul>"},{"location":"UD01/_PENDENT/UD01%20-%20Big%20Data%20Hadoop%20118e913de6c4806493b8e271ad30161a/","title":"UD01 - Big Data. Hadoop.","text":""},{"location":"UD01/_PENDENT/UD01%20-%20Big%20Data%20Hadoop%20118e913de6c4806493b8e271ad30161a/#teoria","title":"Teor\u00eda","text":"<p>UD01 - Hadoop. Introduccio\u0301n.pdf</p> <p>Presentaci\u00f3n, diapositivas</p> <p></p> <p>https://docs.google.com/presentation/d/1M3rEkf7FvIfpyjGgWZ-rXq5TxPM_zb1xgQF8WBmRPmk/edit?usp=sharing</p>"},{"location":"UD01/_PENDENT/UD01%20-%20Big%20Data%20Hadoop%20118e913de6c4806493b8e271ad30161a/#practica","title":"Pr\u00e1ctica","text":"<p>https://www.profesionalreview.com/2020/08/16/como-usar-virtualbox/</p> <p>UD01 1. Instalaci\u00f3n Ubuntu server. Configuraci\u00f3n entorno.</p> <p>UD01 2. Descarga y configuraci\u00f3n Hadoop.</p> <p>UD01 3. Hadoop, ejercicios.</p> <p>UD01 3b. Hadoop, ejercicios (soluciones).</p>"},{"location":"UD010-Practicas/1.clusterdocker/","title":"Creaci\u00f3n de un cluster real con hadoop","text":"<p>Creaci\u00f3n de un cluster con varios nodos utilizando contenedores.</p> <p>https://www.youtube.com/watch?v=f6FJ91f-qpA&amp;list=PLzp6eJhjmaiDX5vPppsjQwYsO2yPQR1m6&amp;index=1</p> <p></p>"},{"location":"UD02/1.instalacion/","title":"HDFS. Cluster Hadoop pseudodistribuido.","text":"<p>Custer Pseudodistribuido</p> <p>Tambi\u00e9n conocido como Cluster de un solo nodo. Como solo tenemos un nodo, el mismo nodo ser\u00e1 el maestro y el esclavo (no pasar\u00e1 nunca en producci\u00f3n ya que siempre tendremos un maestro y varios esclavos).</p>"},{"location":"UD02/1.instalacion/#requisitos","title":"Requisitos","text":"<ul> <li>Hadoop descargado y descomprimido en /opt (lo hicimos anteriormente).</li> <li>El usuario hadoop ha de poder conectarse como usuario hadoop sin password<ul> <li>El usuario <code>hadoop</code>ha de poder realizar un <code>ssh hadoop@localhost</code> y ha de loguearse autom\u00e1ticamente, sin que se le pida password.</li> </ul> </li> </ul>"},{"location":"UD02/1.instalacion/#creacion-configuracion-del-cluster","title":"Creaci\u00f3n / configuraci\u00f3n del cluster","text":"<p>Como usuario <code>hadoop</code> Accedemos por ssh a la m\u00e1quina virtual en la que hab\u00edamos descomprimido hadoop.</p> <pre><code>cd /opt/hadoop/etc/hadoop/\n\n$ ls\ncapacity-scheduler.xml      hadoop-user-functions.sh.example  kms-log4j.properties        ssl-client.xml.example\nconfiguration.xsl           hdfs-rbf-site.xml                 kms-site.xml                ssl-server.xml.example\ncontainer-executor.cfg      **hdfs-site.xml**                     log4j.properties            user_ec_policies.xml.template\n**core-site.xml**               httpfs-env.sh                     mapred-env.cmd              workers\nhadoop-env.cmd              httpfs-log4j.properties           mapred-env.sh               yarn-env.cmd\nhadoop-env.sh               httpfs-site.xml                   mapred-queues.xml.template  yarn-env.sh\nhadoop-metrics2.properties  kms-acls.xml                      **mapred-site.xml**             yarnservice-log4j.properties\nhadoop-policy.xml           kms-env.sh                        shellprofile.d              **yarn-site.xml**\n</code></pre> <p>Ficheros m\u00e1s importantes. <pre><code>core-site.xml \u2192 configuraci\u00f3n general del cluster.\nhdfs-site.xml \u2192 configuraci\u00f3n sistema de ficheros hdfs.\nmapred-site.xml \u2192 configuraci\u00f3n de mapreduce. # de moment no\nhadoop-env.sh -&gt; entorno del servidor\nyarn-site.xml \u2192 configuraci\u00f3n del modo de trabajo del proceso yarn. # de moment no\n</code></pre></p>"},{"location":"UD02/1.instalacion/#core-sitexml","title":"core-site.xml","text":"<p>Inicialmente est\u00e1 vac\u00edo.</p> <p>Lo modificamos para indicarle: name \u2192 Qu\u00e9 sistema de fichero vamos a utilizar en hadoop (por defecto hdfs, hay otros). value \u2192 D\u00f3nde se encuentra el servidor maestro que va a contener los datos (Namenode) estar\u00e1 en nodo1 (la m\u00e1quina donde me encuentro, puerto 9000)</p> <pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://HOSTNAME:9000&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"UD02/1.instalacion/#hdfs-sitexml","title":"hdfs-site.xml","text":"<p><code>dfs.replication</code> \u2192 Por defecto cada bloque se replica 3 veces, como tenemos 1 nodo, indicamos que solo hay 1 nodo que no replique.</p> <p><code>dfs.namenode.name.dir</code>\u2192 D\u00f3nde se encuentra la informaci\u00f3n del maestro (los metadatos que guarda el maestro). Solo se indica en los clusters \u201cmaestro\u201d, pero como estamos haciendo un cluster pseudodistribuido lo indicamos.</p> <p><code>dfs.datanode.data.dir</code>\u2192 En cada esclavo d\u00f3nde se guardan los datos. Solo se indica en los clusters \u201cesclavos\u201d, pero como estamos haciendo un cluster pseudodistribuido lo indicamos.</p> <pre><code>&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n        &lt;value&gt;/datos/namenode&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n        &lt;value&gt;/datos/datanode&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"UD02/1.instalacion/#hadoop-envsh","title":"hadoop-env.sh","text":"<pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n</code></pre>"},{"location":"UD02/1.instalacion/#directorios-y-sistema-de-ficheros","title":"Directorios y sistema de ficheros","text":"<p>Como usuario <code>root</code> 1. Creamos los directorios configurados en el punto anterior <pre><code>/datos/namenode\n/datos/datanode\n</code></pre></p> <ol> <li>Cambiamos el propietario y grupo de /datos a <code>hadoop</code>. <pre><code>$ ls -la /datos\ntotal 16\ndrwxr-xr-x  4 hadoop hadoop 4096 oct 26 08:28 .\ndrwxr-xr-x 19 root   root   4096 oct 26 08:28 ..\ndrwxr-xr-x  2 hadoop hadoop 4096 oct 26 08:28 datanode\ndrwxr-xr-x  2 hadoop hadoop 4096 oct 26 08:28 namenode\n</code></pre></li> </ol> <p>Como usuario <code>hadoop</code></p> <ol> <li>Creamos el sistema de ficheros del namenode (lo crear\u00e1 donde le hemos indicado en el xml). <pre><code># Formateamos el namenode\n$ hdfs namenode -format\n\n# Podemos de ver lo que ha creado...\n$ ls /datos/namenode/*\n</code></pre></li> </ol>"},{"location":"UD02/1.instalacion/#arrancamos-hdfs","title":"Arrancamos HDFS","text":"<p>Hadoop</p> <p>Hadoop tiene dos partes: DATOS y PROCESOS, actualmente estamos trabajando en la parte de DATOS.</p> <p>Arrancamos los procesos de HDFS. Debe arrancar el NAMENODE, el SECONDARY NAMENODE y el DATANODE.</p> <p>Vamos a la carpeta <code>sbin</code> dentro de hadoop. <pre><code>$ cd /opt/hadoop/sbin/\n$ start-dfs.sh\n# 1. Arranca namenode\n# 2. Arranca datanode\n# 3. Arranca secondarynamenode\n</code></pre></p> <p>ERROR 1</p> <p>$ start-dfs.sh Starting namenodes on [localhost] localhost: hadoop@localhost: Permission denied (publickey,password). Starting datanodes localhost: hadoop@localhost: Permission denied (publickey,password). Starting secondary namenodes [hadoopcasa] hadoopcasa: hadoop@hadoopcasa: Permission denied (publickey,password).</p> <p>SOLUCI\u00d3N: El problema es que el usuario hadoop no puede loguearse sin password: Como usuario <code>hadoop</code> : cd ssh-keygen -t rsa cd .ssh cat id_rsa.pub &gt;&gt; authorized_keys</p> <p>ERROR 2</p> <p>$ start-dfs.sh Starting namenodes on [localhost] localhost: ERROR: JAVA_HOME is not set and could not be found. Starting datanodes localhost: ERROR: JAVA_HOME is not set and could not be found. Starting secondary namenodes [hadoopcasa] hadoopcasa: ERROR: JAVA_HOME is not set and could not be found.</p> <p>SOLUCI\u00d3N: No has modificado el archivo hadoop-env.sh tal y como se indicaba anteriormente. La variable JAVA_HOME la hab\u00edamos definido ya en .bashrc, tambi\u00e9n se ha de definir en hadoop-env.sh</p> <p>Una vez arrancado hadoop correctamente, vamos a ver los procesos que ha creado.</p> <p>Dentro de las JDK de java, tenemos un comando para ver los procesos java en ejecuci\u00f3n:</p> <pre><code>$ jps\n3218 DataNode\n3143 NameNode\n3592 Jps\n3485 SecondaryNameNode\n\n# Tambi\u00e9n podemos utilizar\n$ ps -fe | grep java\n</code></pre> <p>Comprobamos las carpetas de datos</p> <pre><code>$ ls /datos/namenode/\ncurrent  in_use.lock\n\n# Ahora esta carpeta ya tiene datos\n$ ls -a /datos/datanode/\n.  ..  current  in_use.lock\n</code></pre> <p>Para acceder a la administraci\u00f3n HDFS via web se utilizan 2 puertos: * 50070 -&gt; Hadoop 2 (versiones anteriores de hadoop) * 9870 -&gt; Hadoop 3, versi\u00f3n actual.</p> <p>Por lo que podemos acceder a la web de administraci\u00f3n desde: http://HOSTNAME_O_IP:PUERTO http://192.168.64.14:9870/</p> <p>Una vez en la web de administraci\u00f3n, pesta\u00f1a DATANODES podemos acceder a un determinado nodo, en nuestro caso al ser un cluster pseudodistribuido solamente tenemos un nodo. http://debianh:9864/ \u2192 Menos opciones ya que accedemos a un nodo determinado</p> <p> </p>"},{"location":"UD02/1.instalacion/#3-trabajar-con-hfs","title":"3. Trabajar con HFS","text":"<p>En <code>/datos/namenode/current</code> hay 3 tipos de ficheros: <pre><code>-rw-r--r-- 1 hadoop hadoop      42 oct 26 09:09 edits_0000000000000000001-0000000000000000002\n-rw-r--r-- 1 hadoop hadoop      42 oct 26 10:09 edits_0000000000000000003-0000000000000000004\n-rw-r--r-- 1 hadoop hadoop      42 oct 26 12:34 edits_0000000000000000005-0000000000000000006\n-rw-r--r-- 1 hadoop hadoop 1048576 oct 26 12:34 edits_inprogress_0000000000000000007\n-rw-r--r-- 1 hadoop hadoop     401 oct 26 10:09 fsimage_0000000000000000004\n-rw-r--r-- 1 hadoop hadoop      62 oct 26 10:09 fsimage_0000000000000000004.md5\n-rw-r--r-- 1 hadoop hadoop     401 oct 26 12:34 fsimage_0000000000000000006\n-rw-r--r-- 1 hadoop hadoop      62 oct 26 12:34 fsimage_0000000000000000006.md5\n-rw-r--r-- 1 hadoop hadoop       2 oct 26 12:34 seen_txid\n-rw-r--r-- 1 hadoop hadoop     214 oct 26 08:35 VERSION\n</code></pre></p> <pre><code>$ cat VERSION\n#Thu Oct 26 08:35:28 CEST 2023\nnamespaceID=623528578\nblockpoolID=BP-1493409649-127.0.1.1-1698302128453\nstorageType=NAME_NODE\ncTime=1698302128453\nclusterID=CID-ca9bde81-b05d-4fcc-8c4c-b54887eada3b\nlayoutVersion=-66\n</code></pre> <p>Si paramos el cluster con <code>stop-dfs.sh</code> y lo volvemos a arrancar los n\u00fameros en los nombres de ficheros incrementan.</p> <ul> <li>edits_: cambios dentro de la base de datos de HDFS.</li> <li>edits_inprogress_: lo que se est\u00e1 escribiendo en este momento.</li> <li>fsimage_: copia \u201cfoto\u201d, de un momento en el tiempo del sistema de ficheros.</li> <li> <p>Fichero VERSION</p> <ul> <li> <p><code>namespaceID</code>: identificador \u00fanico para el sistema de archivos HDFS en un cl\u00faster. Este identificador se utiliza para distinguir entre diferentes instancias del sistema de archivos HDFS en cl\u00fasteres distintos. Cada cl\u00faster de Hadoop debe tener un <code>namespaceID</code> \u00fanico para evitar conflictos. Si clonas o replica un cl\u00faster Hadoop, es importante que el <code>namespaceID</code> sea diferente en cada cl\u00faster para que no haya confusiones.</p> </li> <li> <p><code>blockpoolID</code>: identificador \u00fanico para el \"block pool\" en HDFS. El \"block pool\" es una colecci\u00f3n de bloques de datos que se utilizan para almacenar los datos de los archivos en HDFS. Cada \"block pool\" tiene su propio <code>blockpoolID</code>, que se utiliza para diferenciar entre m\u00faltiples \"block pools\" en el mismo cl\u00faster. Esto es importante para la escalabilidad y la administraci\u00f3n de bloques en el sistema de archivos.</p> </li> <li> <p><code>clusterID</code>: identificador \u00fanico para el cl\u00faster Hadoop. Este valor se utiliza para identificar un cl\u00faster espec\u00edfico y es importante para asegurarse de que los nodos del cl\u00faster se est\u00e9n conectando al cl\u00faster correcto. El <code>clusterID</code> es necesario para garantizar que no haya problemas de conexi\u00f3n entre nodos cuando hay varios cl\u00fasteres en la misma red o en escenarios de recuperaci\u00f3n de desastres.</p> </li> </ul> </li> </ul>"},{"location":"UD02/1.instalacion/#otros-errores","title":"Otros errores","text":"<ol> <li> <p>Permission denied: user=dr.who, access=WRITE, inode=\"/\":hadoop:supergroup:drwxr-xr-x </p> </li> <li> <p>Error en el sistema de ficheros HDFS, podemos utilizar fsck</p> </li> </ol> <p>En el log de datanode: <code>$ tail -f logs/hadoop-hadoop-datanode-debianh.log</code></p> <p>2023-10-26 18:24:38,407 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool (Datanode Uuid 6dfbb007-34b1-4a07-a045-274aad0e2936) service to debianh/127.0.1.1:9000. Exiting.</p> <pre><code>$ hdfs fsck / -includeSnapshots\n# Borramos la carpeta current del namenode\n$ rm -rf /datos/datanode/current\n\n# Iniciamos datenode unicamente\n$ hdfs --daemon start datanode\n\n$ jps\n2688 NameNode\n3486 DataNode\n3582 Jps\n\n# Cuando ya no caiga DataNode\n$ start-all.sh\n</code></pre> <ol> <li>Eliminar los warnings Error: WARN util.NativeCodeLoader</li> </ol> <p></p> <p>No encuentra librer\u00edas nativas, no importa, es un warning (podr\u00eda ir m\u00e1s lento debido a este error)</p> <p>Para eliminar el warning hay que a\u00f1adir en .bashrc</p> <pre><code>export HADOOP_HOME_WARN_SUPRESS=1\nexport HADOOP_ROOT_LOGGER=\"WARN,DRFA\"\n</code></pre>"},{"location":"UD02/2.comandosHDFS/","title":"Comandos HDFS","text":"<p>ATENCI\u00d3N</p> <p>Hay que asegurarse que est\u00e1 arrancado HDFS</p> <p>El comando hdfs se encuentra de la carpeta <code>/opt/hadoop/bin</code>y el sistema busca en esta carpeta porque la hab\u00edamos incluido en nuestro <code>$PATH</code>.</p> <p>Mostramos todas las opciones posibles del comando <code>hdfs dfs</code>. El comando hdfs \"emula\" comandos Linux como ver\u00e1s posteriormente.</p> <p><pre><code>$ echo $PATH\n\n$ hdfs dfs #Muestra todos los comandos posibles\n</code></pre> </p> <p>Para visualizar el contenido actual del sistema de ficheros HDFS.</p> <pre><code>$ hdfs dfs -ls /\n</code></pre> <p>Ejercicio 1: - Crear un fichero simple en local (prueba.txt). - Crear un directorio en el sistema de ficheros DFS (temporal) - Copiar el archivo que hemos creado al directorio remoto.</p> <ul> <li>Soluci\u00f3n <pre><code>## Creamos un archivo con contenido\n$ echo \"Hola\" &gt; prueba.txt\n$ hdfs dfs -mkdir /temporal\n$ hdfs dfs -ls /\n# Subimos un fichero de nuestro sistema a HDFS\n$ hdfs dfs -put prueba.txt /temporal \n$ hdfs dfs -ls /\n</code></pre></li> </ul> <p>Desde el explorador la web de administraci\u00f3n, podemos ver informaci\u00f3n del archivo y bloques. </p> <p>Nos fijamos en <code>Bloock ID</code>  y <code>Bloock Pool</code></p> <p>\u00bfC\u00f3mo hacerlo a trav\u00e9s de un comando?</p> <pre><code>$ hdfs fsck /temporal/prueba.txt -files -blocks\n\nConnecting to namenode via http://debianh:9870/fsck?ugi=hadoop&amp;files=1&amp;blocks=1&amp;path=%2Fhola.txt\n    FSCK started by hadoop (auth:SIMPLE) from /127.0.0.1 for path /hola.txt at Mon Nov 06 14:08:52 CET 2023\n\n/temporal/prueba.txt 5 bytes, replicated: replication=1, 1 block(s):  OK\n 0. **BP-673766993-127.0.1.1-1698337131957**:blk_**1073741825**_1001 len=5 Live_repl=1\n</code></pre> <p>Ahora vamos a comprobar c\u00f3mo HDFS ha guardado el archivo en local. Para ello vamos a: <pre><code>$ cd /datos/datanode/current\n</code></pre></p> <p>Encontraremos una carpeta con el mismo nombre que el <code>Bloock Pool</code>, en mi caso entrar\u00e9 con:</p> <pre><code>cd BP-673766993-127.0.1.1-1698337131957\n# Accedemos a sus subdirectorios\ncd current/finalized/subdir0/subdir0/ \n</code></pre> <p>En este momento podremos ver el bloque que debe encajar con el <code>Block ID</code></p> <pre><code>$ ls\nblk_1073741825\n\n$ cat blk_1073741825\nhola\n</code></pre> <p>Podemos borrar todo recursivamente con: <pre><code>hdfs dfs -rm -r /temporal\n</code></pre></p>"},{"location":"UD02/2.comandosHDFS/#errores","title":"Errores","text":"<p>No aparece informaci\u00f3n del archivo  Se trata de un archivo vac\u00edo.</p>"},{"location":"UD02/3.comandosHDFSejercicios/","title":"Ejercicios hdfs","text":"<p>Pod\u00e9is encontrar los archivos de datos en el siguiente repositorio: https://github.com/josepgarcia/datos</p>"},{"location":"UD02/3.comandosHDFSejercicios/#ejercicio-1-fichero-de-logs","title":"Ejercicio 1. Fichero de logs.","text":"<ul> <li>Descargamos un fichero m\u00e1s grande (access_log.gz )</li> <li>Lo descomprimimos. Quedar\u00e1 un archivo de 482mb.</li> <li>Lo movemos a HDFS, dentro la carpeta </li> <li>Comprobamos en cuantos bloques se encuentra.</li> <li>\u00bfA qu\u00e9 archivos apunta en la m\u00e1quina virtual?</li> </ul>"},{"location":"UD02/3.comandosHDFSejercicios/#ejercicio-2","title":"Ejercicio 2","text":"<ol> <li>Crea un fichero \u201csaludo.txt\u201d en local, que contenga el texto \u201cHola\u201d.    S\u00fabelo a HDFS, a la carpeta /temporal (si no existe hay que crearla)    Borra el fichero en local    Muestra el contenido del fichero (en remoto).</li> <li>Copia el fichero saludo.txt a local con el nombre (saludolocal.txt)</li> <li>Entra a la web de administraci\u00f3n para ver que existe el fichero.</li> <li>Borra el fichero remoto.</li> <li>Aseg\u00farate que se ha borrado el fichero con ls.</li> <li>Borra el directorio temporal.</li> </ol>"},{"location":"UD02/3.comandosHDFSejercicios/#ejercicio-3","title":"Ejercicio 3","text":"<ol> <li>Crea un fichero \u201cotrosaludo.txt\u201d en local, que contenga el texto \u201cHola\u201d. MU\u00c9VELO a HDFS, dentro de la carpeta /ejercicios/saludos/    Comprueba que ya no existe el fichero en local</li> <li>Crea un directorio en local llamado prueba    Dentro de este directorio crea un fichero llamado ejercicioprueba.txt    Mueve todo el directorio prueba a HDFS, dentro de la carpeta /ejercicios    Comprueba que ya no existe la carpeta en local    Realiza una copia de HDFS a local de la carpeta que acabas de subir.</li> </ol>"},{"location":"UD02/3.comandosHDFSejercicios/#ejercicio-4","title":"Ejercicio 4","text":"<ol> <li>Crea un archivo en /tmp llamado archivogrande que tenga un tama\u00f1o de 500MB (aproximadamente)  <code>Utiliza el comando dd para crear el fichero</code></li> <li>Crea una carpeta en HDFS llamada datos2.</li> <li>Sube el archivo a la carpeta creada.</li> </ol>"},{"location":"UD02/3.comandosHDFSejerciciosSOL/","title":"Ejercicios hdfs","text":"<p>Pod\u00e9is encontrar los archivos de datos en el siguiente repositorio: https://github.com/josepgarcia/datos</p>"},{"location":"UD02/3.comandosHDFSejerciciosSOL/#ejercicio-1-fichero-de-logs","title":"Ejercicio 1. Fichero de logs.","text":"<ul> <li>Descargamos un fichero m\u00e1s grande (access_log.gz )</li> <li>Lo descomprimimos. Quedar\u00e1 un archivo de 482mb.</li> <li>Lo movemos a HDFS, dentro la carpeta </li> <li>Comprobamos en cuantos bloques se encuentra.</li> <li>\u00bfA qu\u00e9 archivos apunta en la m\u00e1quina virtual?</li> </ul>"},{"location":"UD02/3.comandosHDFSejerciciosSOL/#ejercicio-2","title":"Ejercicio 2","text":"<ol> <li>Crea un fichero \u201csaludo.txt\u201d en local, que contenga el texto \u201cHola\u201d.    S\u00fabelo a HDFS, a la carpeta /temporal (si no existe hay que crearla)    Borra el fichero en local    Muestra el contenido del fichero (en remoto).</li> <li>Copia el fichero saludo.txt a local con el nombre (saludolocal.txt)</li> <li>Entra a la web de administraci\u00f3n para ver que existe el fichero.</li> <li>Borra el fichero remoto.</li> <li>Aseg\u00farate que se ha borrado el fichero con ls.</li> <li>Borra el directorio temporal. <pre><code>$ hdfs dfs -cat /temporal/prueba.txt \n$ hdfs dfs -get /temporal/prueba.txt /tmp/borrar.txt \n$ hdfs dfs -mkdir directorio1 $ hdfs dfs -rm /temporal/prueba.txt\n</code></pre></li> </ol>"},{"location":"UD02/3.comandosHDFSejerciciosSOL/#ejercicio-3","title":"Ejercicio 3","text":"<ol> <li>Crea un fichero \u201cotrosaludo.txt\u201d en local, que contenga el texto \u201cHola\u201d. MU\u00c9VELO a HDFS, dentro de la carpeta /ejercicios/saludos/    Comprueba que ya no existe el fichero en local</li> <li>Crea un directorio en local llamado prueba    Dentro de este directorio crea un fichero llamado ejercicioprueba.txt    Mueve todo el directorio prueba a HDFS, dentro de la carpeta /ejercicios    Comprueba que ya no existe la carpeta en local    Realiza una copia de HDFS a local de la carpeta que acabas de subir. <pre><code># Move file / Folder from Local disk to HDFS \n$ hdfs dfs -moveFromLocal /local-file-path /hdfs-file-path \n# Copy \n$ hdfs dfs -copyFromLocal /hdfs-file-path /local-file-path \n# Move a File to HDFS from Local \n$ hdfs dfs -moveToLocal /hdfs-file-path /local-file-path \n# Copy \n$ hdfs dfs -copyToLocal /hdfs-file-path /local-file-path\n</code></pre> https://sparkbyexamples.com/apache-hadoop/hadoop-hdfs-dfs-commands-and-starting-hdfs-dfs-services/</li> </ol>"},{"location":"UD02/3.comandosHDFSejerciciosSOL/#ejercicio-4","title":"Ejercicio 4","text":"<ol> <li>Crea un archivo en /tmp llamado archivogrande que tenga un tama\u00f1o de 500MB (aproximadamente)     -&gt; Utiliza el comando dd</li> <li>Crea una carpeta en HDFS llamada datos2.</li> <li>Sube el archivo a la carpeta creada. <pre><code># Crear archivo de 500Mb \ndd if=/dev/zero of=/tmp/archivogrande bs=1024 count=512k \n# Verificar el tama\u00f1o del archivo creado \ndu -sh /tmp/archivogrande\n</code></pre></li> </ol>"},{"location":"UD02/4.administracionHDFS/","title":"Administraci\u00f3n del cluster HDFS","text":""},{"location":"UD02/4.administracionHDFS/#dfsadmin","title":"dfsadmin","text":"<p>Con la herramienta <code>dfsadmin</code> podemos examinar el estado del cluster HDFS.</p> <pre><code>$ hdfs dfsadmin \n\n# Ejemplo: \n$ hdfs dfsadmin -safemode enter\n</code></pre> <p>Opciones dfsadmin</p> <ol> <li>hdfs dfsadmin -report: Resumen completo del sistema HDFS; estado de todos los nodos del cl\u00faster, su capacidad, el espacio utilizado\u2026</li> <li>hdfs fsck: Comprobar la integridad del sistema de archivos HDFS. Para verificar un directorio hay que a\u00f1adirlo como segundo par\u00e1metro. <code>hdfs fsck /datos/prueba</code></li> <li>hdfs dfsadmin -printTopology: Este comando revela la topolog\u00eda del cl\u00faster HDFS. Proporciona informaci\u00f3n sobre la distribuci\u00f3n de nodos y muestra a qu\u00e9 rack pertenece cada nodo.</li> <li>hdfs dfsadmin -listOpenFiles: Una tarea cr\u00edtica en la administraci\u00f3n de HDFS es garantizar que no haya archivos abiertos que puedan causar problemas en el sistema. Este comando lista todos los archivos abiertos en el cl\u00faster, lo que facilita la identificaci\u00f3n y soluci\u00f3n de problemas.</li> <li>hdfs dfsadmin -safemode enter: El modo seguro de HDFS es una caracter\u00edstica importante para prevenir modificaciones accidentales en el sistema de archivos. Al ingresar al modo seguro, se evita la escritura y modificaci\u00f3n de datos en el cl\u00faster. Esto puede ser \u00fatil durante operaciones de mantenimiento o actualizaciones cr\u00edticas.</li> <li>hdfs dfsadmin -safemode leave: Cuando se completa la operaci\u00f3n en modo seguro y se requiere que el sistema vuelva a estar en pleno funcionamiento, se puede salir del modo seguro con este comando.</li> </ol>"},{"location":"UD02/4.administracionHDFS/#ejercicios","title":"\u270f\ufe0f Ejercicios","text":"<ol> <li>Muestra un resumen de recursos. </li> <li>Comprueba el estado del sistema de ficheros. </li> <li>Comprueba el estado del directorio datos2. </li> <li>Muestra la topolog\u00eda actual.     Hadoop es consciente de los racks que tiene la m\u00e1quina (se lo decimos nosotros), as\u00ed organiza mejor los bloques (los replica en racks distintos) </li> <li>\u00bfHay alg\u00fan fichero abierto?</li> </ol> Soluci\u00f3n <p>$ hdfs dfsadmin -report       \"Under replicated blocks\" -&gt; Si configuramos como r\u00e9plica por ejemplo 3 y hay alg\u00fan bloque con menos. Si esto baja de un determinado porcentaje, hadoop se pone en modo seguro porque piensa que hay alg\u00fan problema.   $ hdfs fsck /   $ hdfs fsck /datos2   $ hdfs dfsadmin -printTopology   $ hdfs dfsadmin -listOpenFiles</p>"},{"location":"UD02/6.snapshots/","title":"Snapshots","text":"<p>SNAPSHOT</p> <p>Un snapshot es una \"foto\" de un sistema de ficheros en un momento determinado. Puede ser utilizado para operaciones de: backup, recuperaci\u00f3n, para mantener un hist\u00f3rico...</p> <p>https://labex.io/tutorials/hadoop-how-to-restore-a-directory-from-a-snapshot-in-hadoop-hdfs-414945</p> <p>https://labex.io/tutorials/hadoop-how-to-check-contents-of-a-restored-snapshot-in-hadoop-hdfs-414942</p> <p>En esta pr\u00e1ctica vamos a realizar un snapshot y utilizarlo para recuperar un fichero que hemos borrado accidentalmente.</p> <p>Pasos a seguir:</p> <ol> <li>Creamos un directorio llamado <code>datos</code> dentro de DFS</li> <li>Subimos a ese directorio un fichero llamado <code>f1.txt</code> que contenga \u201cEsto es una prueba\u201d.</li> </ol> <p>Ahora vamos a ver d\u00f3nde ha dejado este fichero: - \u00bfC\u00f3mo se localiza el fichero desde webdfs? - \u00bfC\u00f3mo se localiza el fichero desde la l\u00ednea de comandos?</p> <p></p> Soluci\u00f3n <p><code>$ hdfs fsc /datos/f1.txt -blocks -locations -files</code></p>"},{"location":"UD02/6.snapshots/#realizar-el-snapshot","title":"Realizar el snapshot","text":"<ol> <li>Activamos para que se puedan hacer snapshots y realizamos uno</li> </ol> <pre><code>hdfs dfsadmin -allowSnapshot /datos\n\nhdfs dfs -createSnapshot /datos snap1\n## La creaci\u00f3n del snapshot no se hace con dfsadmin sino con dfs\n\nhdfs dfs -ls /datos/.snapshot/snap1\n## Nos muestra el fichero tal y como estaba cuando hicimos el snap\n</code></pre> <ul> <li>\u00bfEl snapshot aparece en webdfs? Compru\u00e9balo</li> </ul>"},{"location":"UD02/6.snapshots/#borrar-y-recuperar-fichero","title":"Borrar y recuperar fichero","text":"<p>Una vez tenemos la copia realizada borramos el fichero f1.txt en HDFS <pre><code>hdfs dfs -rm /datos/f1.txt   \n</code></pre></p> <p>Lo podemos recuperar desde el snapshot con:</p> <pre><code>hadoop fs -cp /datos/.snapshot/snap1/f1.txt /datos/\n## -cp solo hace copias entre HDFS, para \"sacar\" el fichero de HDFS hay que utilizar -get\n\nhdfs dfs -ls /datos\n</code></pre>"},{"location":"UD02/6.snapshots/#borrar-snapshot","title":"Borrar snapshot","text":"<pre><code>hdfs dfs -deleteSnapshot /datos snap1\n</code></pre>"},{"location":"UD02/6.snapshots/#desactivar-snapshots","title":"Desactivar snapshots","text":"<pre><code>hdfs dfsadmin -disallowSnapshot /datos\n</code></pre> <p>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html</p>"},{"location":"UD02/7.HDFSdesdePython/","title":"Consulta HDFS desde python","text":"<p>En la misma m\u00e1quina donde tenemos instalado hadoop:</p> <ol> <li>Creamos un entorno virtual.</li> <li>Instalamos los paquetes:<ul> <li>hdfs</li> <li>pandas</li> </ul> </li> <li>Ejecutamos el siguiente script.</li> </ol> <pre><code>import pandas as pd\nfrom hdfs import InsecureClient\n\nclient = InsecureClient('http://localhost:9870')\n\nwith client.read('/productos.csv') as file:\n    df = pd.read_csv(file)\n\nprint(df.head())\n\n\nprint(df.info())\n\nprint(df.describe().transpose())\n\nprint(client.list('/'))\n\n# Bajando un archivo de Hadoop a nuestro local\nclient.download('/productos.csv', './productos.csv')\n\n# Bajando multiples archivos\n#for file in client.list('/'):\n#    client.download(f'/{file}', './csv')\n\nclient.upload('/','test.py')\n\nprint(client.list('/'))\n\nclient.delete('/test.py')\n</code></pre>"},{"location":"UD02/8.clusterdocker/","title":"Configuraci\u00f3n de un Cl\u00faster Hadoop Pseudo-Distribuido en Docker","text":"<p>\u00daltima Versi\u00f3n</p> <p>Puedes encontrar una versi\u00f3n actualizada de este contenedor en: https://github.com/AdrianPerez3/hadoop-pseudo-cluster-docker</p> <p>Este documento proporciona una gu\u00eda detallada para configurar un cl\u00faster Hadoop en modo pseudo-distribuido utilizando Docker. Al final de esta gu\u00eda, tendr\u00e1s un entorno Hadoop completamente funcional en un contenedor Docker, ideal para prop\u00f3sitos de desarrollo y pruebas.</p>"},{"location":"UD02/8.clusterdocker/#prerrequisitos","title":"Prerrequisitos","text":"<ul> <li>Docker instalado en tu sistema.</li> <li>Conocimientos b\u00e1sicos de Linux y Docker.</li> <li>Conexi\u00f3n a Internet para descargar Hadoop y las im\u00e1genes base.</li> </ul>"},{"location":"UD02/8.clusterdocker/#estructura-del-proyecto","title":"Estructura del Proyecto","text":"<pre><code>hadoop-pseudo-docker/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 core-site.xml\n\u2502   \u251c\u2500\u2500 hdfs-site.xml\n\u2502   \u251c\u2500\u2500 mapred-site.xml\n\u2502   \u2514\u2500\u2500 yarn-site.xml\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 hadoop-3.4.1.tar.gz\n\u2514\u2500\u2500 start-hadoop.sh\n</code></pre> <ul> <li>config/: Directorio que contiene los archivos de configuraci\u00f3n de Hadoop.</li> <li>Dockerfile: Archivo de construcci\u00f3n de la imagen Docker.</li> <li>hadoop-3.4.1.tar.gz: Archivo comprimido de Hadoop (debe descargarse previamente).</li> <li>start-hadoop.sh: Script para iniciar los servicios dentro del contenedor.</li> </ul>"},{"location":"UD02/8.clusterdocker/#descargar-hadoop","title":"Descargar Hadoop","text":"<p>Descarga la versi\u00f3n de Hadoop que deseas utilizar desde el sitio oficial de Apache Hadoop. En este caso, se utiliza Hadoop 3.4.1.</p> <pre><code>wget https://downloads.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n</code></pre>"},{"location":"UD02/8.clusterdocker/#configuracion-de-hadoop","title":"Configuraci\u00f3n de Hadoop","text":"<p>Crea los archivos de configuraci\u00f3n en el directorio <code>config/</code> con el siguiente contenido.</p>"},{"location":"UD02/8.clusterdocker/#core-sitexml","title":"core-site.xml","text":"<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"UD02/8.clusterdocker/#hdfs-sitexml","title":"hdfs-site.xml","text":"<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"UD02/8.clusterdocker/#mapred-sitexml","title":"mapred-site.xml","text":"<pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"UD02/8.clusterdocker/#yarn-sitexml","title":"yarn-site.xml","text":"<p>Configura YARN para soportar MapReduce.</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"UD02/8.clusterdocker/#archivo-dockerfile","title":"Archivo Dockerfile","text":"<p>El <code>Dockerfile</code> define c\u00f3mo se construir\u00e1 la imagen Docker. A continuaci\u00f3n, se presenta el contenido completo:</p> <pre><code># Base image\nFROM openjdk:11-jdk\n\n# Set environment variables\nENV HADOOP_VERSION=3.4.1\nENV HADOOP_HOME=/usr/local/hadoop\nENV JAVA_HOME=/usr/local/openjdk-11\nENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n\n# Install necessary packages\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y openssh-server wget rsync sudo &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Create hadoop user and group\nRUN groupadd hadoop &amp;&amp; \\\n    useradd -ms /bin/bash -g hadoop hadoop\n\n# Allow hadoop user to use sudo without password\nRUN echo \"hadoop ALL=(ALL) NOPASSWD:ALL\" &gt;&gt; /etc/sudoers\n\n# Configure SSH for hadoop user\nRUN mkdir -p /home/hadoop/.ssh &amp;&amp; \\\n    ssh-keygen -t rsa -f /home/hadoop/.ssh/id_rsa -q -N \"\" &amp;&amp; \\\n    cat /home/hadoop/.ssh/id_rsa.pub &gt;&gt; /home/hadoop/.ssh/authorized_keys &amp;&amp; \\\n    chown -R hadoop:hadoop /home/hadoop/.ssh &amp;&amp; \\\n    chmod 600 /home/hadoop/.ssh/authorized_keys\n\n# Install Hadoop\nCOPY hadoop-3.4.1.tar.gz /tmp/\nRUN tar -xzvf /tmp/hadoop-3.4.1.tar.gz -C /usr/local/ &amp;&amp; \\\n    mv /usr/local/hadoop-3.4.1 $HADOOP_HOME &amp;&amp; \\\n    rm /tmp/hadoop-3.4.1.tar.gz &amp;&amp; \\\n    chown -R hadoop:hadoop $HADOOP_HOME\n\n# Configure Hadoop environment variables\nRUN echo \"export JAVA_HOME=/usr/local/openjdk-11\" &gt;&gt; $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n\n# Copy configuration files\nCOPY config/* $HADOOP_HOME/etc/hadoop/\nRUN chown -R hadoop:hadoop $HADOOP_HOME/etc/hadoop/\n\n# Switch to hadoop user\nUSER hadoop\n\n# Format HDFS\nRUN $HADOOP_HOME/bin/hdfs namenode -format\n\n# Expose ports\nEXPOSE 9870 8088 9000 8042 22\n\n# Switch back to root to copy the start script\nUSER root\n\n# Copy the start script\nCOPY start-hadoop.sh /start-hadoop.sh\nRUN chmod +x /start-hadoop.sh\n\n# Switch to hadoop user\nUSER hadoop\n\n# Set entry point\nCMD [\"/start-hadoop.sh\"]\n</code></pre>"},{"location":"UD02/8.clusterdocker/#script-de-inicio-start-hadoopsh","title":"Script de Inicio: start-hadoop.sh","text":"<p>Crea el archivo <code>start-hadoop.sh</code> en el directorio ra\u00edz de tu proyecto con el siguiente contenido:</p> <pre><code>#!/bin/bash\n\n# Iniciar el servicio SSH (requiere privilegios de root)\nsudo service ssh start\n\n# Iniciar los servicios de Hadoop\n$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\n\n# Mantener el contenedor en ejecuci\u00f3n\ntail -f /dev/null\n</code></pre> <p>Concede permisos de ejecuci\u00f3n al script:</p> <pre><code>chmod +x start-hadoop.sh\n</code></pre>"},{"location":"UD02/8.clusterdocker/#construccion-y-ejecucion-del-contenedor","title":"Construcci\u00f3n y Ejecuci\u00f3n del Contenedor","text":""},{"location":"UD02/8.clusterdocker/#construir-la-imagen-docker","title":"Construir la Imagen Docker","text":"<p>Ejecuta el siguiente comando desde el directorio ra\u00edz del proyecto para construir la imagen Docker:</p> <pre><code>docker build -t hadoop-pseudo .\n</code></pre> <ul> <li><code>-t hadoop-pseudo</code>: Asigna el nombre <code>hadoop-pseudo</code> a la imagen.</li> <li><code>.</code>: Especifica el contexto de construcci\u00f3n como el directorio actual.</li> </ul>"},{"location":"UD02/8.clusterdocker/#ejecutar-el-contenedor-docker","title":"Ejecutar el Contenedor Docker","text":"<p>Inicia un contenedor a partir de la imagen creada:</p> <pre><code>docker run -it --name hadoop-container \\\n    -p 9870:9870 \\\n    -p 8088:8088 \\\n    -p 9000:9000 \\\n    -p 8042:8042 \\\n    hadoop-pseudo\n</code></pre> <ul> <li><code>-it</code>: Ejecuta el contenedor en modo interactivo.</li> <li><code>--name hadoop-container</code>: Asigna el nombre <code>hadoop-container</code> al contenedor.</li> <li><code>-p</code>: Mapea puertos para permitir el acceso a las interfaces web.</li> <li><code>hadoop-pseudo</code>:  Nombre de la imagen que hemos creado en el punto anterior.</li> </ul>"},{"location":"UD02/8.clusterdocker/#verificacion-de-la-configuracion","title":"Verificaci\u00f3n de la Configuraci\u00f3n","text":""},{"location":"UD02/8.clusterdocker/#comprobar-los-procesos-de-hadoop","title":"Comprobar los Procesos de Hadoop","text":"<p>Dentro del contenedor, ejecuta el comando <code>jps</code> para verificar los procesos en ejecuci\u00f3n:</p> <pre><code>jps\n</code></pre> <p>Deber\u00edas obtener una salida similar a:</p> <pre><code>XXXX NameNode\nXXXX DataNode\nXXXX ResourceManager\nXXXX NodeManager\nXXXX SecondaryNameNode\nXXXX Jps\n</code></pre> <p>Cada uno de estos procesos representa un componente clave de Hadoop.</p>"},{"location":"UD02/8.clusterdocker/#acceder-a-las-interfaces-web","title":"Acceder a las Interfaces Web","text":"<p>Verifica que las interfaces web est\u00e9n funcionando accediendo a las siguientes URL desde tu navegador: - NameNode UI: http://localhost:9870   Proporciona informaci\u00f3n sobre el sistema de archivos distribuido (HDFS).</p> <ul> <li>ResourceManager UI: http://localhost:8088   Muestra informaci\u00f3n sobre los recursos YARN y las aplicaciones en ejecuci\u00f3n.</li> </ul>"},{"location":"UD02/8.clusterdocker/#probar-comandos-hdfs","title":"Probar Comandos HDFS","text":"<p>Prueba el sistema de archivos HDFS creando un directorio:</p> <pre><code>hdfs dfs -mkdir /user/hadoop\nhdfs dfs -ls /user\n</code></pre>"},{"location":"UD02/8.clusterdocker/#acceder-al-contendor-creado","title":"Acceder al contendor creado","text":"<pre><code>docker exec -it hadoop-container /bin/bash\n</code></pre>"},{"location":"UD02/8.clusterdocker/#ejecutar-un-trabajo-de-mapreduce","title":"Ejecutar un Trabajo de MapReduce","text":"<p>Ejecuta un ejemplo de c\u00e1lculo de Pi para probar el framework MapReduce:</p> <pre><code>hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi 2 5\n</code></pre> <p>Este comando ejecutar\u00e1 un trabajo de MapReduce y mostrar\u00e1 una estimaci\u00f3n del valor de Pi.</p>"},{"location":"UD02/8b.clusterdocker/","title":"Versi\u00f3n 2 cl\u00faster docker","text":"<p>Hadoop 3.4.1 + Java 11</p> <p>En preparaci\u00f3n</p> <p>Archivo: <code>Dockerfile</code> <pre><code># Imagen base\nFROM openjdk:11-jdk\n\n# Establecer variables de entorno\nENV HADOOP_VERSION=3.4.1\nENV HADOOP_HOME=/opt/hadoop\nENV JAVA_HOME=/usr/local/openjdk-11\nENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n\n# Instalar paquetes necesarios\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y openssh-server curl vim wget rsync &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/* \n##   &amp;&amp; localedef -i es_ES -c -f UTF-8 -A /usr/share/locale/locale.alias es_ES.UTF-8\n\n# Crear usuario y grupo hadoop\nRUN groupadd hadoop &amp;&amp; \\\n    useradd -ms /bin/bash -g hadoop hadoop\n\n# Instalar sudo\nRUN apt-get update &amp;&amp; apt-get install -y sudo &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Permitir que el usuario hadoop use sudo sin contrase\u00f1a\nRUN echo \"hadoop ALL=(ALL) NOPASSWD:ALL\" &gt;&gt; /etc/sudoers\n\n# Configurar SSH para el usuario hadoop\nRUN mkdir -p /home/hadoop/.ssh &amp;&amp; \\\n    ssh-keygen -t rsa -f /home/hadoop/.ssh/id_rsa -q -N \"\" &amp;&amp; \\\n    cat /home/hadoop/.ssh/id_rsa.pub &gt;&gt; /home/hadoop/.ssh/authorized_keys &amp;&amp; \\\n    chown -R hadoop:hadoop /home/hadoop/.ssh &amp;&amp; \\\n    chmod 600 /home/hadoop/.ssh/authorized_keys\n\n# Instalar Hadoop\n#COPY hadoop-3.3.5.tar.gz /tmp/\n#RUN tar -xzvf /tmp/hadoop-3.3.5.tar.gz -C /usr/local/ &amp;&amp; \\\n    #mv /usr/local/hadoop-3.3.5 $HADOOP_HOME &amp;&amp; \\\n    #rm /tmp/hadoop-3.3.5.tar.gz &amp;&amp; \\\n    #chown -R hadoop:hadoop $HADOOP_HOME\n\n# Descargamos hadoop para instalarlo\nRUN curl -O https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz &amp;&amp; \\\n    tar -xzf hadoop-${HADOOP_VERSION}.tar.gz -C /opt/ &amp;&amp; \\\n    mv /opt/hadoop-${HADOOP_VERSION} $HADOOP_HOME &amp;&amp; \\\n    rm hadoop-${HADOOP_VERSION}.tar.gz &amp;&amp; \\\n    chown -R hadoop:hadoop $HADOOP_HOME\n\n\n# Configurar variables de entorno de Hadoop\nRUN echo \"export JAVA_HOME=/usr/local/openjdk-11\" &gt;&gt; $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n\n# Copiar archivos de configuraci\u00f3n\nCOPY config/* $HADOOP_HOME/etc/hadoop/\n#RUN chown -R hadoop:hadoop $HADOOP_HOME/etc/hadoop/\nRUN chown -R hadoop:hadoop $HADOOP_HOME\n\n# Cambiar al usuario hadoop\nUSER hadoop\n\n# Formatear HDFS\nRUN $HADOOP_HOME/bin/hdfs namenode -format\n\n# Exponer puertos\nEXPOSE 9870 8088 9000 8042 22\n\n# Volver al usuario root para copiar el script de inicio\nUSER root\n\n# Copiar el script de inicio\nCOPY start-hadoop.sh /start-hadoop.sh\nRUN chmod +x /start-hadoop.sh\n\n# Cambiar al usuario hadoop\nUSER hadoop\n\n# Definir el punto de entrada\nCMD [\"/start-hadoop.sh\"]\n</code></pre></p> <p>Archivo: <code>start-hadoop.sh</code> <pre><code>#!/bin/bash\n\n# Iniciar el servicio SSH (requiere privilegios de root)\nsudo service ssh start\n\n# Iniciar los servicios de Hadoop\n$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\n\n# Mantener el contenedor en ejecuci\u00f3n\ntail -f /dev/null\n</code></pre></p>"},{"location":"UD02/9.ejercicio1_wordcount/","title":"El quijote","text":"<p>https://medium.com/@guillermovc/setting-up-hadoop-with-docker-and-using-mapreduce-framework-c1cd125d4f7b</p> <p>https://www.geeksforgeeks.org/generating-word-cloud-python/</p>"},{"location":"UD02/_TODO/","title":"TODO","text":"<p>https://princetonits.com/blog/technology/how-to-configure-replication-factor-and-block-size-for-hdfs/</p>"},{"location":"UD02/_TODO/#integridad-ficheros","title":"Integridad ficheros","text":""},{"location":"UD02/_TODO/#api","title":"API","text":"<p>https://hadoop.apache.org/docs/r1.0.4/webhdfs.html</p> <p>https://hevodata.com/learn/webhdfs/</p>"},{"location":"UD02/_TODO/#dataset-demo","title":"Dataset demo","text":"<p>If you have trouble downloading the ml-100k data set from grouplens.org, use this download location instead:</p> <p>http://media.sundog-soft.com/es/ml-100k.zip</p> <p>Instalar dataset a trav\u00e9s de la l\u00ednea de comandos:</p> <p>https://www.udemy.com/course/the-ultimate-hands-on-hadoop-tame-your-big-data/learn/lecture/5951204#content</p> <p>*Cap\u00edtulo anterior, instalar a trav\u00e9s de Hortonworks</p>"},{"location":"UD03/0.index/","title":"Mapreduce - Yarn","text":"<p>Presentaci\u00f3n</p>"},{"location":"UD03/1.configurarclusterYARN/","title":"Configurar cluster YARN.","text":"<p>Mapreduce - Yarn</p> <p>Vamos a utilizar la versi\u00f3n 2 de MapReduce (YARN), la versi\u00f3n 1 est\u00e1 en desuso.</p>"},{"location":"UD03/1.configurarclusterYARN/#configurar-cluster","title":"Configurar cluster","text":"<p>Pasos a seguir:</p> <p>Paramos el cluster (user hadoop)</p> <pre><code>$ stop-dfs.sh\n</code></pre> <p>Vamos a la carpeta donde se encuentran los ficheros de configuraci\u00f3n</p> <pre><code>$ cd /opt/hadoop/etc/hadoop/\n</code></pre> <p>Editamos para indicar que el motor que gestiona mapreduce ser\u00e1 yarn:</p> <pre><code>$ vi mapred-site.xml\n\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;\n        &lt;value&gt;HADOOP_MAPRED_HOME=/opt/hadoop&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>A\u00f1adimos 3 propiedades a yarn-site.xml</p> <ol> <li>Indicamos donde se encuentra el yarn. Indicamos qui\u00e9n es el maestro que va a gestionar los procesos YARN.</li> <li>Servicios auxiliares que deben iniciar en el NodeManager (si por ejemplo creamos un servicio que se llame \u201cmyservice\u201d lo podr\u00edamos indicar aqu\u00ed).</li> <li>Cu\u00e1l es la clase que va a utilizar para hacer esa gesti\u00f3n.</li> </ol> <pre><code>$ vi yarn-site.xml\n\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n        &lt;value&gt;debianh&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;\n        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Arrancamos</p> <pre><code># Primero la parte de datos\n$ start-dfs.sh\n\n# Despu\u00e9s la parte de procesos\n$ start-yarn.sh\n</code></pre> <p>Comprobamos que todos los servicios est\u00e1n funcionando:</p> <pre><code>$ jps\n791 DataNode\n713 NameNode\n1294 NodeManager\n1214 ResourceManager\n927 SecondaryNameNode\n1647 Jps\n</code></pre>"},{"location":"UD03/1.configurarclusterYARN/#web-admin-yarn","title":"Web admin YARN","text":"<p>Se puede acceder a la web para ver la configuraci\u00f3n del yarn a trav\u00e9s del puerto 8088.</p> <p>Nos aseguramos que est\u00e9 el puerto abierto en la m\u00e1quina virtual:</p> <pre><code># Si no existe el comando netstat, debemos instalar el paquete net-tools\n\n$ netstat -anp | grep 8088\ntcp        0      0   0.0.0.0:8088          0.0.0.0:*               LISTEN      1214/java\n</code></pre>"},{"location":"UD03/1.configurarclusterYARN/#posible-error","title":"Posible error","text":"<p>Si no se puede acceder: https://github.com/apache/samza-hello-samza/pull/1</p> <p>Al configurar todo en el mismo cluster hay un problema con la propiedad que hemos a\u00f1adido en <code>yarn-site.xml</code> indicando el hostname. Por lo que:</p> <ol> <li>Borramos esta propiedad.</li> <li>Paramos yarn.</li> <li>Volvemos a arrancar yarn.</li> </ol> <pre><code> $ netstat -anp | grep 8088\n tcp        0      0 0.0.0.0:8088            0.0.0.0:*               LISTEN      2144/java\n</code></pre> <p>Ya deber\u00eda ser accesible: </p>"},{"location":"UD03/1.configurarclusterYARN/#opciones-web-admin-yarn","title":"Opciones Web admin YARN","text":"<p>Por defecto asume que tendr\u00e1 m\u00ednimo 1Gb y M\u00e1ximo 8Gb (da igual cuanta memoria tenga nuestra m\u00e1quina virtual)</p> <p>Pesta\u00f1a NODOS, nodos activos</p> <p>En nuestro caso habr\u00e1 1 nodo activo y nos podemos comunicar con \u00e9l por el puerto 8042</p> <p>Si pinchamos en el nodo podremos ver sus propiedades, aplicaciones que est\u00e1 lanzando, contenedores (recursos reales que se utilizan para hacer los procesos).</p> <p>Volvemos desde RM Home.</p> <p>Node Labels (para etiquetar los diferentes nodos, no lo vamos a utilizar.</p> <p>Applications: Las aplicaciones que tenemos funcionando en este momento.</p> <p>(Se pueden seleccionar por estado, NEW, NEW SAVING, SUBMITTED, ACCEPTED\u2026)</p> <p>Schenduler: M\u00e9tricas del planificador, podemos ver lo que ha ocurrido cuando lancemos una aplicaci\u00f3n. Por el momento aparecer\u00e1 vac\u00edo.</p> <p>Tools: Podemos ver la configuraci\u00f3n, logs, estad\u00edsticas\u2026.</p>"},{"location":"UD03/1.configurarclusterYARN/#errores","title":"Errores","text":"<p>Para los que us\u00e9is la versi\u00f3n 3 de Hadoop, a veces puede generar un error al realizar alguno de los ejemplos.</p> <p>Para solucionarlo, es necesario explicitar algunas librer\u00edas</p> <p>En concreto hay que a\u00f1adir en el yarn-site.xml la siguiente entrada (por supuesto adaptarlo a vuestro HADOOP_HOME.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;yarn.application.classpath&lt;/name&gt;\n    &lt;value&gt;\n      /opt/hadoop3/hadoop/etc/hadoop,\n      /opt/hadoop3/share/hadoop/common/*,\n      /opt/hadoop3/share/hadoop/common/lib/*,\n      /opt/hadoop3/share/hadoop/hdfs/*,\n      /opt/hadoop3/share/hadoop/hdfs/lib/*,\n      /opt/hadoop3/share/hadoop/mapreduce/*,\n      /opt/hadoop3/share/hadoop/mapreduce/lib/*,\n      /opt/hadoop3/share/hadoop/yarn/*,\n      /opt/hadoop3/share/hadoop/yarn/lib/*\n    &lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"UD03/2.fasesmapreduce/","title":"Fases MapReduce","text":"<p>MapReduce es un modelo de programaci\u00f3n utilizado en Hadoop para procesar y analizar grandes conjuntos de datos distribuidos en un cl\u00faster de computadoras. Las fases de MapReduce se dividen principalmente en dos pasos: Map y Reduce.</p> <p>La fase map estudia el problema, lo divide en trozos y los manda a diferentes m\u00e1quinas para que todos los trozos puedan ejecutarse concurrentemente.</p> <p>Los resultados de este proceso paralelo se recogen y se distribuyen a trav\u00e9s de un distintos servidores que ejecutan una funci\u00f3n \u201creduce\u201d, que toma los resultados de los trozos y los recombina para obtener una respuesta simple</p>"},{"location":"UD03/2.fasesmapreduce/#fase-map","title":"Fase: Map","text":"<p>Es la primera etapa del proceso y se encarga de transformar los datos de entrada en pares clave-valor. </p> <ol> <li>Divisi\u00f3n de Datos: La entrada se divide en fragmentos m\u00e1s peque\u00f1os llamados \"splits\". Cada split se asigna a un mapper.</li> <li> <p>Operaci\u00f3n de Mapeo: El mapper aplica una funci\u00f3n de mapeo a cada elemento del split, generando pares clave-valor. La clave identifica el dato y el valor es el resultado de la operaci\u00f3n de mapeo.</p> </li> <li> <p>Los programas Map Reduce se dividenen Mappers, tareas que se ejecutan en los nodos</p> </li> <li>Cada tarea MAP ataca a un solo bloque de datos HDFS</li> <li>Se ejecuta en el nodo donde reside el bloque (salvo excepciones)</li> </ol>"},{"location":"UD03/2.fasesmapreduce/#fase-shuffle-y-sort","title":"Fase: Shuffle y Sort","text":"<p>Despu\u00e9s de la fase Map, los resultados intermedios se agrupan seg\u00fan sus claves. Los datos con la misma clave se agrupan juntos y se env\u00edan a la fase Reduce. Adem\u00e1s, se realiza un proceso de ordenamiento para facilitar el trabajo de la fase Reduce.</p> <ul> <li>Ordena y consolida los datos intermedios (temporales) que generan los mappers</li> <li>Se lanzan despu\u00e9s de que todos los mappers hayan terminado y antes de que se lancen los procesos Reduce</li> </ul>"},{"location":"UD03/2.fasesmapreduce/#fase-reduce","title":"Fase: Reduce","text":"<ul> <li>Reduce: En esta fase, los datos agrupados se pasan a una funci\u00f3n de reducci\u00f3n (funci\u00f3n Reduce). Esta funci\u00f3n procesa los datos y realiza las operaciones finales. La salida de esta fase es el resultado final del procesamiento.</li> <li>Output: El resultado final despu\u00e9s de la fase Reduce se almacena en el sistema de archivos Hadoop o se utiliza para futuros an\u00e1lisis, seg\u00fan los requisitos del usuario. </li> </ul> <p>Ejemplo con m\u00e1s detalle</p> <p>En la fase Map, se divide y transforma la entrada en pares clave-valor, mientras que en la fase Reduce, se procesan y combinan estos pares para producir el resultado final. La fase Shuffle y Sort es crucial para organizar los datos intermedios y asegurar que los datos con la misma clave se env\u00eden al mismo proceso Reduce. Estas fases son esenciales para manejar eficientemente grandes conjuntos de datos distribuidos en un entorno de cl\u00faster. </p> <p>https://www.guru99.com/introduction-to-mapreduce.html</p>"},{"location":"UD03/2.fasesmapreduce/#paso-a-paso","title":"Paso a paso","text":"<p>Tenemos 3 documentos y vamos a contar las repeticiones de palabras:</p> <ol> <li>Documento 1: \"Hadoop es un framework para procesamiento distribuido.\"</li> <li>Documento 2: \"MapReduce es una parte integral de Hadoop.\"</li> <li>Documento 3: \"El procesamiento distribuido permite manejar grandes conjuntos de datos.\"</li> </ol>"},{"location":"UD03/2.fasesmapreduce/#fase-map_1","title":"Fase MAP","text":"<p>En esta fase, cada documento se divide en palabras y se asigna una clave a cada palabra con un valor de 1. Esto es lo que har\u00eda la funci\u00f3n Map.</p> <pre><code>Documento 1:\n\n(Hadoop, 1)\n(es, 1)\n(un, 1)\n(framework, 1)\n(para, 1)\n(procesamiento, 1)\n(distribuido, 1)\n</code></pre> <pre><code>Documento 2:\n\n(MapReduce, 1)\n(es, 1)\n(una, 1)\n(parte, 1)\n(integral, 1)\n(de, 1)\n(Hadoop, 1)\n</code></pre> <pre><code>Documento 3:\n\n(El, 1)\n(procesamiento, 1)\n(distribuido, 1)\n(permite, 1)\n(manejar, 1)\n(grandes, 1)\n(conjuntos, 1)\n(de, 1)\n(datos, 1)\n</code></pre>"},{"location":"UD03/2.fasesmapreduce/#fase-shuffle-sort","title":"Fase Shuffle - Sort","text":"<p>Los pares clave-valor se agrupan seg\u00fan la clave y se ordenan para facilitar la fase Reduce.</p> <pre><code>(conjuntos, [1])\n(datos, [1, 1])\n(de, [1, 1, 1])\n(distribuido, [1, 1])\n(El, [1])\n(es, [1, 1])\n(framework, [1])\n(grandes, [1])\n(Hadoop, [1, 1])\n(integral, [1])\n(manejar, [1])\n(MapReduce, [1])\n(para, [1])\n(parte, [1])\n(permite, [1])\n(procesamiento, [1, 1])\n(un, [1])\n(una, [1])\n</code></pre>"},{"location":"UD03/2.fasesmapreduce/#fase-reduce_1","title":"Fase Reduce","text":"<p>Se suman los valores para cada clave, produciendo el resultado final.</p> <pre><code>(conjuntos, 1)\n(datos, 3)\n(de, 3)\n(distribuido, 2)\n(El, 1)\n(es, 2)\n(framework, 1)\n(grandes, 1)\n(Hadoop, 2)\n(integral, 1)\n(manejar, 1)\n(MapReduce, 1)\n(para, 1)\n(parte, 1)\n(permite, 1)\n(procesamiento, 2)\n(un, 1)\n(una, 1)\n</code></pre>"},{"location":"UD03/2.fasesmapreduce/#ejemplos","title":"Ejemplos","text":""},{"location":"UD03/2.fasesmapreduce/#ejemplo-1-contador-de-palabras","title":"Ejemplo 1: Contador de palabras","text":"<p>Supongamos que tienes un conjunto de datos de palabras en varios documentos y quieres contar cu\u00e1ntas veces aparece cada palabra. El proceso de mapeo podr\u00eda ser algo as\u00ed:</p> <p>Entrada (fragmento de datos): </p> <pre><code>Hadoop es un framework, Hadoop procesa grandes conjuntos de datos\n</code></pre> <p>Operaci\u00f3n de Mapeo: Para cada palabra en el fragmento, el mapper emite pares clave-valor. Por ejemplo:</p> <pre><code>(Hadoop, 1)\n(es, 1)\n(un, 1)\n(framework, 1)\n(Hadoop, 1)\n(procesa, 1)\n(grandes, 1)\n(conjuntos, 1)\n(de, 1)\n(datos, 1)\n</code></pre> <p>Salida (conjunto intermedio): Las claves se agrupan, y los valores asociados a cada clave se colocan en una lista.</p> <pre><code>(Hadoop, [1, 1])\n(es, [1])\n(un, [1])\n(framework, [1])\n(procesa, [1])\n(grandes, [1])\n(conjuntos, [1])\n(de, [1])\n(datos, [1])\n</code></pre> <p>La fase Map genera este conjunto intermedio de pares clave-valor, que luego se utiliza en la fase Shuffle y Sort antes de pasar a la fase Reduce. Este proceso de mapeo se realiza de forma paralela en m\u00faltiples nodos del cl\u00faster, lo que permite un procesamiento eficiente de grandes conjuntos de datos.</p>"},{"location":"UD03/2.fasesmapreduce/#ejemplo-2-analisis-de-registros-de-acceso-a-un-servidor-web","title":"Ejemplo 2: An\u00e1lisis de Registros de Acceso a un Servidor Web","text":"<p>Supongamos que tienes registros de acceso a un servidor web en el siguiente formato:</p> <pre><code>Registro 1: /pagina1\nRegistro 2: /pagina2\nRegistro 3: /pagina1\nRegistro 4: /pagina3\nRegistro 5: /pagina2\nRegistro 6: /pagina1\n</code></pre> <p>Operaci\u00f3n de Mapeo: El objetivo es contar cu\u00e1ntas veces se ha visitado cada p\u00e1gina. Cada registro de acceso se procesar\u00eda individualmente en un mapper, y la operaci\u00f3n de mapeo generar\u00eda pares clave-valor. En este caso, la URL de la p\u00e1gina ser\u00eda la clave, y el valor ser\u00eda 1 para indicar una visita.</p> <pre><code>(/pagina1, 1)\n(/pagina2, 1)\n(/pagina1, 1)\n(/pagina3, 1)\n(/pagina2, 1)\n(/pagina1, 1)\n</code></pre> <p>Salida intermedia, sort, shuffle</p> <pre><code>(/pagina1, [1, 1, 1])\n(/pagina2, [1, 1])\n(/pagina3, [1])\n</code></pre> <p>Fase Reduce (Operaci\u00f3n de Reducci\u00f3n): En la fase Reduce, se sumar\u00edan los valores asociados a cada clave para obtener el recuento total de visitas por p\u00e1gina.</p> <pre><code>(/pagina1, 3)\n(/pagina2, 2)\n(/pagina3, 1)\n</code></pre>"},{"location":"UD03/2.fasesmapreduce/#ejemplo-3-ejemplo-calculo-del-saldo-total-por-cuenta","title":"Ejemplo 3: Ejemplo: C\u00e1lculo del Saldo Total por Cuenta","text":"<p>Supongamos que tienes registros de transacciones financieras con el siguiente formato:</p> <pre><code>Transacci\u00f3n 1: Cuenta_A, 100.00\nTransacci\u00f3n 2: Cuenta_B, 50.00\nTransacci\u00f3n 3: Cuenta_A, -20.00\nTransacci\u00f3n 4: Cuenta_B, 30.00\nTransacci\u00f3n 5: Cuenta_A, 50.00\n</code></pre> <p>Operaci\u00f3n de Mapeo: El objetivo es calcular el saldo total por cuenta. Cada registro de transacci\u00f3n se procesar\u00eda individualmente en un mapper. La operaci\u00f3n de mapeo generar\u00eda pares clave-valor, donde la clave es el nombre de la cuenta y el valor es el monto de la transacci\u00f3n (puede ser negativo para las transacciones de d\u00e9bito).</p> <pre><code>(Cuenta_A, 100.00)\n(Cuenta_B, 50.00)\n(Cuenta_A, -20.00)\n(Cuenta_B, 30.00)\n(Cuenta_A, 50.00)\n</code></pre> <p>Salida Intermedia (Despu\u00e9s de la fase Shuffle y Sort):</p> <pre><code>(Cuenta_A, [100.00, -20.00, 50.00])\n(Cuenta_B, [50.00, 30.00])\n</code></pre> <p>Fase Reduce (Operaci\u00f3n de Reducci\u00f3n): En la fase Reduce, se sumar\u00edan los valores asociados a cada clave para obtener el saldo total por cuenta.</p> <pre><code>(Cuenta_A, 130.00)\n(Cuenta_B, 80.00)\n</code></pre>"},{"location":"UD03/2.fasesmapreduce/#ejemplo-4-encontrar-la-temperatura-maxima-por-ubicacion","title":"Ejemplo 4: Encontrar la Temperatura M\u00e1xima por Ubicaci\u00f3n","text":"<p>Supongamos que tienes registros de temperatura con el siguiente formato:</p> <pre><code>Registro 1: Ubicacion_A, 25.5\u00b0C\nRegistro 2: Ubicacion_B, 28.3\u00b0C\nRegistro 3: Ubicacion_A, 27.8\u00b0C\nRegistro 4: Ubicacion_B, 30.2\u00b0C\nRegistro 5: Ubicacion_A, 26.0\u00b0C\n</code></pre> <p>Operaci\u00f3n de Mapeo: El objetivo es encontrar la temperatura m\u00e1xima para cada ubicaci\u00f3n. Cada registro de temperatura se procesar\u00eda individualmente en un mapper. La operaci\u00f3n de mapeo generar\u00eda pares clave-valor, donde la clave es la ubicaci\u00f3n y el valor es la temperatura.</p> <pre><code>(Ubicacion_A, 25.5\u00b0C)\n(Ubicacion_B, 28.3\u00b0C)\n(Ubicacion_A, 27.8\u00b0C)\n(Ubicacion_B, 30.2\u00b0C)\n(Ubicacion_A, 26.0\u00b0C)\n</code></pre> <p>Salida Intermedia (Despu\u00e9s de la fase Shuffle y Sort):</p> <pre><code>(Ubicacion_A, [25.5\u00b0C, 27.8\u00b0C, 26.0\u00b0C])\n(Ubicacion_B, [28.3\u00b0C, 30.2\u00b0C])\n</code></pre> <p>Fase Reduce (Operaci\u00f3n de Reducci\u00f3n): En la fase Reduce, se encontrar\u00eda la temperatura m\u00e1xima para cada ubicaci\u00f3n.</p> <pre><code>(Ubicacion_A, 27.8\u00b0C)\n(Ubicacion_B, 30.2\u00b0C)\n</code></pre>"},{"location":"UD03/2.fasesmapreduce/#ejemplo-5-calculo-del-tiempo-total-por-usuario-en-una-plataforma-en-linea","title":"Ejemplo 5: C\u00e1lculo del Tiempo Total por Usuario en una Plataforma en L\u00ednea","text":"\u2705 **Corregida**   <p>Datos de Entrada:</p> <pre><code>Registro 1: Usuario_A, Inicio_Sesi\u00f3n, 2023-01-01 08:00:00\nRegistro 2: Usuario_A, Acci\u00f3n, 2023-01-01 08:15:00\nRegistro 3: Usuario_B, Inicio_Sesi\u00f3n, 2023-01-01 09:30:00\nRegistro 4: Usuario_A, Fin_Sesi\u00f3n, 2023-01-01 09:35:00\nRegistro 5: Usuario_B, Acci\u00f3n, 2023-01-01 10:00:00\n</code></pre> <p>Operaci\u00f3n de Mapeo: El objetivo es calcular el tiempo total que cada usuario ha pasado en la plataforma. Cada registro de actividad se procesar\u00eda individualmente en un mapper. La operaci\u00f3n de mapeo generar\u00eda pares clave-valor, donde la clave es el nombre del usuario y el valor es la duraci\u00f3n de la actividad (por ejemplo, tiempo entre el inicio de sesi\u00f3n y la acci\u00f3n o entre la acci\u00f3n y el cierre de sesi\u00f3n).</p> <pre><code>(Usuario_A, 15 minutos)\n(Usuario_B, 30 minutos)\n(Usuario_A, 80 minutos)\n</code></pre> <p>Salida Intermedia (Despu\u00e9s de la fase Shuffle y Sort):</p> <pre><code>(Usuario_A, [15 minutos, 80 minutos])\n(Usuario_B, [30 minutos])\n</code></pre> <p>Fase Reduce (Operaci\u00f3n de Reducci\u00f3n): En la fase Reduce, se sumar\u00edan los tiempos asociados a cada usuario para obtener el tiempo total que cada usuario ha pasado en la plataforma.</p> <pre><code>(Usuario_A, 95 minutos)\n(Usuario_B, 30 minutos)\n</code></pre>"},{"location":"UD03/2.fasesmapreduce/#ejemplo-6-identificacion-de-usuarios-influyentes-en-una-red-social","title":"Ejemplo 6: Identificaci\u00f3n de Usuarios Influyentes en una Red Social","text":"<p>Datos de Entrada:</p> <pre><code>Evento 1: Usuario_A public\u00f3 un mensaje.\nEvento 2: Usuario_B coment\u00f3 en el mensaje de Usuario_A.\nEvento 3: Usuario_C dio \"Me gusta\" al mensaje de Usuario_A.\nEvento 4: Usuario_A comparti\u00f3 el mensaje de Usuario_B.\nEvento 5: Usuario_C coment\u00f3 en el mensaje compartido por Usuario_A.\nEvento 6: Usuario_B dio \"Me gusta\" al comentario de Usuario_C.\n</code></pre> <p>Operaci\u00f3n de Mapeo: El objetivo es identificar patrones de interacci\u00f3n entre usuarios. Cada evento se procesar\u00eda individualmente en un mapper. La operaci\u00f3n de mapeo generar\u00eda pares clave-valor, donde la clave es el nombre del usuario y el valor es la acci\u00f3n realizada.</p> <p>Salida mapeo</p> <pre><code>(Usuario_A, [public\u00f3, comparti\u00f3])\n(Usuario_B, [coment\u00f3, dio_me_gusta])\n(Usuario_C, [dio_me_gusta, coment\u00f3])\n(Usuario_A, [comparti\u00f3])\n(Usuario_C, [coment\u00f3, dio_me_gusta])\n(Usuario_B, [dio_me_gusta])\n</code></pre> <p>Salida Intermedia (Despu\u00e9s de la fase Shuffle y Sort):</p> <pre><code>(Usuario_A, [public\u00f3, comparti\u00f3, comparti\u00f3])\n(Usuario_B, [coment\u00f3, dio_me_gusta, dio_me_gusta])\n(Usuario_C, [dio_me_gusta, coment\u00f3, coment\u00f3])\n</code></pre> <p>Fase Reduce (Operaci\u00f3n de Reducci\u00f3n): En la fase Reduce, se analizar\u00edan los patrones de interacci\u00f3n para determinar la influencia de cada usuario. Por ejemplo, podr\u00edas asignar puntajes a los usuarios seg\u00fan el n\u00famero de publicaciones, comentarios y \"Me gusta\" recibidos, y luego sumar esos puntajes para cada usuario.</p> <pre><code>(Usuario_A, Puntaje: 3)\n(Usuario_B, Puntaje: 3)\n(Usuario_C, Puntaje: 3)\n</code></pre>"},{"location":"UD03/2.fasesmapreduce/#ejemplo-7-deteccion-de-anomalias-en-datos-de-sensores-industriales","title":"Ejemplo 7: Detecci\u00f3n de Anomal\u00edas en Datos de Sensores Industriales","text":"<p>Datos de Entrada:</p> <pre><code>Evento 1: Sensor_A, Temperatura, 30\u00b0C\nEvento 2: Sensor_B, Presi\u00f3n, 100 psi\nEvento 3: Sensor_A, Temperatura, 32\u00b0C\nEvento 4: Sensor_B, Presi\u00f3n, 105 psi\nEvento 5: Sensor_A, Temperatura, 28\u00b0C\nEvento 6: Sensor_B, Presi\u00f3n, 110 psi\n</code></pre> <p>Operaci\u00f3n de Mapeo: El objetivo es identificar patrones de comportamiento an\u00f3malo en los datos del sensor. Cada evento se procesar\u00eda individualmente en un mapper. La operaci\u00f3n de mapeo generar\u00eda pares clave-valor, donde la clave es el tipo de sensor y el valor es el valor del evento.</p> <p>Salida mapeo:</p> <pre><code>(Sensor_A, [30\u00b0C, 32\u00b0C, 28\u00b0C])\n(Sensor_B, [100 psi, 105 psi, 110 psi])\n</code></pre> <p>Salida Intermedia (Despu\u00e9s de la fase Shuffle y Sort):</p> <pre><code>(Sensor_A, [30\u00b0C, 32\u00b0C, 28\u00b0C])\n(Sensor_B, [100 psi, 105 psi, 110 psi])\n</code></pre> <p>Fase Reduce (Operaci\u00f3n de Reducci\u00f3n): En la fase Reduce, se analizar\u00edan los patrones de comportamiento para identificar anomal\u00edas. Por ejemplo, podr\u00edas calcular estad\u00edsticas como el promedio y la desviaci\u00f3n est\u00e1ndar para cada tipo de sensor y luego identificar eventos que se desv\u00eden significativamente de estas estad\u00edsticas como posibles anomal\u00edas.</p> <pre><code>(Sensor_A, Promedio: 30\u00b0C, Desviaci\u00f3n Est\u00e1ndar: 2\u00b0C, Anomal\u00edas: [32\u00b0C, 28\u00b0C])\n(Sensor_B, Promedio: 105 psi, Desviaci\u00f3n Est\u00e1ndar: 5 psi, Anomal\u00edas: [110 psi])\n</code></pre>"},{"location":"UD03/3.contarpalabras/","title":"Ejemplo. Contar palabras","text":""},{"location":"UD03/3.contarpalabras/#practica-hadoop-wordcount","title":"Pr\u00e1ctica Hadoop Wordcount","text":"<pre><code># RECORDATORIO: En la UD02, ejecutamos el comando \"grep\" de hadoop\n$ hadoop jar hadoop-mapreduce-examples-3.3.6.jar grep /tmp/entrada /tmp/salida/ 'kms'\n</code></pre>"},{"location":"UD03/3.contarpalabras/#ejemplo-mapreduce-wordcount","title":"Ejemplo MapReduce: wordcount","text":"<ol> <li>Vamos a ejecutar el proceso \u201cwordcount\u201d que ya hab\u00edamos utilizado anteriormente.</li> <li>Descargamos el libro del quijote en formato texto: Don Quijote by Miguel de Cervantes Saavedra - Free Ebook (gutenberg.org) y lo guardamos en el directorio <code>/tmp</code> https://www.gutenberg.org/cache/epub/2000/pg2000.txt</li> <li>Trasladamos el libro a nuestro sistema HDFS, dentro de la carpeta <code>libros</code></li> <li>Queremos obtener una salida con el n\u00famero de palabras que tiene y cuantas veces aparece la palabra. Vamos a situarnos en el directorio de mapreduce (<code>/opt/hadoop/share/hadoop/mapreduce/</code>) y ejecutar dentro del Jar \u201cMapReduce examples\u201d el programa \u201cwordcount\u201d. El programa wordcount busca en un directorio los ficheros planos y deja el resultado en otro directorio.</li> </ol> Soluci\u00f3n <p>hadoop jar hadoop-mapreduce-examples-3.3.6.jar wordcount /libros /salida_libros </p> <p>Podemos ver c\u00f3mo se va ejecutando la tarea:  Si hacemos click en \u201cId\u201d en la ejecuci\u00f3n que queremos, veremos el detalle de esta. En los logs podemos ver el detalle de la ejecuci\u00f3n y revisar si hay alg\u00fan error.</p> <p></p> <p>Si vamos al Attempt Id podremos ver el n\u00famero de peticiones que se han hecho, numero de containers lanzado, etc.</p> <p>El enlace al hist\u00f3rico no funcionara porque lo tenemos desactivado.</p> <ol> <li>\u00bfQu\u00e9 diferencia existe en el comando que acabamos de ejecutar y el que utilizamos en la UD02?</li> <li>Vamos a revisar la salida de la ejecuci\u00f3n descargando el fichero del resultado a nuestro sistema de ficheros (fuera del HDFS).</li> </ol>"},{"location":"UD03/3.contarpalabras/#errores","title":"Errores","text":""},{"location":"UD03/3.contarpalabras/#error-1","title":"Error 1","text":"<p>Fichero mapred-site.xml, a\u00f1adir esta propiedad:</p> <pre><code>  &lt;property&gt;\n        &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;\n        &lt;value&gt;HADOOP_MAPRED_HOME=/opt/hadoop&lt;/value&gt;\n    &lt;/property&gt;\n</code></pre>"},{"location":"UD03/3.contarpalabras/#error-2","title":"Error 2","text":"<p>Si ocurre un error de memoria similar a:</p> <p>Current usage: 449 MB of 1 GB physical memory used; 2.6 GB of 2.1 GB virtual memory used. Killing container</p> <p>Modificaremos yarn-site.xml para \u201cenga\u00f1ar\u201d a hadoop.</p> <pre><code>&lt;property&gt;\n    &lt;name&gt;\n        yarn.nodemanager.vmem-check-enabled\n    &lt;/name&gt;\n    &lt;value&gt;\n        False\n    &lt;/value&gt;\n&lt;/property&gt;\n</code></pre>"},{"location":"UD03/3.contarpalabras/#error3","title":"Error3","text":"<pre><code># file: hadoop-env.sh\n\nexport JAVA_HOME=/usr/lib/jvm/jdk-11-oracle-x64\nexport HADOOP_HOME=/opt/hadoop\nexport HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}\nexport HADOOP_SSH_OPTS=\"-p 22\"\nexport HADOOP_CLASSPATH+=\" $HADOOP_CONF_DIR/lib/*.jar\"\n</code></pre>"},{"location":"UD03/3.contarpalabras/#error-4","title":"Error 4","text":"<pre><code># file: yarn-site.xml\n\n&lt;property&gt;\n    &lt;name&gt;yarn.application.classpath&lt;/name&gt;\n        &lt;value&gt;\n            /opt/hadoop/hadoop/etc/hadoop,\n            /opt/hadoop/share/hadoop/common/*,\n            /opt/hadoop/share/hadoop/common/lib/*,\n            /opt/hadoop/share/hadoop/hdfs/*,\n            /opt/hadoop/share/hadoop/hdfs/lib/*,\n            /opt/hadoop/share/hadoop/mapreduce/*,\n            /opt/hadoop/share/hadoop/mapreduce/lib/*,\n            /opt/hadoop/share/hadoop/yarn/*,\n            /opt/hadoop/share/hadoop/yarn/lib/*\n        &lt;/value&gt;\n    &lt;/property&gt;\n</code></pre>"},{"location":"UD03/3.contarpalabras/#2-practica-java-wordcount","title":"2. Pr\u00e1ctica Java wordcount","text":"<p>ContarPalabras.java</p> <p>ContarPalabras.java.tgz</p> <ol> <li>En la m\u00e1quina virtual, creamos una carpeta en ~/practicas</li> <li>Copiamos el archivo ContarPalabras.java y lo revisamos para ver su funcionamiento</li> <li>Lo compilamos con:</li> </ol> <pre><code>hadoop com.sun.tools.javac.Main ContarPalabras.java\n\njar cf ContarPalabras.jar Contar*.class\n\n# crear\u00e1 un archivo llamado ContarPalabras.jar\n</code></pre> <ol> <li>Ya lo tenemos compilado y preparado para ejecutar con MapReduce. Aseguremos que tenemos la web abierta para visualizar la ejecuci\u00f3n http://nodo1:8088/cluster/apps</li> <li>Vamos a activar el proceso para generar el hist\u00f3rico de ejecuciones con todo el detalle.</li> </ol> <pre><code>mapred --daemon start historyserver\n</code></pre> <ol> <li>Ejecutamos ContarPalabras.java y podemos ir revisando tanto la Shell como la web viendo c\u00f3mo avanza la ejecuci\u00f3n. </li> </ol> <pre><code>$ hadoop jar ContarPalabras.jar ContarPalabras /libros/quijote.txt /salida_libros2\n</code></pre> <p>\ud83e\uddd1\ud83c\udffc\u200d\ud83c\udf93 Ejercicio aules</p> <p>Cuenta las palabras del fichero <code>access_log.gz</code> que utilizamos en la unidad anterior. Entregar: * PDF con comandos ejecutados. * Capturas donde se vea el cluster funcionando (diferentes webviews), contenedores creados, procesos mapreduce... * Descarga el fichero de salida generado y pega tambi\u00e9n el el documento las 30 primeras l\u00edneas</p>"},{"location":"UD03/4.ejercicioVentas/","title":"Hadoop Ventas","text":"<p>Para comprender mejor c\u00f3mo funciona un trabajo t\u00edpico de Hadoop basado en MapReduce, a continuaci\u00f3n profundizaremos en la programaci\u00f3n y ejecuci\u00f3n de un ejemplo utilizando el lenguaje Python.</p> <p>Hadoop funciona de forma nativa con Java pero cuenta con una herramienta que permite la ejecuci\u00f3n de programas/scripts en diferentes lenguajes como python. Esta es la herramienta \"Streaming\": https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html Este ejemplo trabaja con un conjunto de datos relacionados con las ventas de una empresa con m\u00faltiples tiendas: <code>purchases.txt</code></p> <p>Lo puedes descargar en: https://github.com/josepgarcia/datos</p> <p>A continuaci\u00f3n puede ver un extracto del archivo, que contiene seis campos sobre compras: fecha, hora, ubicaci\u00f3n de la tienda, categor\u00eda de producto, compra total y m\u00e9todo de pago.</p> <pre><code>2012-01-01  09:00   San Jose    Men's Clothing  214.05  Amex\n2012-01-01  09:00   Fort Worth  Women's Clothing    153.57  Visa\n2012-01-01  09:00   San Diego   Music   66.08   Cash\n2012-01-01  09:00   Pittsburgh  Pet Supplies    493.51  Discover\n2012-01-01  09:00   Omaha   Children's Clothing 235.63  MasterCard\n2012-01-01  09:00   Stockton    Men's Clothing  247.18  MasterCard\n2012-01-01  09:00   Austin  Cameras 379.6   Visa\n2012-01-01  09:00   New York    Consumer Electronics    296.8   Cash\n2012-01-01  09:00   Corpus Christi  Toys    25.38   Discover\n2012-01-01  09:00   Fort Worth  Toys    213.88  Visa\n</code></pre>"},{"location":"UD03/4.ejercicioVentas/#ejemplo-1-fase-1","title":"Ejemplo 1: Fase 1","text":"<p>Queremos saber el importe total de ventas de cada tienda (la tienda corresponde a la tercera columna del archivo, hay una tienda por ciudad).</p> <p>El paradigma MapReduce tiene 3 pasos:  mapper() shuffle - sort reducer()</p> <p>mapper El mapper estar\u00e1 a cargo de construir el par clave:valor de cada l\u00ednea. Piensa que cada nodo del cluster ejecutar\u00e1 el mapper en las l\u00edneas del archivo que le corresponden. En cualquier caso, es un trabajo que se ejecuta por l\u00edneas, lo que permite una f\u00e1cil paralelizaci\u00f3n.</p> <p>Nuestro mapper debe devolver como clave el nombre de la tienda y como valor el importe de las ventas. Por ejemplo: <pre><code>San Jos\u00e9\u00a0\u00a0\u00a0 214,05\nFort Worth\u00a0\u00a0\u00a0 153,57\n...\nFort Worth\u00a0\u00a0\u00a0 213,88\n</code></pre></p> <p>shuffle - sort Sustituiremos esta fase por el comando sort, por lo que simplemente haremos un:</p> <pre><code>cat salida_mapper.csv | sort &gt; salida_sort.csv\n</code></pre> <p>reducer El reducer se encargar\u00e1 de sumar los valores correspondientes a cada clave. La salida ser\u00e1, por ejemplo: <pre><code>San Jos\u00e9\u00a0\u00a0\u00a0 214.05\nFort Worth\u00a0\u00a0\u00a0 367,45\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0# (135,57+213,88)\n</code></pre></p> <p>Ejercicio</p> <p>Programa el mapper y reducer con python, ten en cuenta que cuando los datos lleguen al reducer estar\u00e1n ordenados (se realizar\u00e1 un sort antes).</p> <p>Ejemplo de funcionamiento</p> <pre><code>python mapper.py \n\nSan Jose    214.05\nFort Worth  153.57\nSan Diego   66.08\nPittsburgh  493.51\nOmaha   235.63\nStockton    247.18\nAustin  379.6\nNew York    296.8\n\n# Lo guardamos en un archivo intermedio\npython mapper.py &gt; salida_mapper.txt\n\n# Hacemos el shuffle + sort \"manualmente\"\ncat salida_mapper.txt| sort &gt; salida_sort.txt\n\n# El archivo salida_sort debe contener los datos ordenados\n#Albuquerque    440.7\n#Anchorage  22.36\n#Anchorage  298.86\n#Anchorage  368.42\n#Anchorage  390.2\n#Anchorage  6.38\n#Atlanta    254.62\n\n# Hacemos el reducer\npython3 reducer_simple.py\n\nAlbuquerque      440.7 # suma de todas las l\u00edneas con clave Albuquerque\nAnchorage    1086.2200000000003\nAtlanta      254.62\nAurora   117.81\nAustin   1304.7800000000002\nBoise    481.09000000000003\nBoston   418.94\n</code></pre>"},{"location":"UD03/4.ejercicioVentas/#ejemplo-1-fase-2","title":"Ejemplo 1: Fase 2","text":"<p>Uso de tuber\u00edas.</p> <p>\u00bfC\u00f3mo funcionan las tuber\u00edas en bash?</p> <p>Para que podamos ejecutar el programa a trav\u00e9s de hadoop, ha de hacer uso de tuber\u00edas no de ficheros.</p> <p>Modifica el c\u00f3digo para que en vez de leer de un fichero lea del stream de entrada (sys.stdin)</p> <p>Puedes comprobar el funcionamiento ejecutando: <pre><code>cat purchases.txt | python mapper.py | sort | python reducer.py &gt; salida.txt\n</code></pre></p>"},{"location":"UD03/4.ejercicioVentas/#ejemplo-1-fase-3","title":"Ejemplo 1: Fase 3","text":"<p>Mejora el c\u00f3digo:</p> <p>[ ] Todas las l\u00edneas del archivo de entrada deben tener el mismo n\u00famero de campos, si no es as\u00ed en alguna l\u00ednea hay que descartarla.  [ ] Aseg\u00farate que el valor num\u00e9rico sea float.  [ ] Como se ha mencionado anteriormente, hay que tener en cuenta que al reducer le llegan los datos ordenados. [ ] \u00bfOtras mejoras?</p>"},{"location":"UD03/4.ejercicioVentas/#ejemplo-1-fase-4","title":"Ejemplo 1: Fase 4","text":"<ol> <li>Subir a hdfs el dataset, en mi caso <code>/compras/test.txt</code></li> <li>Subir al contenedor los archivos .py <pre><code>mapred streaming -files mapper_tuberias.py,reducer_tuberias.py -input /compras/test.txt -output /comprasxtienda -mapper ./mapper_tuberias.py -reducer ./reducer_tuberias.py\n\n## Si el comando anterior falla:\n\nmapred streaming -files mapper_tuberias.py,reducer_tuberias.py -input /compras/test.txt -output /comprasxtienda -mapper \"python3 ./mapper_tuberias.py\" -reducer \"python3 ./reducer_tuberias.py\"\n</code></pre></li> <li>Una vez ejecutado el comando, podemos ver la salida en /comprasxtienda/part-00000</li> <li>Si la ejecuci\u00f3n es correcta existir\u00e1 un archivo SUCCESS <pre><code>dfs -ls /comprasxtienda\n-rw-r--r--   1 hadoop supergroup          0 2024-12-05 11:11 /comprasxtienda7/_SUCCESS\n-rw-r--r--   1 hadoop supergroup       1201 2024-12-05 11:11 /comprasxtienda7/part-00000\n</code></pre></li> </ol> <p>CORRECCI\u00d3N DE ERRORES: https://stackoverflow.com/questions/50927577/could-not-find-or-load-main-class-org-apache-hadoop-mapreduce-v2-app-mrappmaster</p>"},{"location":"UD03/4.ejercicioVentas/#ejercicios","title":"Ejercicios","text":""},{"location":"UD03/4.ejercicioVentas/#ejercicio-1","title":"Ejercicio 1","text":"<p>Redefine el mapper para que mapreduce devuelva como salida las ventas totales por categor\u00eda.</p> <pre><code>Ejemplo de resultado (datos no reales):\n\nPet Supplies   1123.4  \nMusic          22344.56\nClothing       3356.45\n</code></pre>"},{"location":"UD03/4.ejercicioVentas/#ejercicio-2","title":"Ejercicio 2","text":"<p>Redefine el mapper y reducer para que se obtenga la venta m\u00e1s alta para cada tipo de pago de las registradas en todo el archivo.</p> <p>No hay que devolver las ventas totales por tipo de pago</p> <pre><code>Ejemplo de resultado (datos no reales):\n\nVisa           133.5   -&gt; la venta m\u00e1s alta de todas las hechas con visa\nCash           223.56  -&gt; la venta m\u00e1s alta de todas las hechas con cash\nMastercard     1356.45 -&gt; la venta m\u00e1s alta de todas las hechas con Mastercard\n</code></pre>"},{"location":"UD03/4.ejercicioVentas/#ejercicio-3","title":"Ejercicio 3","text":"<p>Realiza las modificaciones necesarias para que mapreduce nos devuelve la venta m\u00e1s alta de todas las realizadas</p>"},{"location":"UD03/4.ejercicioVentas/#ejercicio-4","title":"Ejercicio 4","text":"<p>Redefine los scripts para que se obtengan la suma de todas las ventass.</p>"},{"location":"UD03/4.ejercicioVentasSOL/","title":"Hadoop Ventas","text":""},{"location":"UD03/4.ejercicioVentasSOL/#fase-1","title":"Fase 1","text":"<p>Para comprender mejor c\u00f3mo funciona un trabajo t\u00edpico de Hadoop basado en MapReduce, a continuaci\u00f3n profundizaremos en la programaci\u00f3n y ejecuci\u00f3n de un ejemplo utilizando el lenguaje Python.</p> <p>Hadoop funciona de forma nativa con Java pero cuenta con una herramienta que permite la ejecuci\u00f3n de programas/scripts en diferentes lenguajes como python. Esta es la herramienta \"Streaming\": https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html</p> <p>Este ejemplo trabaja con un conjunto de datos relacionados con las ventas de una empresa con m\u00faltiples tiendas: <code>purchases.txt</code></p> <p>https://drive.google.com/file/d/1m9B0fDUrYWnspWJO1pslIwR_cUcPccZD/view?usp=drive_link</p> <p>A continuaci\u00f3n puede ver un extracto del archivo, que contiene seis campos sobre compras: fecha, hora, ubicaci\u00f3n de la tienda, categor\u00eda de producto, compra total y m\u00e9todo de pago.</p> <pre><code>2012-01-01  09:00   San Jose    Men's Clothing  214.05  Amex\n2012-01-01  09:00   Fort Worth  Women's Clothing    153.57  Visa\n2012-01-01  09:00   San Diego   Music   66.08   Cash\n2012-01-01  09:00   Pittsburgh  Pet Supplies    493.51  Discover\n2012-01-01  09:00   Omaha   Children's Clothing 235.63  MasterCard\n2012-01-01  09:00   Stockton    Men's Clothing  247.18  MasterCard\n2012-01-01  09:00   Austin  Cameras 379.6   Visa\n2012-01-01  09:00   New York    Consumer Electronics    296.8   Cash\n2012-01-01  09:00   Corpus Christi  Toys    25.38   Discover\n2012-01-01  09:00   Fort Worth  Toys    213.88  Visa\n</code></pre> <p>El programa resultante se puede ejecutar, dependiendo de tu sistema de las siguientes formas:</p> <pre><code>1) python3 mapper.py\n2) ./mapper.py #(hay que darle permisos)\n</code></pre>"},{"location":"UD03/4.ejercicioVentasSOL/#ejemplo-1","title":"Ejemplo 1","text":"<p>Es interesante conocer el importe total de ventas de cada tienda.</p> <p>El paradigma MapReduce se compone de dos funciones: mapper() y reducer()</p> <p>mapper El mapper estar\u00e1 a cargo de construir el par clave:valor de cada l\u00ednea. Piensa que cada nodo del cluster ejecutar\u00e1 el mapper en las l\u00edneas del archivo que le corresponden. En cualquier caso, es un trabajo que se ejecuta por l\u00edneas, lo que permite una f\u00e1cil paralelizaci\u00f3n.</p> <p>Por ejemplo: <pre><code>San Jos\u00e9\u00a0\u00a0\u00a0 214,05\nFort Worth\u00a0\u00a0\u00a0 153,57\n...\nFort Worth\u00a0\u00a0\u00a0 213,88\n</code></pre></p> <p>reducer</p> <p>El reducer se encargar\u00e1 de sumar los valores correspondientes a unas mismas claves.</p> <p>Por ejemplo: <pre><code>San Jos\u00e9\u00a0\u00a0\u00a0 214.05\nFort Worth\u00a0\u00a0\u00a0 367,45\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0# (135,57+213,88)\n</code></pre></p> <p>Entre el mapper y el reducer se ejecutan las tareas shuffle y sort, que agrupan los pares clave:valor seg\u00fan las claves, permitiendo la posterior operaci\u00f3n de reducci\u00f3n.</p> <p>Cuando ejecutemos los programas desde la consola, utilizaremos el comando \u201csort\u201d para sustituir a shuffle y sort.</p> <p></p> <p>Crea el c\u00f3digo necesario para construir el mapper y el reducer</p> <ul> <li>Sol <pre><code>https://github.com/bigdatawirtz/bigdata-aplicado-2022/blob/main/mapreduce/mapper.py\n\n    **mapper.py**\n    #!/usr/bin/python3\n\n    # Format of each line is:\n    # date\\ttime\\tstore name\\titem description\\tcost\\tmethod of payment\n    #\n    # We want elements 2 (store name) and 4 (cost)\n    # We need to write them out to standard output, separated by a tab\n\n    import sys\n\n    for line in sys.stdin:\n        data = line.strip().split(\"\\t\")\n        date, time, store, item, cost, payment = data\n        print(store+\"\\t\"+cost)\n\n    **reducer.py**\n    #!/usr/bin/python3\n\n    import sys\n\n    salesTotal = 0\n    oldKey = None\n\n    # Loop around the data\n    # It will be in the format key\\tval\n    # Where key is the store name, val is the sale amount\n    #\n    # All the sales for a particular store will be presented,\n    # then the key will change and we'll be dealing with the next store\n\n    for line in sys.stdin:\n        data_mapped = line.strip().split(\"\\t\")\n        if len(data_mapped) != 2:\n            # Something has gone wrong. Skip this line.\n            continue\n\n        thisKey, thisSale = data_mapped\n\n        # Escribe un par key:value ante un cambio na key\n        # Reinicia o total\n        if oldKey and oldKey != thisKey:\n            print(oldKey+\"\\t\"+str(salesTotal))\n            oldKey = thisKey;\n            salesTotal = 0\n\n        oldKey = thisKey\n        salesTotal += float(thisSale)\n\n    # Escribe o ultimo par, unha vez rematado o bucle\n    if oldKey != None:\n        print(oldKey+\"\\t\"+str(salesTotal))\n    ```\n\n\n### Primera aproximaci\u00f3n\n\nCrea los programas mapper y reducer necesarios.\n\n**PASO1: mapper**\n\nMapper leer\u00e1 de un fichero de texto que contendr\u00e1 50 o 100 l\u00edneas del fichero original (as\u00ed ser\u00e1n m\u00e1s r\u00e1pidas las pruebas).\n\nLo ejecutaremos de la siguiente manera:\n\n```bash\npython mapper.py\n</code></pre></li> </ul> <p>Y producir\u00e1 una salida tipo (campos separados por tabulador):</p> <pre><code>San Jos\u00e9\u00a0\u00a0\u00a0 214,05\nFort Worth\u00a0\u00a0\u00a0 153,57\n...\nFort Worth\u00a0\u00a0\u00a0 213,88\n</code></pre> <p>Una vez produzca la salida deseada utilizaremos el operador de redirecci\u00f3n \u201c&gt;\u201d para guardar la salida en un archivo de texto:</p> <pre><code>python mapper.py &gt; salida_mapper.csv\n</code></pre> <ul> <li>[ ]  Aseg\u00farate que la temperatura sea un float.</li> </ul> <p>PASO2: shuffle y sort</p> <p>Sustituiremos esta fase por el comando sort, por lo que simplemente haremos un:</p> <pre><code>cat salida_mapper.csv | sort &gt; salida_sort.csv\n</code></pre> <p>PASO3: reducer</p> <p>Leer\u00e1 del fichero salida_sort.csv y procesar\u00e1 los datos.</p> <p>Hay que tener en cuenta que al llegar a esta fase ya tenemos los datos ordenados, por lo que ser\u00e1 m\u00e1s sencillo sumar los diferentes registros:</p> <pre><code>**Ejemplo de fichero**\n\n...\nFort Worth\u00a0\u00a0\u00a0 10\nFort Worth\u00a0\u00a0\u00a0 20\nFort Worth\u00a0\u00a0\u00a0 15\n....\nSan Jos\u00e9\u00a0\u00a0\u00a0 133\nSan Jos\u00e9\u00a0\u00a0\u00a0 7\n....\n</code></pre> <p>Esta segunda ejecuci\u00f3n devolver\u00e1 el sumatorio de los importes de cada ciudad (campos separados por tabulador):</p> <pre><code>**Por ejemplo:\n...**\nFort Worth\u00a0\u00a0\u00a0 45\n...\n****San Jos\u00e9\u00a0\u00a0\u00a0 140\n...\n</code></pre>"},{"location":"UD03/4.ejercicioVentasSOL/#segunda-aproximacion","title":"Segunda aproximaci\u00f3n","text":"<p>Uso de tuber\u00edas</p> <p>El programa debe ejecutarse a trav\u00e9s de tuber\u00edas, no de ficheros:</p> <pre><code>cat purchases.txt | python mapper.py | sort | python reducer.py &gt; salida.txt\n</code></pre> <p>Por lo que has de modificar el c\u00f3digo, para que en vez de leer de un fichero, lea del stream de entrada (sys.stdin).</p>"},{"location":"UD03/4.ejercicioVentasSOL/#fase-2","title":"Fase 2","text":"<p>Una vez verificado el c\u00f3digo (funciona a trav\u00e9s de tuber\u00edas) vamos a mandarlo a una ejecuci\u00f3n distribuida con hadoop.</p> <ol> <li>Sube los ficheros a la m\u00e1quina donde tengas instalado hadoop (mapper, reducer y purchases.txt)</li> <li>Crea en HDFS una carpeta llamada purchases</li> <li>Sube all\u00ed el fichero purchases.txt</li> <li>Los ficheros .py est\u00e1n en tu m\u00e1quina virtual y el fichero purchases.txt est\u00e1 en HDFS, as\u00ed hadoop ya puede dividirlo y crear los diferentes jobs que considere.</li> <li>Vamos a utilizar mapreduce pero a trav\u00e9s de nuestro mapper y reducer. Para poder ejecutar nuestros scripts para crear procesos \u201cmapreduce\u201d utilizamos \u201chadoop streaming\u201d https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html</li> </ol> <p>Lanzamos el siguiente comando: <pre><code>mapred streaming -files mapper.py,reducer.py \n        -input /PURCHASES/purchases.txt -output COMPRASXTIENDA \n        -mapper \"python3 mapper.py\" -reducer \"python3 reducer.py\"\n\n------------------------------------------------------------------\n## Sintaxis del comando:\nmapred streaming \n    -files mapper.py,reducer.py\n\u00a0\u00a0\u00a0 -input myInputDirs \\\n\u00a0\u00a0\u00a0 -output myOutputDir \\\n\u00a0\u00a0\u00a0 -mapper script_mapper\n\u00a0\u00a0\u00a0 -reducer script_reducer\n</code></pre></p> <ul> <li> <p>\u2139\u00a0Otra forma de ejecutar el c\u00f3digo:</p> <pre><code>$ mapred streaming -files mapper2.py,reducer.py -input /quijote.txt \n-output /quijote_salida1 -mapper ./mapper2.py -reducer ./reducer.py\n\n**$ hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -file mapper2.py \n-mapper mapper2.py -file reducer.py -reducer reducer.py -input /quijote.txt \n-output /quijote_salida2**\n</code></pre> </li> <li> <p>\u00bfD\u00f3nde se encuentra el resultado?</p> </li> </ul>"},{"location":"UD03/4.ejercicioVentasSOL/#ejercicios","title":"Ejercicios","text":"<p>Como puedes ver en el c\u00f3digo, no es un algoritmo complejo y tambi\u00e9n se puede ejecutar desde la consola ejecutando los scripts manualmente.</p> <p>La ventaja que ofrece Hadoop es que se pueden utilizar los mismos algoritmos en archivos mucho m\u00e1s grandes, distribuidos en un sistema de archivos distribuido, y se pueden utilizar varias computadoras para procesar los datos.</p>"},{"location":"UD03/4.ejercicioVentasSOL/#ejercicio-11-mejora-el-codigo","title":"Ejercicio 1.1. Mejora el c\u00f3digo.","text":"<p>La funci\u00f3n del mapper se ejecutar\u00e1 en todas y cada una de las l\u00edneas del archivo de datos. El c\u00f3digo del mapper producir\u00e1 un error si encuentra alguna l\u00ednea que no coincida con el n\u00famero exacto de valores separados por tabulaciones.</p> <ul> <li>[ ]  Mejora el c\u00f3digo para que cualquier l\u00ednea con un formato inadecuado sea descartada y contin\u00fae trabajando con la siguiente l\u00ednea.</li> <li>[ ]  Aseg\u00farate que el valor num\u00e9rico sea un float</li> <li> <p>Sol</p> <pre><code>for line in sys.stdin:\n    data = line.strip().split(\"\\t\")\n    **if len(data) == 6:**\n        date, time, store, item, cost, payment = data\n        print(f'{store}\\t{cost}')\n</code></pre> </li> </ul>"},{"location":"UD03/4.ejercicioVentasSOL/#ejercicio-12","title":"Ejercicio 1.2","text":"<p>Redefine el mapper para que mapreduce devuelva como salida las ventas totales por categor\u00eda.</p> <pre><code>**Ejemplo de resultado (datos no reales):**\n\nPet Supplies   1123.4  \nMusic          22344.56\nClothing       3356.45\n</code></pre> <ul> <li> <p>Sol</p> <pre><code>for line in sys.stdin:\n    data = line.strip().split(\"\\t\")\n    if len(data) == 6:\n        date, time, store, item, cost, payment = data\n        **print(f'{item}\\t{cost}')**\n</code></pre> </li> </ul>"},{"location":"UD03/4.ejercicioVentasSOL/#ejercicio-13","title":"Ejercicio 1.3","text":"<p>Redefine el mapper y reducer para que se obtenga la venta m\u00e1s alta para cada tipo de pago de las registradas en todo el archivo.</p> <p>** No hay que devolver las ventas totales por tipo de pago **</p> <pre><code>**Ejemplo de resultado (datos no reales):**\n\nVisa           133.5   -&gt; la venta m\u00e1s alta de todas las hechas con visa\nCash           223.56  -&gt; la venta m\u00e1s alta de todas las hechas con cash\nMastercard     1356.45 -&gt; la venta m\u00e1s alta de todas las hechas con Mastercard\n</code></pre> <ul> <li> <p>Sol</p> <pre><code># El mapper devuelve el par paymet:cost\nfor line in sys.stdin:\n    data = line.strip().split(\"\\t\")\n    if len(data) == 6:\n****        date, time, store, item, cost, payment = data\n        **print(f'{payment}\\t{cost}')**\n\n# El reducer no hace la suma, si no que va comparando si\n# la venta fu\u00e9 mayor que la anterior\noldKey = thisKey\n    **if thisSale &gt;= salesMax:\n        salesMax = float(thisSale)**\n</code></pre> </li> </ul>"},{"location":"UD03/4.ejercicioVentasSOL/#ejercicio-14","title":"Ejercicio 1.4","text":"<p>Realiza las modificaciones necesarias para que el proceso mapreduce nos devuelva la venta m\u00e1s alta de todas las realizadas.</p> <ul> <li> <p>Sol</p> <pre><code># Una posible soluci\u00f3n ser\u00eda modificar solo el asignador \n#y crear una \u00fanica categor\u00eda \"todos\" en lugar de utilizar \n#los diferentes valores para el pago. De esta forma, \n#el reductor pensar\u00e1 que todas las ventas son con el mismo \n#tipo de pago, es decir, no se preocupar\u00e1 por qu\u00e9 tipo \n#de pago se utiliz\u00f3.\nfor line in sys.stdin:\n    data = line.strip().split(\"\\t\")\n    if len(data) == 6:\n        date, time, store, item, cost, payment = data\n        **print(f'all\\t{cost}')**\n</code></pre> </li> </ul>"},{"location":"UD03/4.ejercicioVentasSOL/#ejercicio-15","title":"Ejercicio 1.5","text":"<p>Redefine los scripts par que se obtengan las ventas totales.</p> <ul> <li> <p>Sol</p> <pre><code>#Como en el caso anterior, podemos utilizar una \u00fanica \n# categor\u00eda para todos \"todos\" en el mapeador.\nfor line in sys.stdin:\n    data = line.strip().split(\"\\t\")\n    if len(data) == 6:\n        date, time, store, item, cost, payment = data\n        **print(f'all\\t{cost}')**\n</code></pre> </li> </ul>"},{"location":"UD03/5.ejercicioTemperatura/","title":"Ejercicio Temperatura","text":"<p>Entregables</p> <p>Entregables en AULES</p> <p>Descarga el dataset <code>city_temperature.csv.zip</code> de https://github.com/josepgarcia/datos</p> <p>Realiza los scripts necesarios y ejec\u00fatalos en hadoop para contestar a las siguientes preguntas:</p> <ol> <li> <p>Temperatura m\u00e1s alta de cada pa\u00eds siempre (de todos los a\u00f1os). Indicar pa\u00eds, mes, a\u00f1o y temperatura. <pre><code>Ejemplo de salida (datos no reales):\n\nEspa\u00f1a  7  1980  41\nFrancia 8  1975  43\nItalia  7  1990  44\n.....\n</code></pre></p> </li> <li> <p>Media anual de temperatura por regi\u00f3n. <pre><code>Ejemplo de salida (datos no reales):\n\nAfrica  43,5\nEuropa  37,3\n.....\n</code></pre></p> </li> <li> <p>Media anual de temperatura por pa\u00eds y a\u00f1o. <pre><code>Ejemplo de salida (datos no reales):\n\nEspa\u00f1a  1975  34\nEspa\u00f1a  1976  35\nEspa\u00f1a  1977  33,5\nEspa\u00f1a  1978  34,8\nFrancia 1975  33\nFrancia 1976  34,3\nFrancia 1977  35,6\n.....\n</code></pre></p> </li> <li> <p>M\u00ednima temperatura por regi\u00f3n (mostrar tambi\u00e9n qu\u00e9 mes y a\u00f1o fue). <pre><code>Ejemplo de salida (datos no reales):\n\n\u00c1frica  2  1980  -10\nEuropa  3  1975  -13\n.....\n</code></pre></p> </li> </ol>"},{"location":"UD03/5.ejercicioTemperaturaSOL/","title":"Ejercicio Temperatura","text":"<p>Entregables</p> <p>Entregables en AULES</p> <p>Descarga el dataset <code>city_temperature.csv.zip</code> de https://github.com/josepgarcia/datos</p> <p>Realiza los scripts necesarios y ejec\u00fatalos en hadoop para contestar a las siguientes preguntas:</p> <ol> <li> <p>Temperatura m\u00e1s alta de cada pa\u00eds siempre (de todos los a\u00f1os). Indicar pa\u00eds, mes, a\u00f1o y temperatura. <pre><code>Ejemplo de salida (datos no reales):\n\nEspa\u00f1a  7  1980  41\nFrancia 8  1975  43\nItalia  7  1990  44\n.....\n</code></pre></p> </li> <li> <p>Media anual de temperatura por regi\u00f3n. <pre><code>Ejemplo de salida (datos no reales):\n\nAfrica  43,5\nEuropa  37,3\n.....\n</code></pre></p> </li> <li> <p>Media anual de temperatura por pa\u00eds y a\u00f1o. <pre><code>Ejemplo de salida (datos no reales):\n\nEspa\u00f1a  1975  34\nEspa\u00f1a  1976  35\nEspa\u00f1a  1977  33,5\nEspa\u00f1a  1978  34,8\nFrancia 1975  33\nFrancia 1976  34,3\nFrancia 1977  35,6\n.....\n</code></pre></p> </li> <li> <p>M\u00ednima temperatura por regi\u00f3n (mostrar tambi\u00e9n qu\u00e9 mes y a\u00f1o fue). <pre><code>Ejemplo de salida (datos no reales):\n\n\u00c1frica  2  1980  -10\nEuropa  3  1975  -13\n.....\n</code></pre></p> </li> </ol>"},{"location":"UD03/5.ejercicioTemperaturaSOL/#con-pandas","title":"Con pandas","text":"<pre><code>import pandas as pd\n\n# Supongamos que tienes un DataFrame con los datos de temperatura\n#df = pd.read_csv('./city_temperature.csv', low_memory=False, dtype={\ndf = pd.read_csv('./city_temperature.csv', dtype={\n    'column1': 'string',\n    'column2': 'string',\n    'column3': 'int',\n    'column4': 'float',\n    'City': 'string',\n    'Month': 'int',\n    'Year': 'int',\n    'AvgTemperature': 'float'\n})\n\n\ndef temperatura_mas_alta_por_pais(df):\n    # Agrupar por pa\u00eds y encontrar la temperatura m\u00e1xima\n    max_temp = df.loc[df.groupby('City')['AvgTemperature'].idxmax()]\n\n    # Seleccionar las columnas relevantes\n    resultado = max_temp[['City', 'Month', 'Year', 'AvgTemperature']]\n\n    return resultado\n\n# Ejemplo de uso\n# df = pd.DataFrame({\n#     'pais': ['Espa\u00f1a', 'Espa\u00f1a', 'Francia', 'Francia'],\n#     'mes': [6, 7, 6, 7],\n#     'a\u00f1o': [2021, 2021, 2021, 2021],\n#     'temperatura': [35, 40, 30, 32]\n# })\n\nprint(temperatura_mas_alta_por_pais(df))\n</code></pre>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/","title":"Ejercicio 1: an\u00e1lisis de ventas","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#problema-de-analisis-de-ventas-utilizando-hadoop-y-hdfs","title":"Problema de An\u00e1lisis de Ventas utilizando Hadoop y HDFS","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#contexto","title":"Contexto","text":"<p>Imagina que una cadena de tiendas de electr\u00f3nica tiene presencia en diferentes pa\u00edses y ha recopilado datos de ventas durante varios a\u00f1os. Estas tiendas venden diferentes tipos de productos, desde tel\u00e9fonos m\u00f3viles hasta electrodom\u00e9sticos, y los datos de las transacciones est\u00e1n almacenados en archivos distribuidos en un sistema de archivos distribuido, como HDFS (Hadoop Distributed File System).</p> <p>Tu trabajo es dise\u00f1ar un sistema de an\u00e1lisis utilizando Hadoop para procesar y obtener informaci\u00f3n \u00fatil sobre las ventas, con el objetivo de ayudar a los directivos a tomar decisiones estrat\u00e9gicas basadas en datos.</p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#objetivos-del-analisis","title":"Objetivos del An\u00e1lisis","text":"<ol> <li> <p>Identificar los productos m\u00e1s vendidos por cada categor\u00eda.</p> </li> <li> <p>Analizar las tendencias de ventas a lo largo del tiempo (por mes, trimestre y a\u00f1o).</p> </li> <li> <p>Determinar las regiones o ciudades que generan mayores ingresos.</p> </li> <li> <p>Estimar el impacto de las promociones en las ventas.</p> </li> <li> <p>Predecir las ventas futuras basadas en datos hist\u00f3ricos (opcional, si se desea extender el proyecto con aprendizaje autom\u00e1tico).</p> </li> </ol>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#datos-disponibles","title":"Datos Disponibles","text":"<p>Los datos est\u00e1n organizados en m\u00faltiples archivos CSV almacenados en HDFS, con el siguiente formato:</p> <p>Archivo: ventas.csv</p> <p>Cada fila en este archivo representa una transacci\u00f3n de venta:</p> <p>\u2022   transaction_id: Identificador \u00fanico de la transacci\u00f3n.</p> <p>\u2022   date: Fecha de la transacci\u00f3n (formato YYYY-MM-DD).</p> <p>\u2022   store_id: ID de la tienda.</p> <p>\u2022   product_id: ID del producto vendido.</p> <p>\u2022   category: Categor\u00eda del producto (por ejemplo, \u201cElectr\u00f3nica\u201d, \u201cElectrodom\u00e9sticos\u201d).</p> <p>\u2022   quantity: Cantidad de productos vendidos.</p> <p>\u2022   price: Precio unitario del producto.</p> <p>\u2022   total_amount: Cantidad total (precio * cantidad).</p> <p>\u2022   promotion: Indicador binario (0 o 1) para se\u00f1alar si la transacci\u00f3n se realiz\u00f3 con una promoci\u00f3n activa.</p> <p>\u2022   region: Regi\u00f3n o ciudad donde se realiz\u00f3 la venta.</p> <p>Archivo: productos.csv</p> <p>Informaci\u00f3n detallada de cada producto:</p> <p>\u2022   product_id: Identificador \u00fanico del producto.</p> <p>\u2022   name: Nombre del producto.</p> <p>\u2022   category: Categor\u00eda a la que pertenece el producto.</p> <p>\u2022   brand: Marca del producto.</p> <p>Archivo: tiendas.csv</p> <p>Informaci\u00f3n sobre las tiendas:</p> <p>\u2022   store_id: Identificador \u00fanico de la tienda.</p> <p>\u2022   store_name: Nombre de la tienda.</p> <p>\u2022   region: Regi\u00f3n donde se encuentra la tienda (ciudad/pa\u00eds).</p> <p>\u2022   opening_date: Fecha en que la tienda abri\u00f3 sus puertas.</p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#solucion","title":"Soluci\u00f3n","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#almacenamiento-de-datos-en-hdfs","title":"Almacenamiento de Datos en HDFS","text":"<p>\u2022   Cargar los archivos ventas.csv, productos.csv y tiendas.csv en HDFS.</p> <p>\u2022   Crear directorios en HDFS para almacenar los datos, por ejemplo:</p> <p>\u2022   /user/ventas/</p> <p>\u2022   /user/productos/</p> <p>\u2022   /user/tiendas/</p> <p>\u2022   Usar los comandos de HDFS para mover los archivos desde el sistema local a HDFS:</p> <pre><code>hdfs dfs -mkdir /user/ventas\nhdfs dfs -mkdir /user/productos\nhdfs dfs -mkdir /user/tiendas\n\nhdfs dfs -put ventas.csv /user/ventas/\nhdfs dfs -put productos.csv /user/productos/\nhdfs dfs -put tiendas.csv /user/tiendas/\n</code></pre>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#procesamiento-de-ventas-hadoop-mapreduce","title":"Procesamiento de Ventas (Hadoop MapReduce)","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#1-total-de-ventas-por-producto-y-por-region","title":"1. Total de ventas por producto y por regi\u00f3n.","text":"<p>El objetivo de este MapReduce es sumar las ventas totales por cada producto y por cada regi\u00f3n. El mapper dividir\u00e1 las ventas por product_id y region, y el reducer sumar\u00e1 los totales.</p> <p>\u2022   Mapper: Procesa cada l\u00ednea de ventas.csv, extrae product_id, region, y total_amount.</p> <p>\u2022   Reducer: Recibe las claves (product_id, region) y suma los valores (total_amount).</p> <pre><code>**mapper**:\n\ndef map(line):\n    fields = line.split(',')\n    product_id = fields[3]\n    region = fields[8]\n    total_amount = float(fields[6])\n    emit((product_id, region), total_amount)\n\n\n**reducer:**\n\ndef reduce(key, values):\n    total_sales = sum(values)\n    emit(key, total_sales)\n</code></pre> <p>Salida esperada:</p> <p>\u2022   (product_id, region) -&gt; total_sales</p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#2-identificar-el-producto-mas-vendido-por-categoria","title":"2. Identificar el producto m\u00e1s vendido por categor\u00eda.","text":"<p>El objetivo de este MapReduce es determinar cu\u00e1l fue el producto m\u00e1s vendido por cada categor\u00eda.</p> <p>\u2022   Mapper: Procesa cada l\u00ednea de ventas.csv, emite category, product_id, y la cantidad vendida (quantity).</p> <p>\u2022   Reducer: Suma las cantidades vendidas por product_id y encuentra el producto con la mayor suma para cada categor\u00eda.</p> <pre><code>**mapper:**\n\ndef map(line):\n    fields = line.split(',')\n    category = fields[4]\n    product_id = fields[3]\n    quantity = int(fields[5])\n    emit((category, product_id), quantity)\n\n**reducer:**\n\ndef reduce(key, values):\n    total_quantity = sum(values)\n    emit(key, total_quantity)\n</code></pre> <p>Paso Adicional: Para identificar el producto m\u00e1s vendido por categor\u00eda, se podr\u00eda ejecutar una segunda fase de reducci\u00f3n donde se elige el producto con la mayor cantidad por categor\u00eda.</p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#analisis-de-tendencias-hive-pig","title":"An\u00e1lisis de Tendencias (HIVE + PIG)","text":"<p>En lugar de usar solo MapReduce, se podr\u00eda utilizar Pig o Hive para analizar las ventas en intervalos de tiempo (mes, trimestre, a\u00f1o).</p> <p>\u2022   Consulta en Hive: Se puede escribir una consulta en Hive para agrupar las ventas por mes, trimestre o a\u00f1o.</p> <p>Ejemplo de consulta en Hive para ventas mensuales:</p> <pre><code>SELECT year(date), month(date), SUM(total_amount)\nFROM ventas\nGROUP BY year(date), month(date)\nORDER BY year(date), month(date);\n</code></pre> <pre><code>\u2022   **Consulta en Pig:** Para Pig, se podr\u00eda cargar el archivo de ventas y luego hacer la agregaci\u00f3n:\n</code></pre> <pre><code>ventas = LOAD '/user/ventas/ventas.csv' USING PigStorage(',') AS (transaction_id:chararray, date:chararray, store_id:int, product_id:int, category:chararray, quantity:int, price:float, total_amount:float, promotion:int, region:chararray);\n\nventas_por_mes = FOREACH ventas GENERATE SUBSTRING(date, 0, 7) as mes, total_amount;\nventas_agrupadas = GROUP ventas_por_mes BY mes;\nventas_totales = FOREACH ventas_agrupadas GENERATE group AS mes, SUM(ventas_por_mes.total_amount) AS total_ventas;\n</code></pre>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Ejercicio%201%20ana%CC%81lisis%20de%20ventas%20126e913de6c480f79d1bc5e4f99b1904/#paso-opcional-exportar-resultados","title":"Paso Opcional: Exportar Resultados","text":"<p>Al finalizar el procesamiento, los resultados podr\u00edan guardarse nuevamente en HDFS o exportarse a bases de datos externas como HBase o una base de datos SQL para generar informes.</p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/Mapreduce%20-%20Bash%20script%20fffe913de6c481c8bbabe93106da5b3d/","title":"Mapreduce - Bash script","text":"<p>https://princetonits.com/blog/technology/hadoop-mapreduce-streaming-using-bash-script/</p> <p>Step 1: Create a mapper script(word_length.sh) on your local file system</p> <pre><code>#!/bin/bash\n#This mapper script will read one line at a time and then break it into words\n#For each word starting LETTER and LENGTH of the word are emitted\nwhile read line\ndo\n for word in $line do\n if [ -n $word ] then\n    wcount=`echo $word | wc -m`;\n    wlength=`expr $wcount - 1`;\n    letter=`echo $word | head -c1`;\n    echo -e \"$lettert$wlength\";\n fi\ndone\ndone\n#The output of the mapper would be \u201cstarting letter of each word\u201d and \u201cits length\u201d, separated by a tab space.\n</code></pre> <p>Step 2: \u00a0Create a reducer script(average_word_length.sh)</p> <pre><code>#!/bin/bash\n#This reducer script will take-in output from the mapper and emit starting letter of each word and average length\n#Remember that the framework will sort the output from the mappers based on the Key\n#Note that the input to a reducer will be of a form(Key,Value)and not (Key,\n#This is unlike the input i.e.; usually passed to a reducer written in Java.\nlastkey=\"\";\ncount=0;\ntotal=0;\niteration=1\nwhile read line\n do\n  newkey=`echo $line | awk '{print $1}'`\n  value=`echo $line | awk '{print $2}'`\n   if [ \"$iteration\" == \"1\" ] then\n        lastkey=$newkey;\n        iteration=`expr $iteration + 1`;\n   fi\n   if [[ \"$lastkey\" != \"$newkey\" ]] then\n    average=`echo \"scale=5;$total / $count\" | bc`;\n    echo -e \"$lastkeyt$average\"\n    count=0;\n    lastkey=$newkey;\n    total=0;\n    average=0;\n   fi\n   total=`expr $total + $value`;\n   lastkey=$newkey;\n   count=`expr $count + 1`;\ndone\n#The output would be Key,Value pairs(letter,average length of the words starting with this letter)\n</code></pre> <p>Step 3: Run the job from the terminal using Hadoop streaming command</p> <p>hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming*.jar -input /input -output /avgwl -mapper mapper.sh -reducer reducer.sh -file /home/user/mr_streaming_bash/mapper.sh -file /home/user/mr_streaming_bash/reducer.sh</p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/","title":"UD03 5. Mapreduce con Python \ud83d\udc0d","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#que-es-mapreduce","title":"\u00bfQu\u00e9 es MapReduce?","text":"<p>El objetivo de esta gu\u00eda es trabajar con la resoluci\u00f3n de ejercicios siguiendo el modelo de programaci\u00f3n MapReduce con Python para entender los principales fundamentos de MapReduce para dar soporte a la computaci\u00f3n paralela sobre grandes colecciones de datos en un contexto de procesamiento distribuido.</p> <p>La mayor\u00eda de las tareas de an\u00e1lisis deben poder combinar los datos de alguna manera, y es posible que los datos le\u00eddos de un disco deban combinarse con los datos de otros. MapReduce proporciona un modelo de programaci\u00f3n que abstrae el problema de las lecturas y escrituras de disco, transform\u00e1ndolo en un c\u00e1lculo sobre conjuntos de claves y valores. Para ello, MapReduce combina su operatoria en dos partes en el c\u00e1lculo: el mapeo y la reducci\u00f3n, y es la interfaz entre las dos donde ocurre la \"mezcla\".</p> <p>MapReduce es un procesador de consultas batch, con capacidad de ejecutar una consulta ad hoc en todo un conjunto de datos, del orden de los petabytes, y obtener resultados en un tiempo razonable.</p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#programando-mapreduce-simple","title":"Programando MapReduce \u201cSimple\u201d","text":"<p>Ejercicio: Vamos a programar un proceso MapReduce que nos permita realizar un conteo de palabras para saber, en un supuesto dataset del orden de los petabytes, la cantidad de veces que aparece cada palabra.</p> <p>Vamos a emular el funcionamiento de Hadoop MapReduce en funci\u00f3n del siguiente esquema de funcionamiento:</p> <p></p> <p>En resumen, en el siguiente ejercicio, vamos a hacer un conteo de palabras en funci\u00f3n de la siguiente operatoria:</p> <ol> <li>Vamos a suponer que tenemos 3 nodos que se distribuyen los datos en 3 split de datos diferentes.</li> <li>En cada nodo se va a procesar un split diferente, transformando cada palabra a un par\u00a0\u00a0(el separador ser\u00e1 el\u00a0tab). <li>Luego, tendremos un sorter.py que ordenar\u00e1 las claves emulando la operaci\u00f3n de Hadoop que realiza esta actividad autom\u00e1ticamente.</li> <li>Luego habr\u00e1 un merger que mezclar\u00e1 los archivos procesados en cada nodo emulando a Hadoop que realiza esta operaci\u00f3n autom\u00e1ticamente.</li> <li>En este ejercicio vamos a suponer que tenemos un \u00fanico Reducer.</li> <li>Por \u00faltimo, habr\u00e1 un reducer.py que integrar\u00e1 los datos en un \u00fanico archivo de salida que tendr\u00e1 el procesamiento final de los datos.</li> <p>El flujo de datos puede observarse en el siguiente esquema:</p> <p></p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#1-mapper","title":"1. Mapper","text":"<p>Cada mapper leer\u00e1 un split de datos y lo procesar\u00e1. El objetivo de esta actividad es convertir la entrada en un par  que permita resolver el problema. Como lo que buscamos es realizar un conteo de palabras para saber, en un supuesto dataset del orden de los petabytes, la cantidad de veces que aparece cada palabra, vamos a tomar como clave la palabra y al valor le asignaremos 1 que indicar\u00e1 la ocurrencia de esa palabra. <ol> <li>Leemos un fichero l\u00ednea a l\u00ednea</li> <li>Separamos cada palabra de la l\u00ednea</li> <li> <p>Imprimimos cada palabra de la siguiente manera:</p> <p>Palabra  1 <pre><code>En     1\nun     1\nlugar  1 \nde     1\nla     1\nMancha 1\n...\nde     1\n...\nMancha 1\n.....\n</code></pre> <p>Aunque aparezca la misma palabra despu\u00e9s, siempre se indicar\u00e1 un uno despu\u00e9s del tabulador</p> <li> <p>C\u00f3digo</p> <pre><code>#!/usr/bin/env python\n\"\"\"mapper_simple.py\"\"\"  # &lt;- Triple comilla, generar documentacion\n\nimport string\n\nfile = 'test.txt'\n\nf = open(file, 'r')\n\nfor line in f:\n    # Eliminar espacios iniciales y finales\n    line = line.strip()\n    # Separar l\u00ednea el palabras\n    words = line.split()\n    # Recorremos las palabras\n    for word in words:\n        print(word, '\\t', 1)\n\nf.close()\n</code></pre> </li>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#2-sorter","title":"2. Sorter","text":"<p>Una de las operaciones que provee autom\u00e1ticamente Hadoop MapReduce es el sort por claves a la salida de cada nodo. </p> <p>Nosotros lo programamos puesto que estamos emulando el funcionamiento.</p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#3-reducer","title":"3. Reducer","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#ejecutar-hadoop-mapreducer-utilizando-los-scripts-realizados","title":"Ejecutar hadoop mapreducer utilizando los scripts realizados","text":"<ul> <li>En primer lugar, iniciar HDFS y YARN.</li> <li>Copiar en la m\u00e1quina virtual los archivos mapper.py y reducer.py</li> <li>Descargar quijote.txt</li> </ul> <p>https://babel.upm.es/~angel/teaching/pps/quijote.txt</p> <ul> <li>Ejecutar el siguiente comando, vamos a utilizar la utilidad <code>hadoop-streaming</code></li> </ul> <pre><code>$ hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -file mapper.py -mapper mapper.py -file reducer.py -reducer reducer.py -input ../quijote.txt -output /quijote_salida\n</code></pre> <p>Error 1:</p> <pre><code>ERROR streaming.StreamJob: Error Launching job : Input path does not exist: hdfs://debianh:9000/user/quijote.txt\nStreaming Command Failed!\n</code></pre> <ul> <li> <p>Sol</p> <pre><code>hdfs dfs -put ../quijote.txt /\n</code></pre> </li> </ul> <p>Error 2:</p> <pre><code>Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapreduce.v2.app.MRAppMaster\nPlease check whether your &lt;HADOOP_HOME&gt;/etc/hadoop/mapred-site.xml contains the below configuration:\n&lt;property&gt;\n&lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;\n&lt;value&gt;HADOOP_MAPRED_HOME=${full path of your hadoop distribution directory}&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;mapreduce.map.env&lt;/name&gt;\n&lt;value&gt;HADOOP_MAPRED_HOME=${full path of your hadoop distribution directory}&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;mapreduce.reduce.env&lt;/name&gt;\n&lt;value&gt;HADOOP_MAPRED_HOME=${full path of your hadoop distribution directory}&lt;/value&gt;\n&lt;/property&gt;\nFor more detailed output, check the application tracking page: http://debianh:8088/cluster/app/application_1700062943857_0003 Then click on links to logs of each attempt.\n. Failing the application.\n</code></pre> <ul> <li> <p>Sol</p> <pre><code>https://stackoverflow.com/questions/50927577/could-not-find-or-load-main-class-org-apache-hadoop-mapreduce-v2-app-mrappmaster\n\nEdit mapred-site.xml\nAdd:\n&lt;property&gt;\n   &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;\n   &lt;value&gt;HADOOP_MAPRED_HOME=/opt/hadoop&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> </li> </ul> <p>Error 3:</p> <pre><code>Error: Could not find or load main class org.apache.hadoop.mapred.YarnChild\n</code></pre> <ul> <li> <p>Sol</p> <pre><code>Ejecutar: \n\n$ hadoop classpath\n\nA\u00f1adir en yarn-site.xml la salida del comando anterior\n\n&lt;property&gt;\n    &lt;name&gt;yarn.application.classpath&lt;/name&gt;\n    &lt;value&gt;**SALIDA COMANDO ANTERIOR**&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> </li> </ul> <p>Error 4:</p> <pre><code>FAILED\nError: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 127\n</code></pre> <ul> <li> <p>Sol</p> <pre><code>Cambiar el entorno de ejecuci\u00f3n de los scripts en pyton a:\n\n#!/usr/bin/env python3\n</code></pre> </li> </ul> <p>Error 5, problema con el reducer</p> <p>https://stackoverflow.com/questions/52076577/error-java-lang-runtimeexception-pipemapred-waitoutputthreads-subprocess-fa</p> <pre><code>first line :  `#!/usr/bin/python` \n\nsecond line : `# -*-coding:utf-8 -*`\n\nchmod +x WordCountMapper.py\nchmod +x WordCountReducer.py\n\n########### Use this command :\nmapred streaming -files WordCountMapper.py,WordCountReducer.py \\\n-input /data/input/README.TXT \\\n-output /data/output \\\n-mapper ./WordCountMapper.py \\\n-reducer ./WordCountReducer.py\n</code></pre> <p>\u00bfC\u00f3mo solucionar el Warning?</p> <pre><code>WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n\n# Otra llamada con -fles\n\n**hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\**\n  -D stream.num.map.output.key.fields=2 \\\n  -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n  -D mapreduce.job.output.key.comparator.class=\\\norg.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n  -D mapreduce.partition.keycomparator.options=\"-k1n -k2nr\" \\\n****  -files secondary_sort_map.py,secondary_sort_reduce.py \\\n  -input input/ncdc/all \\\n  -output output-secondarysort-streaming \\\n  **-mapper ch09-mr-features/src/main/python/secondary_sort_map.py \\**\n  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n  **-reducer ch09-mr-features/src/main/python/secondary_sort_reduce.py**\n</code></pre> <p>Podemos ver el proceso</p> <p></p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#funciona","title":"Funciona","text":"<pre><code>**mapper.py**\n#!/usr/bin/env python3\n\nimport sys\n\nfor line in sys.stdin:\n    line = line.strip()\n    words = line.split()\n    for word in words:\n        print('%s\\t%s' % (word, 1))\n\n####################################################################################\n####################################################################################\n\n**reducer.py**\n#!/usr/bin/env python3\n\"\"\"reducer.py\"\"\"\n\nimport sys\n\nword = None\ncurrent_word = None\ncurrent_count = 0\n\n# input comes from STDIN (standard input)\nfor line in sys.stdin:\n        line = line.strip()\n        word, count = line.split('\\t', 1)\n\n        try:\n          count = int(count)\n        except ValueError:\n          continue\n\n        # Para que funciona la entrada debe estar ordenada (sort)\n        # en hadoop funciona directamente porque ya hace un sort\n        if current_word == word:\n          current_count += count\n        else:\n          if current_word: # Primera iteraci\u00f3n es None\n            print ('%s\\t%s' % (current_word, current_count))\n          current_word = word\n          current_count = 1\n\nif current_word == word:\n  print ('%s\\t%s' % (current_word, current_count))\n\n####################################################################################\n####################################################################################\n\n$ mapred streaming -files mapper2.py,reducer.py -input /quijote.txt \n-output /quijote_salida1 -mapper ./mapper2.py -reducer ./reducer.py\n\n$ hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -file mapper2.py \n-mapper mapper2.py -file reducer.py -reducer reducer.py -input /quijote.txt \n-output /quijote_salida2\n</code></pre>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#mejorando-mapreducer","title":"Mejorando MapReducer","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#1-mapper_1","title":"1. Mapper","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#2-sorter_1","title":"2. Sorter","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#3-reducer_1","title":"3. Reducer","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#revisar","title":"Revisar:","text":"<ul> <li>[ ]  https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/<ul> <li>[ ]  Runing the phython code in hadoop</li> <li>[ ]  Se puede acceder desde web a hadoop</li> </ul> </li> <li>[ ]  https://colab.research.google.com/github/bdm-unlu/2021/blob/main/guias/Guia_IntroMapReduce.ipynb#scrollTo=Wc5iyJ0qB8qh</li> <li>[ ]  </li> </ul>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20Mapreduce%20con%20Python%20%F0%9F%90%8D%20fffe913de6c4815b9aecfc4cc69f6780/#creditos-info","title":"Cr\u00e9ditos + Info","text":"<p>https://colab.research.google.com/github/bdm-unlu/2021/blob/main/guias/Guia_IntroMapReduce.ipynb#scrollTo=4OqzVfBpU8yx</p> <p>https://github.com/bdm-unlu/2023/blob/master/guias/Guia_IntroMapReduce.ipynb</p>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20%5BEJ%5D%20Hadoop%20Temperatura%20fffe913de6c4813592e6fcd836f1ece0/","title":"UD03 5. [EJ] Hadoop Temperatura.","text":"<p>Descarga el dataset (aules: <code>Dataset temperatura</code>)</p> <p>Realiza los scripts necesarios y ejec\u00fatalos en hadoop para contestar a las siguientes preguntas:</p> <ol> <li> <p>Temperatura m\u00e1s alta de cada pa\u00eds siempre (de todos los a\u00f1os). Indicar pa\u00eds, mes, a\u00f1o y temperatura.</p> <pre><code>Ejemplo de salida (datos no reales):\n    Espa\u00f1a  7  1980  41\n  Francia 8  1975  43\n  Italia  7  1990  44\n  .....\n</code></pre> </li> <li> <p>Media anual de temperatura por regi\u00f3n.</p> <pre><code>Ejemplo de salida (datos no reales):\n    Africa  43,5\n    Europa  37,3\n  .....\n</code></pre> </li> <li> <p>Media anual de temperatura por pa\u00eds y a\u00f1o.</p> <pre><code>Ejemplo de salida (datos no reales):\n    Espa\u00f1a  1975  34\n    Espa\u00f1a  1976  35\n    Espa\u00f1a  1977  33,5\n    Espa\u00f1a  1978  34,8\n  Francia 1975  33\n  Francia 1976  34,3\n  Francia 1977  35,6\n  .....\n</code></pre> </li> <li> <p>M\u00ednima temperatura por regi\u00f3n (mostrar tambi\u00e9n qu\u00e9 mes y a\u00f1o fue).</p> <pre><code>Ejemplo de salida (datos no reales):\n    \u00c1frica  2  1980  -10\n  Europa  3  1975  -13\n  .....\n</code></pre> </li> </ol>"},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20%5BGuiado%5D%20MovieLens%20Dataset%20127e913de6c4802ba951fbfc0f95237b/","title":"UD03 5. [Guiado] MovieLens Dataset","text":""},{"location":"UD03/_PENDENT/UD03%20-%20Mapreduce%20Yarn%20fffe913de6c48192bd2aceae7263b15e/UD03%205%20%5BGuiado%5D%20MovieLens%20Dataset%20127e913de6c4802ba951fbfc0f95237b/#instalando-dataset","title":"Instalando dataset","text":"<p>Seguir los pasos</p> <p>https://www.udemy.com/course/the-ultimate-hands-on-hadoop-tame-your-big-data/learn/lecture/5951202#overview</p>"},{"location":"UD04/0.index/","title":"Hive. Sqoop.","text":""},{"location":"UD04/0.index/#hive","title":"Hive","text":"<p>UD04 Hive (PRESENTACI\u00d3N)</p>"},{"location":"UD04/0.index/#sqoop","title":"Sqoop","text":"<p>C\u00f3mo utilizar bases de datos relacionales con Hadoop. </p>"},{"location":"UD04/1.hive-instalacion/","title":"HIVE: Instalaci\u00f3n y configuraci\u00f3n inicial.","text":"<p>Herramienta desarrollada por Facebook y luego donada a la Apache Software Foundation.</p> <p>Proporciona una interfaz de consulta similar a SQL (Structured Query Language) para interactuar con grandes conjuntos de datos almacenados en Hadoop Distributed File System (HDFS) o en otros sistemas de almacenamiento compatibles con Hadoop.</p> <p>Funcionamiento:</p> <ol> <li>Metastore: Almacena informaci\u00f3n sobre el esquema de datos y las particiones en una base de datos relacional para facilitar la administraci\u00f3n de metadatos.</li> <li>HQL (Hive Query Language): Los usuarios escriben consultas en un lenguaje similar a SQL llamado HQL para expresar las operaciones que desean realizar en los datos.</li> <li>Traducci\u00f3n a MapReduce: Las consultas HQL se traducen internamente en tareas MapReduce, que se ejecutan en un cl\u00faster Hadoop distribuido.</li> <li>Optimizaciones internas: Hive realiza optimizaciones para mejorar el rendimiento, como la eliminaci\u00f3n de pasos redundantes y la reorganizaci\u00f3n eficiente de tareas MapReduce.</li> <li>Almacenamiento de datos: Los resultados de las consultas se almacenan en HDFS o en otros sistemas compatibles con Hadoop, organizados en tablas y en formatos espec\u00edficos para un almacenamiento eficiente.</li> <li>Integraci\u00f3n: Hive se integra con diversas herramientas en el ecosistema Hadoop, permitiendo a los usuarios utilizar diferentes herramientas seg\u00fan sus necesidades.</li> </ol>"},{"location":"UD04/1.hive-instalacion/#componentes","title":"Componentes","text":""},{"location":"UD04/1.hive-instalacion/#instalacion","title":"Instalaci\u00f3n","text":"<ol> <li>Descargaremos en /usr/local la versi\u00f3n 2.3.9 Versi\u00f3n compatible con el Java 11 que tenemos instalado, con la versi\u00f3n 3.1.3 dar\u00e1 error.</li> </ol> <p>https://archive.apache.org/dist/hive/hive-2.3.9/apache-hive-2.3.9-bin.tar.gz</p> <pre><code>1. Descargar en /opt/hadoop o en /usr/local dependiendo de donde hayas instalado hadoop\nwget https://archive.apache.org/dist/hive/hive-2.3.9/apache-hive-2.3.9-bin.tar.gz\n2. Descomprimir\n\n3. Crear \"enlace\" blando llamado \"hive\" que apunte a la carpeta descomprimida\n</code></pre> <ol> <li>A\u00f1adimos HIVE_HOME a .bashrc y modificamos el PATH</li> </ol> <pre><code>## Sustituye el path si tu instalaci\u00f3n de hive no se encuentra en este directorio\n\nexport HIVE_HOME=/opt/hadoop/hive\nexport PATH=$PATH:$HIVE_HOME/bin\n</code></pre> <p>ATENCI\u00d3N</p> <p>Hemos puesto nuevas variables y paths en el .bashrc, hemos de \u201cejecutarlo\u201d para que estos cambios hagan efecto en la sesi\u00f3n actual.</p> <p>Dentro del directorio bin de hive podemos ver los siguientes binarios: - <code>beeline</code>: herramienta cliente modo comandos. - <code>hive</code>: herramienta cliente. - <code>hiveserver2</code>: servidor de hive. - <code>schematools</code>: herramienta que nos permite trabajar con los metadatos de hive.</p> <ol> <li>Configuramos Hive (carpeta <code>conf</code> dentro de hive)</li> </ol> <pre><code># Copiamos los siguientes templates por defecto\n\ncp hive-default.xml.template hive-site.xml\ncp hive-env.sh.template hive-env.sh\ncp hive-exec-log4j2.properties.template hive-exec-log4j2.properties\ncp hive-log4j2.properties.template hive-log4j2.properties\ncp beeline-log4j2.properties.template beeline-log4j2.properties\n</code></pre> <ol> <li>Editamos <code>hive-env.sh</code> y a\u00f1adimos:</li> </ol> <pre><code>export HADOOP_HOME=/opt/hadoop\nexport HIVE_CONF_DIR=/opt/hadoop/hive/conf\n</code></pre> <ol> <li>Hive ataca al sistema de ficheros HDFS de Hadoop. Por lo tanto, es necesario que para trabajar con Hive creemos una serie de directorios en el HDFS.</li> </ol> <pre><code># Dar\u00e1 error si ya existe\nhdfs dfs -mkdir /tmp\n\n# Como vamos a trabajar con el usuario hadoop\n# damos permisos a /tmp para que no de problemas\nhdfs dfs -chmod g+w /tmp\n\n# Directorio donde trabaja Hive, no cambiar de nombre\nhdfs dfs -mkdir -p /user/hive/warehouse\n\n# Damos permisos para que Hive pueda trabajar aqu\u00ed.\n# Le daremos permisos al grupo hadoop, que es el que usaremos con Hive\nhdfs dfs -chmod g+w /user/hive/warehouse\n</code></pre> <ol> <li>A\u00f1adimos al archivo hive-site.xml:</li> </ol> <pre><code>&lt;property&gt;\n    &lt;name&gt;system:java.io.tmpdir&lt;/name&gt;\n    &lt;value&gt;/tmp/hive/java&lt;value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;system:user.name&lt;/name&gt;\n    &lt;value&gt;${user.name}&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>\u2192 Directorio temporal</p> <p>\u2192 Que utilice como nombre de usuario el del usuario que lo est\u00e1 ejecutando (en nuestro caso ser\u00e1 hadoop), hubi\u00e9semos podido escribir en value hadoop</p> <ol> <li>Creamos una base de datos a trav\u00e9s del cliente hive.</li> </ol> <pre><code>$ pwd\n/usr/local/hive\n\n# Crea una carpeta y si la creaci\u00f3n ha sido correcta entra en ella\n$ mkdir bbdd &amp;&amp; cd bbdd\n\n# Inicialicamos una base de datos (un esquema)\n$ schematool -dbType derby -initSchema\n\n$ ls\nderby.log # log de la base de datos\nmetastore_db # directorio de la base de datos, no tocar ficheros\n\n# validar el schema que hemos creado, para conocer el comando por si posteriormente hay alg\u00fan error\n$ schematool -validate -dbType -verbose\n## Este comando dar\u00e1 error, falta un par\u00e1metro\n\n# El comando debe devolver:\nStarting metastore validation\n\nValidating schema version\nSucceeded in schema version validation.\n[SUCCESS]\n\nValidating sequence number for SEQUENCE_TABLE\nSucceeded in sequence number validation for SEQUENCE_TABLE\n[SUCCESS]\n\nValidating metastore schema tables\nSucceeded in schema table validation.\n[SUCCESS]\n\nValidating database/table/partition locations\nSucceeded in database/table/partition location validation\n[SUCCESS]\n\nValidating columns for incorrect NULL values\nSucceeded in column validation for incorrect NULL values\n[SUCCESS]\n\nDone with metastore validation: [SUCCESS]\nschemaTool completed\n</code></pre> <p>POSIBLE ERROR <pre><code>Resolved by using below steps:step 1: Go to the HADOOP_INSTALLATION_DIR/share/hadoop/common/lib and check the guava.jar version step 2: Now go to HIVE_INSTALLATION_DIR/lib and compare the guava file version of hive with hadoop. If they are not same, delete the older version among them and copy the newer version in both.\n\n[https://stackoverflow.com/questions/58903865/i-installed-hadoop-3-2-1-and-top-of-hadoop-installed-hive-on-centos7-and-getting](https://stackoverflow.com/questions/58903865/i-installed-hadoop-3-2-1-and-top-of-hadoop-installed-hive-on-centos7-and-getting)\n</code></pre></p>"},{"location":"UD04/1.hive-instalacion/#dbtype-derby","title":"dbType derby","text":"<p>\u00bf Qu\u00e9 significa el comando: \u201cschematool -dbType derby -initSchema\u201d?</p> <p>Utilizaremos un \u201cgestor de base de datos\u201d llamado \u201cderby\u201d, se utiliza para testing, es muy ligero. Al inicializar se crean 2 archivos (log y metastore), por eso hemos inicializado dentro de una carpeta (bbdd), para que no queden en la ra\u00edz de hive.</p> <p>Otras opciones posibles ser\u00edan:</p> <pre><code>schematool -dbType mysql -initSchema\nschematool -dbType postgres -initSchema\nschematool -dbType oracle -initSchema\nschematool -dbType mssql -initSchema\n</code></pre> <ol> <li>Atacamos el metastore con el cliente hive</li> </ol> <p>ATENCI\u00d3N</p> <p>Para ejecutar hive debemos estar en la carpeta de la bbdd que hemos creado anteriormente, a esta carpeta la hemos llamado bbdd y ten\u00eda los metadatos para dbType \"derby\"</p> <pre><code>$ hive\n\n....\n\nhive&gt; create database ejemplo;\nshow databases;\nuse ejemplo;\nshow tables;\n\ncreate table if not exists t1 (\nname string\n);\n\nshow tables;\n</code></pre> <ol> <li>Vamos a la p\u00e1gina de hadoop (puerto 9870) y localizamos la base de datos.</li> </ol> <p></p> <ol> <li>Insertamos una fila</li> </ol> <pre><code>insert into t1 values ('Mi nombre');\n\nselect * from t1;\n</code></pre> <p>start-yarn.sh</p> <ol> <li>Creamos otra tabla e insertamos otro registro</li> </ol> <pre><code>create table if not exists t2 (\n    codigo integer\n);\n\nInsert into t2 values (10);\nselect * from t2;\n</code></pre> <p>\u2705 Ejercicio: instalaci\u00f3n y configuraci\u00f3n. 1. Muestra el contenido de las tablas a trav\u00e9s de la web de HDFS y realiza una captura de pantalla. 2. Muestra el contenido de las tablas a trav\u00e9s de la consola, con el comando hdfs y realiza una captura de pantalla.</p>"},{"location":"UD04/1.hive-instalacion/#comandos","title":"Comandos","text":""},{"location":"UD04/2.hive-tablas/","title":"Hive. Tablas.","text":""},{"location":"UD04/2.hive-tablas/#tipos-de-tablas","title":"Tipos de tablas","text":""},{"location":"UD04/2.hive-tablas/#tablas-internas","title":"Tablas Internas","text":"<ul> <li>Son gestionadas completamente por Hive.</li> <li>Los datos y la estructura est\u00e1n bajo el control total de Hive, incluyendo la eliminaci\u00f3n de datos cuando se elimina la tabla.</li> </ul> <pre><code>CREATE TABLE empleados (\n  id INT,\n  nombre STRING,\n  salario DOUBLE\n);\n</code></pre>"},{"location":"UD04/2.hive-tablas/#tablas-externas","title":"Tablas Externas","text":"<ul> <li>Hive gestiona solo la estructura de metadatos, pero los datos residen fuera de Hive.</li> <li>Los datos NO se eliminan autom\u00e1ticamente cuando se elimina la tabla en Hive.</li> </ul> <pre><code>CREATE EXTERNAL TABLE empleados_externos (\n  id INT,\n  nombre STRING,\n  salario DOUBLE\n)\nLOCATION '/ruta/en/hadoop/datos/empleados';\n</code></pre> <p>En ambos casos, estas tablas pueden ser consultadas y utilizadas en consultas SQL dentro de Hive, pero la diferencia clave radica en la gesti\u00f3n de los datos y la persistencia.</p>"},{"location":"UD04/2.hive-tablas/#cuales-utilizamos","title":"\u00bfCu\u00e1les utilizamos?","text":"<p>\u00bfPor qu\u00e9 utilizar tablas internas? \u00bfen qu\u00e9 casos?</p> <ul> <li>Se necesita que Hive se encargue de garantizar la integridad y coherencia de datos.</li> <li>Gesti\u00f3n de recursos, con tablas internas se elimina el dato completamente.</li> <li>Hive puede optimizar consultas ya que tiene el control total sobre la distribuci\u00f3n y organizaci\u00f3n de los datos.</li> <li>Proporcionan un extra de seguridad ya que se gestionan pol\u00edticas de seguridad a nivel de Hive.</li> </ul> <p>El uso de tablas internas en Hive es beneficioso cuando se busca un control m\u00e1s granular sobre la administraci\u00f3n de datos, garantizando la consistencia, la seguridad y la eficiencia en los procesos anal\u00edticos y de negocio.</p> <p>\u00bfPor qu\u00e9 utilizar datas externas? \u00bfen qu\u00e9 casos?</p> <ul> <li>Cuando los datos son compartidos entre varios sistemas.</li> <li>Si los datos son generados por procesos externos a Hive.</li> <li>Cuando los datos est\u00e1n en formatos espec\u00edficos (orc, avro\u2026) y se quiere acceder a ellos sin tener que realizar una carga previa de los datos.</li> <li>Cuando los datos se encuentran en almacenamientos externos como Amazon S3, Azure y se desea acceder a ellos sin moverlos f\u00edsicamente a HDFS.</li> </ul> <p>Las tablas externas son ideales cuando se trata de acceder a datos que est\u00e1n fuera del control directo de Hive, proporcionando flexibilidad en el acceso a informaci\u00f3n compartida entre sistemas y permitiendo la lectura directa de datos en su ubicaci\u00f3n original.</p>"},{"location":"UD04/2.hive-tablas/#internas","title":"Internas","text":"<p>Nos creamos un fichero plano con una nueva terminal.</p> <pre><code>cd /tmp/hadoop\n\nvi empleados.txt\nRosa,50\nPedro,60\nRaul,56\nMaria,35\n</code></pre> <p>Vamos a la terminal conectada a Hive y la base de datos ejemplo. Creamos una nueva tabla empleados especificando que la cargaremos de fichero especificando que esta formateadas por filas y separadas por comas.</p> <pre><code>hive&gt;\ncreate table empleados\n(\nnombre string,\nedad integer\n)\nrow format delimited\nfields terminated by ',';\nload data local inpath '/tmp/hadoop/empleados.txt' into table empleados;\n</code></pre> <p>Volvemos a repetir la creaci\u00f3n de otro fichero (p.e. copiando los datos del anterior) y lo cargamos en Hive. Veremos que Hive lo guarda todo como ficheros en el directorio correspondiente de hdfs.</p> <pre><code>load data local inpath '/tmp/hadoop/empleados2.txt' into table empleados; \n## (local especifica que busca el fichero en la maquina local)\n</code></pre> <p>Podemos realizar b\u00fasquedas:</p> <pre><code>select * from empleados where edad &gt; 50;\n</code></pre> <p>** Accedemos a web HDFS para ver c\u00f3mo ha importado los archivos) **</p> <p>Si ahora borramos la tabla con Hive. Veremos c\u00f3mo desaparece tanto la tabla como los ficheros en HDFS. Es el propio Hive quien gestiona los datos a todos los niveles.</p> <pre><code>drop table empleados;\n</code></pre>"},{"location":"UD04/2.hive-tablas/#externas","title":"Externas","text":"<p>Vamos a subir el fichero de empleados a trav\u00e9s de hdfs.</p> <pre><code>hdfs dfs -mkdir /prueba\nhdfs dfs -put empleados.txt /prueba\n</code></pre> <p>Creamos la tabla como external y especificando la localizaci\u00f3n. Visualizaremos como crea autom\u00e1ticamente el directorio en el sistema hdfs. Luego cargamos la informaci\u00f3n del fichero.</p> <pre><code>create external table empleados\n(\nnombre string,\nedad integer\n)\nrow format delimited\nfields terminated by ','\nlocation '/user/hive/datos/empleados';\n</code></pre> <p>location: d\u00f3nde vamos a tener los datos.</p> <p>** Entrar en webdfs para ver que se ha creado un directorio llamado datos en la carpeta que hemos indicado.</p> <p>Cargamos los datos:</p> <pre><code>load data inpath '/prueba/empleados.txt' into table empleados;\n</code></pre> <p>Ahora ya podemos ver el fichero desde webdfs.</p> <p>Lo m\u00e1s importante de las tablas externas, es que cuando hacemos ahora un drop de la tabla, el fichero no desaparece en hdfs.</p> <pre><code>drop table empleados;\n</code></pre> <p>Podemos ver que a\u00fan existen el webdfs.</p>"},{"location":"UD04/3.hive-practica-empleados/","title":"Hive. Pr\u00e1ctica empleados.","text":""},{"location":"UD04/3.hive-practica-empleados/#preparar-datos","title":"Preparar datos","text":"<p>Utilizando la librer\u00eda de python  faker, crea un archivo con 1000 datos de empleados que tenga la siguiente estructura:</p> <pre><code>$ cat empleados.txt\nMichael|Montreal,Toronto|Male,30|DB:80|Product:Developer:Lead\nWill|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead\nShelley|New York|Female,27|Python:80|Test:Lead,COE:Architect\nLucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead\n</code></pre>"},{"location":"UD04/3.hive-practica-empleados/#tablas-internas","title":"Tablas internas","text":"<ol> <li>Comprobar si existe la base de datos empleados_internal</li> <li>Creamos una base de datos llamada empleados_internal</li> <li>Nos conectamos a ella para poder utilizarla</li> <li>Creamos una tabla</li> </ol> <pre><code>CREATE TABLE IF NOT EXISTS empleados_internal\n(\n    name string,\n    work_place ARRAY&lt;string&gt;,\n    sex_age STRUCT&lt;sex:string,age:int&gt;,\n    skills_score MAP&lt;string,int&gt;,\n    depart_title MAP&lt;STRING,ARRAY&lt;STRING&gt;&gt;\n)\nCOMMENT 'This is an internal table'\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '|'\nCOLLECTION ITEMS TERMINATED BY ','\nMAP KEYS TERMINATED BY ':'\n</code></pre> <ol> <li>La cargamos con los datos del fichero empleados.txt</li> <li>Hacemos un SELECT de los datos.</li> <li>Comprobar que existe el directorio warehouse de HIVE, dentro de la bd empleados.</li> </ol>"},{"location":"UD04/3.hive-practica-empleados/#tablas-externas","title":"Tablas externas","text":"<ol> <li>Creamos una nueva tabla, empleados_external</li> <li>Lo cargamos con los mismos datos.</li> <li>Hacemos un SELECT de los datos</li> <li>Hacer una SELECT para buscar a la empleada X (un nombre de empleada que exista)</li> <li>Borrar las dos tablas</li> <li>Comprobar que ha borrado la interna pero los datos de la externa permanecen.</li> </ol>"},{"location":"UD04/4.hive-deslizamientos/","title":"Hive. Pr\u00e1ctica, deslizamientos de la tierra.","text":"<p>Vamos a realizar unas cuantas SELECT contra una DataSet de la NASA, que contiene informaci\u00f3n sobre deslizamientos de tierra ocurridos alrededor del mundo. Al finalizar generaremos una tabla (google sheets o excel) en base a la consulta de datos.</p> <p>https://github.com/josepgarcia/datos</p> <p>El fichero <code>deslizamientos.csv</code> tiene los datos</p> <p>Estructura de la tabla en HIVE.</p> <pre><code>create table deslizamientos\n(\nid bigint,\nfecha string,\nhora string,\ncountry string,\nnearest_places string,\nhazard_type string,\nlandslide_type string,\nmotivo string,\nstorm_name string,\nfatalities bigint,\ninjuries string,\nsource_name string,\nsource_link string,\nlocation_description string,\nlocation_accuracy string,\nlandslide_size string,\nphotos_link string,\ncat_src string,\ncat_id bigint,\ncountryname string,\nnear string,\ndistance double,\nadminname1 string,\nadminname2 string,\npopulation bigint,\ncountrycode string,\ncontinentcode string,\nkey string,\nversion string,\ntstamp string,\nchangeset_id string,\nlatitude double,\nlongitude double,\ngeolocation string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ';';\n</code></pre> <ol> <li>Carga los datos del fichero <code>deslizamientos.csv</code></li> <li>\u201cDescribimos\u201d la tabla para entender su contenido</li> <li>Contamos cuantos deslizamientos se han insertado (9563)</li> <li>Muestra el nombre y fecha de las 5 primeras filas.</li> <li>Averiguar el pa\u00eds, el tipo de deslizamiento y el motivo de aquellos sitios donde haya habido m\u00e1s de 100 v\u00edctimas.</li> <li>Averiguar los deslizamientos ocurridos por tipos de deslizamiento (landslide_type)</li> </ol> <ol> <li>Crear tabla para importar los pa\u00edses <code>countries.csv</code></li> <li>Cargar la tabla.</li> <li>Mostrar los 10 primeros pa\u00edses.</li> <li>Cu\u00e1les son los pa\u00edses 10 primeros pa\u00edses que tienen m\u00e1s movimientos registrados.</li> <li>Cantidad de deslizamientos y el motivo por Pa\u00edses (nombre y c\u00f3digo)</li> <li>Exportamos la select a un fichero para importarlo con alguna herramienta (google sheets) y hacer un gr\u00e1fico con el resultado.</li> </ol> <pre><code>insert overwrite local directory '/tmp/datos' row format delimited fields\nterminated by ',' select a.cod,b.country,b.motivo,count(*) from paises a\njoin deslizamientos b on a.nombre=b.country group by a.cod,b.country,b.motivo;\n</code></pre> <p>Resultado final: </p>"},{"location":"UD04/6.sqoop/","title":"Sqoop","text":"<ul> <li>Permite transferir grandes volumenes de datos de manera eficiente entre Hadoop y gestores de datos estructurados, como Bases de datos relacionales</li> <li>Ofrece conectores para integrar Hadoop con otros sistemas, como por ejemplo Oracle o SqlServer</li> </ul> <p>http://sqoop.apache.org/</p> <p></p> <p>https://www.udemy.com/course/monta-un-cluster-hadoop-big-data-desde-cero/learn/lecture/9075166#overview</p> <p>https://aitor-medrano.github.io/iabd/hadoop/sqoop.html</p> <p>http://localhost:8888/notebooks/Tema_4/Tutor%C3%ADa_BDA_04_Sqoop_Flume.ipynb</p>"},{"location":"UD04/6.sqoop/#instalacion-sqoop","title":"Instalaci\u00f3n sqoop","text":"<p>https://www.tutorialspoint.com/sqoop/sqoop_installation.htm</p> <pre><code>wget http://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz\n\ntar -xf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz\n\nmv sqoop-1.4.7.bin__hadoop-2.6.0 /usr/local/sqoop \n</code></pre> <p>A\u00f1adir en .bashrc <pre><code>export SQOOP_HOME=/usr/lib/sqoop \nexport PATH=$PATH:$SQOOP_HOME/bin\n</code></pre></p> <p>Configurar: <pre><code>cd $SQOOP_HOME/conf\n\nmv sqoop-env-template.sh sqoop-env.sh\n\n## A\u00f1\u00e1dir al archivo sqoop-env.sh\nexport HADOOP_COMMON_HOME=/usr/local/hadoop \nexport HADOOP_MAPRED_HOME=/usr/local/hadoop\n</code></pre></p>"},{"location":"UD04/6.sqoop/#verificar-la-instalacion-de-sqoop","title":"Verificar la instalaci\u00f3n de sqoop","text":"<pre><code>cd $SQOOP_HOME/bin\n\nsqoop-version\n\nWarning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail.\nPlease set $HBASE_HOME to the root of your HBase installation.\nWarning: /usr/local/sqoop/../hcatalog does not exist! HCatalog jobs will fail.\nPlease set $HCAT_HOME to the root of your HCatalog installation.\nWarning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail.\nPlease set $ACCUMULO_HOME to the root of your Accumulo installation.\nWarning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail.\nPlease set $ZOOKEEPER_HOME to the root of your Zookeeper installation.\n2024-12-03 18:54:23,587 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7\nSqoop 1.4.7\ngit commit id 2328971411f57f0cb683dfb79d19d4d19d185dd8\nCompiled by maugli on Thu Dec 21 15:59:58 STD 2017\n</code></pre> <p>Es normal que aparezcan warnings, no tenemos esas herramientas que se indican.</p>"},{"location":"UD04/6.sqoop/#instalacion-mysql","title":"Instalaci\u00f3n mysql","text":"<p>Instala mysql en el contenedor</p>"},{"location":"UD04/6.sqoop/#actualizacion-imagen-del-cluster","title":"Actualizaci\u00f3n imagen del cluster","text":"<p>Actualiza la imagen de docker con las l\u00edneas necesarias para que despu\u00e9s de instalar hadoop + yarn a\u00f1ada a la instalaci\u00f3n sqoop + mysql</p>"},{"location":"UD04/6.sqoop/#comandos-sqoop","title":"Comandos sqoop","text":"<pre><code>$ ls -la bin/\ntotal 88\ndrwxr-xr-x 1 hadoop hadoop  586 Dec 18  2017 .\ndrwxr-xr-x 1 hadoop hadoop  336 Dec 18  2017 ..\n-rwxr-xr-x 1 hadoop hadoop 6770 Dec 18  2017 configure-sqoop\n-rwxr-xr-x 1 hadoop hadoop 6533 Dec 18  2017 configure-sqoop.cmd\n-rwxr-xr-x 1 hadoop hadoop 3133 Dec 18  2017 sqoop\n-rwxr-xr-x 1 hadoop hadoop 3060 Dec 18  2017 sqoop-codegen\n-rwxr-xr-x 1 hadoop hadoop 3070 Dec 18  2017 sqoop-create-hive-table\n-rwxr-xr-x 1 hadoop hadoop 3057 Dec 18  2017 sqoop-eval\n-rwxr-xr-x 1 hadoop hadoop 3059 Dec 18  2017 sqoop-export\n-rwxr-xr-x 1 hadoop hadoop 3057 Dec 18  2017 sqoop-help\n-rwxr-xr-x 1 hadoop hadoop 3059 Dec 18  2017 sqoop-import\n-rwxr-xr-x 1 hadoop hadoop 3070 Dec 18  2017 sqoop-import-all-tables\n-rwxr-xr-x 1 hadoop hadoop 3069 Dec 18  2017 sqoop-import-mainframe\n-rwxr-xr-x 1 hadoop hadoop 3056 Dec 18  2017 sqoop-job\n-rwxr-xr-x 1 hadoop hadoop 3067 Dec 18  2017 sqoop-list-databases\n-rwxr-xr-x 1 hadoop hadoop 3064 Dec 18  2017 sqoop-list-tables\n-rwxr-xr-x 1 hadoop hadoop 3058 Dec 18  2017 sqoop-merge\n-rwxr-xr-x 1 hadoop hadoop 3062 Dec 18  2017 sqoop-metastore\n-rwxr-xr-x 1 hadoop hadoop 3060 Dec 18  2017 sqoop-version\n-rwxr-xr-x 1 hadoop hadoop 1055 Dec 18  2017 sqoop.cmd\n-rwxr-xr-x 1 hadoop hadoop 3987 Dec 18  2017 start-metastore.sh\n-rwxr-xr-x 1 hadoop hadoop 1564 Dec 18  2017 stop-metastore.sh\n</code></pre> <p>Se pueden ver las opciones disponibles tambi\u00e9n con: <code>sqoop help</code></p>"},{"location":"UD04/6.sqoop/#sqoop-import","title":"sqoop-import","text":"<p>Importar movie data a mysql  https://www.udemy.com/course/the-ultimate-hands-on-hadoop-tame-your-big-data/learn/lecture/5963236#learning-tools</p> <p>Pasar de mysql a hdfs https://www.udemy.com/course/the-ultimate-hands-on-hadoop-tame-your-big-data/learn/lecture/5963266#learning-tools</p>"},{"location":"UD04/6.sqoop/#sqoop-export","title":"sqoop-export","text":"<p>Exportar de hadoop a mysql https://www.udemy.com/course/the-ultimate-hands-on-hadoop-tame-your-big-data/learn/lecture/5963272#learning-tools</p>"},{"location":"UD04/_TODO/","title":"TODO","text":""},{"location":"UD04/_TODO/#no-incluit","title":"No incluit","text":"<p>https://www.udemy.com/course/apache-hive-for-data-engineers-hands-on/?couponCode=NEWYEARCAREER</p>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/","title":"UD04 - HIVE","text":""},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/#practica","title":"Pr\u00e1ctica","text":"<p>UD04 3. Tipos de tablas.</p> <p>UD04 4. Pr\u00e1ctica tablas.</p> <p>UD04 4. Pr\u00e1ctica tablas. (SOL)</p> <p>[[./UD04 - HIVE 15ae913de6c4808faa08c7bc62c140fe/UD04 5 [[FALTA]] Conexiones remotas 15ae913de6c4812a8d5dd0c525de9ef4|UD04 5. FALTA Conexiones remotas.]]</p> <p>UD04 6. Pr\u00e1ctica, deslizamientos de la tierra.</p> <p>UD04 6. Pr\u00e1ctica, deslizamientos de la tierra. [SOL]</p> <p>UD04 7. Pr\u00e1ctica MovieLens</p> <p>UD04 7. Pr\u00e1ctica MovieLens (SOL)</p> <p>UD04 4. Importaciones. (1)</p> <p>UD04 8. Integraci\u00f3n MySql &amp; Hadoop</p> <p>UD04 X. XXXXXX</p> <p>https://aitor-medrano.github.io/iabd2223/hadoop/06hive.html</p>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/#revisar","title":"Revisar","text":"<ul> <li>[ ]  https://vimeo.com/632915867</li> <li>[ ]  https://vimeo.com/632917555</li> <li>[ ]  https://vimeo.com/632919473</li> <li>[ ]  PASS:  UCLMIABD-2022</li> </ul>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%204%20Importaciones%20%281%29%2015ae913de6c481659b2cf5f1cb099b08/","title":"UD04 4. Importaciones. (1)","text":""},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%204%20Pra%CC%81ctica%20tablas%20%28SOL%29%2015ae913de6c48185af5fe5b314cb0365/","title":"UD04 4. Pr\u00e1ctica tablas. (SOL)","text":"<p>20-Practicas+BigData_HIVE_tablas_externas_e_internas.pdf</p>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%204%20Pra%CC%81ctica%20tablas%20%28SOL%29%2015ae913de6c48185af5fe5b314cb0365/#external","title":"EXTERNAL","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS empleados_external\n(\n        name string,\n        work_place ARRAY&lt;string&gt;,\n        sex_age STRUCT&lt;sex:string,age:int&gt;,\n        skills_score MAP&lt;string,int&gt;,\n        depart_title MAP&lt;STRING,ARRAY&lt;STRING&gt;&gt;\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '|'\nCOLLECTION ITEMS TERMINATED BY ','\nMAP KEYS TERMINATED BY ':'\nLOCATION '/ejemplo/empleados';\n\n// CARGAMOS LOS DATOS (local)\nLOAD DATA LOCAL INPATH '/home/curso/Desktop/empleados.txt'\nOVERWRITE INTO TABLE empleados_external;\n</code></pre>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%205%20%5B%5BFALTA%5D%5D%20Conexiones%20remotas%2015ae913de6c4812a8d5dd0c525de9ef4/","title":"UD04 5. [[FALTA]] Conexiones remotas.","text":"<p>Necesitamos configurar para evitar complicaciones en el fichero \u201chive-site.xml\u201d un par\u00e1metro para que nos deje hacer login la herramienta beeline por el usuario por defecto. Modificamos l par\u00e1metro doAs a \u201cfalse\u201d.</p> <p>![[./UD04 5 [[FALTA]] Conexiones remotas 15ae913de6c4812a8d5dd0c525de9ef4/CleanShot_2023-12-01_at_09.40.142x.png|CleanShot 2023-12-01 at 09.40.14@2x.png]]</p> <ul> <li>hive.server2.enable.doAs \u2192 false, nos conectamos con el mismo usuario que ha ejecutado el hive server (hadoop)</li> </ul> <p>Arrancamos el proceso dentro del directorio bbdd que hemos creado anteriormente y d\u00f3nde se ubican los metadatos de Hive. Luego arrancamos la herramienta beeline de Hive. Y conectamos v\u00eda jdbc. Cuando pida usuario/contrase\u00f1a si no ponemos nada usar\u00e1 el mismo en el cual estamos conectados.</p> <pre><code>$ pwd\n/opt/hadoop/hive/bbdd\n\n$ hiveserver2 &amp; ## El &amp; lo ejecuta en segundo plano\n\n$ ps -aux | grep hives ## \u00bf\u00bf El proceso ha inciciado?\n\n$ tail -f /tmp/hadoop/hive.log ## Posibles errores\n\n$ netstat -an | grep 10000 ## Comprobar puertos abiertos\n\n## Nos conectamos, opci\u00f3n 1\n$ beeline -u jdbc:hive2://NOMBREMAQUINA:10000\n\n## Nos conectamos, opci\u00f3n 2\n$ beeline\n!connect jdbc:hive2://NOMBREMAQUINA:10000\n\n# Una vez en la consola beeline\n\n$ help ## Muestra los diferentes comandos\n</code></pre> <ul> <li> <p>Errores</p> <p>Error1: ERROR XSDB6: Another instance of Derby may have already booted the database</p> <p>mv db* /tmp/</p> </li> </ul> <p>En otro terminal, nos situamos en \u201c/opt/hadoop\u201d y copiamos la carpeta de Hive al nodo que queremos usar para conectarnos. Por ejemplo, en nodo2.</p> <pre><code>scp \u2013r hive nodo2:/opt/hadoop\n</code></pre> <p>Nos conectamos en otra terminal al nodo 2 y nos situamos en la carpeta de hadoop/bin y ejecutamos beeline. Nos conectamos a hive al nodo1.</p> <pre><code>ssh nodo2\ncd opt/hadoop/hive/bin\n./beeline\n!connect jdbc:hive2://nodo1:10000\nshow databases;\nshow tables;\n...\n</code></pre>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%206%20Pra%CC%81ctica%2C%20deslizamientos%20de%20la%20tierra%20%5BSOL%2015ae913de6c481c594c6ed41cd2f1436/","title":"UD04 6. Pr\u00e1ctica, deslizamientos de la tierra. [SOL]","text":"<p>1</p> <pre><code>load data local inpath '/tmp/deslizamientos.csv' into table deslizamientos;\n</code></pre> <p>2</p> <pre><code>desc deslizamientos;\n</code></pre> <p>3</p> <pre><code>select count(*) from deslizamientos;\n</code></pre> <p>4</p> <pre><code>\u00bf\u00bf\u00bf SELECT country,fecha FROM deslizamientos LIMIT 5;\n</code></pre> <p>5</p> <pre><code>select country,fecha, landslide_type,motivo,fatalities from\n        deslizamientos where fatalities &gt; 100;\n</code></pre> <p>6</p> <pre><code>select landslide_type, count(*) from deslizamientos group by\n    landslide_type;\n</code></pre> <p>7</p> <pre><code>create table paises\n(\nnombre string,\ncod string)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY \u2018,\u2019;\n</code></pre> <p>8</p> <pre><code>load data local inpath '/tmp/countries.csv' into table paises;\n</code></pre> <p>9</p> <pre><code>select * from paises limit 10;\n</code></pre> <p>10</p> <pre><code>select country,count(*) as total from deslizamientos group by country\norder by total desc limit 10;\n</code></pre> <p>11</p> <pre><code>select a.cod,b.country,b.motivo,count(*) from paises a join deslizamientos b\non a.nombre=b.country group by a.cod,b.country,b.motivo;\n</code></pre> <p>12</p> <pre><code>insert overwrite local directory '/tmp/datos' row format delimited fields\nterminated by ',' select a.cod,b.country,b.motivo,count(*) from paises a\njoin deslizamientos b on a.nombre=b.country group by a.cod,b.country,b.motivo;\n</code></pre> <p>Consultas resueltas:</p> <pre><code>--  Cantidad de deslizamientos y el motivo por Pa\u00edses (nombre y c\u00f3digo)\n\nselect a.cod,b.country,b.motivo,count(*) from paises a join deslizamientos b\non a.nombre=b.country group by a.cod,b.country,b.motivo;\n--  Guardamos el resultado de esta consulta en un fichero para tratarlo con herramientas externas (p.e. Excel, PowerBI\u2026).\n\n--escritura a fichero del resultado de una sql\n\ninsert overwrite local directory '/tmp/datos' row format delimited fields\nterminated by ',' select a.cod,b.country,b.motivo,count(*) from paises a\njoin deslizamientos b on a.nombre=b.country group by a.cod,b.country,b.motivo;\n</code></pre>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%207%20Pra%CC%81ctica%20MovieLens%20%28SOL%29%2015ae913de6c481f9924edd4871b282a0/","title":"UD04 7. Pr\u00e1ctica MovieLens (SOL)","text":"\u26a0\ufe0f **Entregar en aules** Descargar archivo MovieLens"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%207%20Pra%CC%81ctica%20MovieLens%20%28SOL%29%2015ae913de6c481f9924edd4871b282a0/#archivos","title":"Archivos","text":"<p>Al descomprimirlo en el archivo README encontraremos informaci\u00f3n de cada uno de los diferentes archivos (tablas), como por ejemplo:</p> <p><code>u.data</code></p> <p>-- The full u data set, 100000 ratings by 943 users on 1682 items. Each user has rated at least 20 movies.  Users and items are numbered consecutively from 1.  The data is randomly ordered. This is a tab separated list of user id | item id | rating | timestamp (The time stamps are unix seconds since 1/1/1970 UTC)</p> <p><code>u.item</code></p> <p>-- Information about the items (movies); this is a tab separated list of</p> <p>movie id | movie title | release date | video release date | IMDb URL | unknown | Action | Adventure | Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western |</p> <p>The last 19 fields are the genres, a 1 indicates the movie is of that genre, a 0 indicates it is not; movies can be in several genres at once.</p> <p>The movie ids are the ones used in the u.data data set.</p>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%207%20Pra%CC%81ctica%20MovieLens%20%28SOL%29%2015ae913de6c481f9924edd4871b282a0/#ejercicio-para-entregar","title":"Ejercicio para entregar","text":"<p>** Para la pr\u00e1ctica he cortado el fichero u.item y lo he dejado en 2 columnas (las 2 primeras), lo he llamado <code>u.item.mini</code></p> <ol> <li>En la primera tabla, u.data vemos un campo llamado \u201ctimestamp\u201d, \u00bfQu\u00e9 significa? \u00bfPara qu\u00e9 sirve? </li> <li>\u00bfQu\u00e9 pasar\u00e1 el 18 de Enero de 2038?</li> <li>Importar las dos tablas en HDFS.</li> </ol> <pre><code>cd /opt/hadoop/hive/bbdd\n\n* Subir ficheros a DHFS\n\n--- CREATE TABLE\n/** Si llamamos al campo tiemestamp da error **/\nCREATE DATABASE movies;\nUSE movies;\nCREATE TABLE u_data\n(\n    user_id int,\n    item_id int,\n    rating int,\n    tim int \n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t';\n\nload data inpath '/tmp/u.data' into table u_data; \n# Los datos estan en HDFS /tmp/u.data\n## load data local inpath '/tmp/u.data' into table u_data;\n\nCREATE TABLE u_item\n(\n    movie_id int,\n    movie_title string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '|';\nload data inpath '/tmp/u.item.mini' into table u_item;\n\nselect count(*) from u_item;\n</code></pre> <ol> <li>Mostra el nombre de la pel\u00edcula y el total de valoraciones que ha tenido (ordenar por total de valoraciones descendente), limitar a 10 resultados.</li> </ol> <pre><code>SELECT u_item.movie_title, COUNT(u_data.rating) AS total_ratings\nFROM u_item\nLEFT JOIN u_data ON u_item.movie_id = u_data.item_id\nGROUP BY u_item.movie_title\nORDER BY total_ratings DESC\nLIMIT 10;\n</code></pre> <ol> <li>Encuentra las pel\u00edculas que han sido clasificadas con un 5 y que tengan m\u00e1s de 100 valoraciones.</li> </ol> <pre><code>SELECT u_item.movie_title\nFROM u_item\nJOIN u_data ON u_item.movie_id = u_data.item_id\nWHERE u_data.rating = 5\nGROUP BY u_item.movie_title\nHAVING COUNT(u_data.rating) &gt; 100;\n</code></pre> <ol> <li>Encuentra el usuario que ha dado el mayor n\u00famero de valoraciones</li> </ol> <pre><code>SELECT u_data.user_id, COUNT(u_data.rating) AS total_ratings\nFROM u_data\nGROUP BY u_data.user_id\nORDER BY total_ratings DESC\nLIMIT 1;\n</code></pre> <ol> <li>Obtener las 10 pel\u00edculas con mejor valoraci\u00f3n</li> </ol> <pre><code>SELECT u_item.movie_id, u_item.movie_title, AVG(u_data.rating) AS average_rating\nFROM u_data\nJOIN u_item ON u_data.item_id = u_item.movie_id\nGROUP BY u_item.movie_id, u_item.movie_title\nORDER BY average_rating DESC\nLIMIT 10;\n</code></pre> <ol> <li>Obtener las 10 pel\u00edculas que m\u00e1s valoraciones de 1 han recibido.</li> </ol> <pre><code>SELECT u_item.movie_id, u_item.movie_title, COUNT(u_data.rating) AS count_1_ratings\nFROM u_data\nJOIN u_item ON u_data.item_id = u_item.movie_id\nWHERE u_data.rating = 1\nGROUP BY u_item.movie_id, u_item.movie_title\nORDER BY count_1_ratings DESC\nLIMIT 10;\n</code></pre>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%207%20Pra%CC%81ctica%20MovieLens%2015ae913de6c481398b8fe9304b2a0dd4/","title":"UD04 7. Pr\u00e1ctica MovieLens","text":"<p>\u26a0\ufe0f Entregar en aules Descargar archivo MovieLens</p>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%207%20Pra%CC%81ctica%20MovieLens%2015ae913de6c481398b8fe9304b2a0dd4/#archivos","title":"Archivos","text":"<p>Al descomprimirlo en el archivo README encontraremos informaci\u00f3n de cada uno de los diferentes archivos (tablas), como por ejemplo:</p> <p><code>u.data</code></p> <p>-- The full u data set, 100000 ratings by 943 users on 1682 items. Each user has rated at least 20 movies.  Users and items are numbered consecutively from 1.  The data is randomly ordered. This is a tab separated list of user id | item id | rating | timestamp (The time stamps are unix seconds since 1/1/1970 UTC)</p> <p><code>u.item</code></p> <p>-- Information about the items (movies); this is a tab separated list of</p> <p>movie id | movie title | release date | video release date | IMDb URL | unknown | Action | Adventure | Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western |</p> <p>The last 19 fields are the genres, a 1 indicates the movie is of that genre, a 0 indicates it is not; movies can be in several genres at once.</p> <p>The movie ids are the ones used in the u.data data set.</p>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%207%20Pra%CC%81ctica%20MovieLens%2015ae913de6c481398b8fe9304b2a0dd4/#ejercicio-para-entregar","title":"Ejercicio para entregar","text":"<ol> <li>En la primera tabla, u.data vemos un campo llamado \u201ctimestamp\u201d, \u00bfQu\u00e9 significa? \u00bfPara qu\u00e9 sirve? </li> <li>\u00bfQu\u00e9 pasar\u00e1 el 18 de Enero de 2038?</li> <li>Importar las dos tablas en HDFS.</li> <li>Mostrar el nombre de la pel\u00edcula y el total de valoraciones que ha tenido (ordenar por total de valoraciones descendente), limitar a 10 resultados.</li> <li>Encuentra las pel\u00edculas que han sido clasificadas con un 5 y que tengan m\u00e1s de 100 valoraciones.</li> <li>Encuentra el usuario que ha dado el mayor n\u00famero de valoraciones</li> <li>Obtener las 10 pel\u00edculas con mejor valoraci\u00f3n</li> <li>Obtener las 10 pel\u00edculas que m\u00e1s valoraciones de 1 han recibido.</li> </ol>"},{"location":"UD04/_PENDENT/UD04%20-%20HIVE%2015ae913de6c4808faa08c7bc62c140fe/UD04%208%20Integracio%CC%81n%20MySql%20%26%20Hadoop%2015ae913de6c481dc8092e14141e916f2/","title":"UD04 8. Integraci\u00f3n MySql &amp; Hadoop","text":""},{"location":"UD05-Spark/1.instalacion/","title":"Instalaci\u00f3n y configuraci\u00f3n inicial.","text":"<p>Presentaci\u00f3n</p> <p>https://keepcoding.io/blog/usar-spark-con-scala/</p>"},{"location":"UD05-Spark/1.instalacion/#que-es-spark","title":"\u00bfQu\u00e9 es Spark?","text":"<p>Spark es una plataforma open source muy usada en la industria para el procesamiento de grandes vol\u00famenes de datos y ejecuci\u00f3n de c\u00f3mputo intensivo sobre estos. Un framework que ofrece gran valor transformando y analizando datos relevantes que ayudan a grandes compa\u00f1\u00edas a tomar mejores decisiones de negocio.</p> <p>Esta plataforma contiene m\u00f3dulos y librer\u00edas para trabajar mejor con los datos que se quieren procesar. Adem\u00e1s, Spark es multilenguaje, por lo que se puede programar tanto en Scala, Phyton, Java o R.</p>"},{"location":"UD05-Spark/1.instalacion/#por-que-programar-en-spark-con-scala","title":"\u00bfPor qu\u00e9 programar en Spark con Scala?","text":"<p>Se trata de un lenguaje de programaci\u00f3n con recorrido ya que tiene dos d\u00e9cadas en el mercado. Adem\u00e1s, Scalable language (Scala), es un lenguaje h\u00edbrido entre programaci\u00f3n orientada a objetos y programaci\u00f3n funcional. Por lo que, al tener las ventajas de uno y otro, es un lenguaje bastante funcional y pr\u00e1ctico. De ah\u00ed que sea bueno usar Spark con Scala.</p> <p>Tiene menos c\u00f3digo para realizar algunas funciones en comparaci\u00f3n con otros lenguajes. Esto es de utilidad debido a que se puede reducir el c\u00f3digo a la m\u00ednima expresi\u00f3n y as\u00ed leerlo m\u00e1s r\u00e1pido para corregir posibles problemas.</p> <p>Adem\u00e1s, es compatible con la m\u00e1quina virtual de Java, esto significa que podr\u00e1s reusar librer\u00edas de Java en tus aplicaciones Scala, tendr\u00e1s compatibilidad con el c\u00f3digo en Java y te podr\u00e1s beneficiar de una comunidad consolidada en el panorama de la programaci\u00f3n de Spark con Scala.</p>"},{"location":"UD05-Spark/1.instalacion/#rdd-resilient-distributed-dataset","title":"RDD - Resilient Distributed Dataset","text":"<p>La unidad b\u00e1sica de datos que se utiliza en este ecosistema (stack) se llama RDD (Resilient Distributed Dataset). Se trata de una lista de datos, pero que no est\u00e1 toda junta, sino que est\u00e1 en diferentes m\u00e1quinas (seg\u00fan el cl\u00faster).</p> <p>Esta colecci\u00f3n de datos es inmutable (no se puede cambiar), aunque s\u00ed que se pueden aplicar transformaciones u operar con ellos mediante actions.</p> <p>En cuanto al ciclo de vida del RDD, de una unidad b\u00e1sica se realizan transformaciones de los conjuntos de datos (seg\u00fan los objetivos previstos para cada caso), se aplica una acci\u00f3n y al final obtenemos un resultado. Es decir, si tenemos varios petabytes con informaci\u00f3n de vuelos comerciales en los \u00faltimos a\u00f1os, realizaremos transformaciones para quedarnos con los datos que nos interesan y podr\u00edamos aplicar una acci\u00f3n para sumarle un n\u00famero espec\u00edfico a los RDD (con el objetivo final de facilitarnos un sumatorio de vuelos realizados, por ejemplo).</p> <p>Ahora llevemos toda esta teor\u00eda a la pr\u00e1ctica y, para eso, aqu\u00ed te dejamos un ejercicio hecho por Marco Doncel, en donde descubrir\u00e1s c\u00f3mo funciona el c\u00f3digo de Spark con Scala.</p>"},{"location":"UD05-Spark/1.instalacion/#instalacion-y-configuracion","title":"Instalaci\u00f3n y configuraci\u00f3n","text":"<ol> <li> <p>Descargar en la m\u00e1quina virtual (en /usr/local/), usamos el usuario hadoop https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-without-hadoop.tgz</p> </li> <li> <p>Descomprimir</p> </li> <li>Crear enlace a carpeta llamado \u201cspark\u201d</li> <li>Modificar .bashrc</li> </ol> <pre><code>## spark\nexport PATH=$PATH:/usr/local/spark/bin:/usr/local/spark/sbin\nexport SPARK_DIST_CLASSPATH=$(hadoop classpath)\n</code></pre>"},{"location":"UD05-Spark/1.instalacion/#ejemplo-con-scala","title":"Ejemplo con scala","text":"<p>Ejecutamos el comando: <code>spark-shell</code></p> <p>Al ejecutar spark-shell se crea una sesi\u00f3n o contexto, tal y como indica</p> <pre><code>Spark context Web UI available at http://192.168.65.4:4040\nSpark context available as 'sc' (master = local[*], app id = local-1705859488688).\nSpark session available as 'spark'.\n</code></pre> <p>Para realizar cualquier instancia a Spark a continuaci\u00f3n, deberemos proceder a trav\u00e9s del contexto (<code>sc</code>).</p> <p>Podemos ver los comandos disponibles con:</p> <pre><code>scala&gt; :help\n</code></pre>"},{"location":"UD05-Spark/1.instalacion/#ejercicio","title":"Ejercicio","text":"<p>Leer un fichero ubicado dentro del directorio spark descomprimido y contar palabras dentro (por defecto, si no ponemos ubicaci\u00f3n, buscar\u00e1 en el sistema HDFS de Hadoop). Recordar instanciar a trav\u00e9s del contexto \"sc\".</p> <pre><code>scala&gt; val fichero=sc.textFile(\"file:///usr/local/spark/README.md\")\n</code></pre> <p>Si instanciamos \u201cfichero\u201d como vemos anteriormente, nos devuelve un objeto RDD tipo String y que apunta al fichero indicado.</p> <pre><code>fichero: org.apache.spark.rdd.RDD[String] = file:///opt/hadoop/spark/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:23\n</code></pre> <p>Podemos hacer un \u201ccount\u201d para saber cu\u00e1ntas l\u00edneas tiene el fichero cargado en el RDD.</p> <pre><code>fichero.count()\n</code></pre> <p></p> <p>Resultado</p> <pre><code>res0: Long = 124\n</code></pre> <p>Salimos de la shell con</p> <pre><code>:q\n</code></pre>"},{"location":"UD05-Spark/1.instalacion/#ejemplo-con-python","title":"Ejemplo con python","text":"<ol> <li>Abrimos una shell de <code>pyspark</code></li> </ol> <pre><code>$ pyspark\n</code></pre> <ol> <li>Ejecutamos el siguiente c\u00f3digo</li> </ol> <pre><code>fichero = spark.read.text(\"file:///usr/local/spark/README.md\")\nfichero\nfichero.count()\n</code></pre> <p>Si da error, es que necesitamos instalar python3</p>"},{"location":"UD05-Spark/2.scala-spark-hadoop/","title":"Scala - Spark - Hadoop.","text":""},{"location":"UD05-Spark/2.scala-spark-hadoop/#scala","title":"Scala","text":"<p>https://scala-lang.org/ Scala es un lenguaje de programaci\u00f3n moderno multi-paradigma dise\u00f1ado para expresar patrones de programaci\u00f3n comunes de una forma concisa, elegante, y con tipado seguro. Integra f\u00e1cilmente caracter\u00edsticas de lenguajes orientados a objetos y funcionales.</p>"},{"location":"UD05-Spark/2.scala-spark-hadoop/#spark-shell-hdfs","title":"Spark shell HDFS","text":"<p>Archivo utilizado: <code>puertos.csv</code> <pre><code>https://github.com/josepgarcia/datos/blob/main/puertos.csv\n</code></pre></p> <p>Creamos un directorio en el sistema HDFS para guardar los ficheros de datos. Y subimos al directorio creado en HDFS \u201cspark\u201d el fichero facilitado en la pr\u00e1ctica \"puertos.csv\" (datos de cargas de puertos mar\u00edtimos) o lo creamos en la m\u00e1quina y copiamos su contenido y guardamos.</p> <pre><code>hdfs dfs -mkdir /spark\nhdfs dfs -put puerto.csv /spark\n</code></pre> <p>Creamos una sesi\u00f3n con \"spark-shell\" para conectar en el entorno Spark con la terminal Scala.</p> <pre><code>spark-shell\n</code></pre> <p>Vamos acceder a este fichero del sistema HDFS con Scala. Ya lo identifica a trav\u00e9s de las variables ya definidas.</p> <pre><code>val vl = sc.textFile(\"/spark/puertos.csv\")\nvl.count()\n</code></pre> <p>Info</p> <p>El comando que utilizamos en el punto anterior, para cargar un fichero desde fuera de hadoop era: <code>scala&gt; val fichero=sc.textFile(\"file:///opt/hadoop/spark/README.md\")</code></p> <p>Vamos a hacer una b\u00fasqueda sobre el RDD buscando los puertos de Barcelona y guardarlo en otro RDD.</p> <pre><code>val v2 = vl.filter(line=&gt;line.contains(\"Barcelona\"))\nv2.count()\n</code></pre>"},{"location":"UD05-Spark/2.scala-spark-hadoop/#spark-scala-hadoop","title":"Spark scala \u2192 Hadoop","text":"<p>Ahora vamos a lanzar una aplicaci\u00f3n SPARK con Scala contra YARN. El argumento m\u00e1s importante es el \u201c\u2014master yarn\u201d que indica que vamos a ejecutarlo sobre Hadoop, no contra local. Nota: Ejecutar desde fuera del interprete de Scala/Python.</p> <p>spark-submit: Para lanzar un comando contra un claster hadoop <pre><code>-class \u2192 Clase a utilizar\n-master yarn \u2192 Ejecutamos el comando contra nuestro cluster hadoop\n-deploy-mode cluster \u2192 Para que se ejecute en modo cluster, en todos los nodos\n-name \u2192 para poder localizarla\n.jar \u2192 fichero de ejemplos (el nuestro calcula el n\u00famero PI)\n</code></pre></p> <pre><code>:quit ## Salimos de spark-shell\n</code></pre> <pre><code>spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster --name \"apli1\" /opt/hadoop/spark/examples/jars/spark-examples_2.12-3.3.0.jar 5\n</code></pre> <p>C\u00f3digo de SparkPi </p> <p>Al lanzarlo se conecta al cluster, sube el fichero para trabajar con \u00e9l\u2026</p> <p>Pasa a la cola \u2192 Se acepta ACCEPTED \u2192 RUNNING \u2192 FINISHED</p> <p>Una vez finalizado el proceso, vamos a la web de monitorizaci\u00f3n y buscamos en el apartado de aplicaciones finalizadas con \u00e9xito: http://IP_MAQUINA_HADOOP:8088/</p> <p>Buscamos el apartado de los logs que ha generado la ejecuci\u00f3n de la aplicaci\u00f3n \u201capli1\u201d. El fichero stdout tendremos la salida con el resultado de la ejecuci\u00f3n. </p> <p>** La aplicaci\u00f3n se ha lanzado sobre el cluster, lo que significa que ha utilizado todos sus recursos disponibles (nodos, memoria, disco\u2026)</p>"},{"location":"UD05-Spark/2.scala-spark-hadoop/#spark-python-hadoop","title":"Spark python \u2192 Hadoop","text":"<p>Vamos a crear una aplicaci\u00f3n con python y lanzarla contra Hadoop. <code>contarPalabras.py</code></p> <p>Fichero a contar: https://github.com/josepgarcia/datos/blob/main/el_quijote.txt</p> <pre><code>import sys\ntry:\n        # Permiten poner el entorno, contexto de spark (anteriormente contexto sc sparkcontext)\n    from pyspark import SparkConf, SparkContext\n\n    conf = SparkConf() # Creamos la configuraci\u00f3n\n    sc = SparkContext(conf=conf) # Creamos el contexto\n    inputPath = sys.argv[1] # Par\u00e1metro 1 entrada, fichero\n    outputPath = sys.argv[2] # Par\u00e1metro 2 entrada, directorio salida\n\n        # Opciones para poder acceder a HDFS\n    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path \n    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n    Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n    fs = FileSystem.get(Configuration())\n\n    if(fs.exists(Path(inputPath)) == False):\n        print(\"El fichero de entrada no existe\")\n    else:\n        if(fs.exists(Path(outputPath))):\n            fs.delete(Path(outputPath), True)\n                # Divide el contenido del fichero en espacios en blanco y va contando (como wordcount de java)\n                # El resultado lo deja en outputPath\n        sc.textFile(inputPath).flatMap(lambda l: l.split(\" \")).map(lambda w: (w, 1)).reduceByKey(lambda t, e: t + e).saveAsTextFile(outputPath)\n\n    print (\"Se han importado los modulos de Spark\")\n\nexcept ImportError as e:\n    print (\"No puedo importar los modulos de Spark\", e)\n    sys.exit(1)\n</code></pre> <p>Lo lanzamos con spark-submit <pre><code># Todo el comando en la misma l\u00ednea\nspark-submit --master yarn --deploy-mode cluster --name \"ContarPalabras\"\n/home/hadoop/contarPalabras.py /tmp/el_quijote.txt\n/salida_spark_wc\n</code></pre></p> <p>ERROR</p> <p>Dar\u00e1 un error, hay que solucionarlo.</p> <p> Error de ejecuci\u00f3n, no encuentra el fichero del quijote </p>"},{"location":"UD05-Spark/2.scala-spark-hadoop/#spark-ui","title":"Spark UI","text":"<p>Cuando lanzamos <code>spark-shell</code> y creamos un context, autom\u00e1ticamente se levanta la UI de spark en el puerto 4040. </p>"},{"location":"UD05-Spark/3.pyspark/","title":"PySpark. Entornos de ejecuci\u00f3n","text":"<p>PySpark es la interfaz de Apache Spark para Python. Apache Spark es un motor de procesamiento de datos de c\u00f3digo abierto, dise\u00f1ado para procesar y analizar grandes vol\u00famenes de datos de manera distribuida y en paralelo. PySpark permite a los desarrolladores utilizar Spark con el lenguaje Python, lo que facilita la manipulaci\u00f3n de datos, el an\u00e1lisis y la implementaci\u00f3n de algoritmos de aprendizaje autom\u00e1tico a gran escala.</p> <p>PySpark vs Pandas </p>"},{"location":"UD05-Spark/3.pyspark/#docker","title":"docker","text":"<p>Fuente:</p> <p>https://towardsdatascience.com/stuck-trying-to-get-pyspark-to-work-in-your-data-science-environment-here-is-another-way-fb80a4bb7d8f</p> <p>Utilizaremos la siguiente imagen:</p> <p>https://hub.docker.com/r/jupyter/pyspark-notebook</p> <p></p> <pre><code>docker pull jupyter/pyspark-notebook\n</code></pre> <p>Arrancamos el contenedor con:</p> <pre><code># if running on Windows (cmd.exe):\ndocker run -it -p 8888:8888 -v %cd%:/home/jovyan/work/projects/ jupyter/pyspark-notebook\n\n# if running on a mac or linux computer:\ndocker run -it -p 8888:8888 \n-v `(pwd)`:/home/jovyan/work/projects/ \njupyter/pyspark-notebook\n</code></pre> <pre><code>### Arrancar contenedor desde mac (josep), \n### cambiamos puerto para que no coinicida con apache\ndocker run -it -p 7777:8888 -v /Users/josepgarcia/Webs/spark:/home/jovyan/work/projects/ jupyter/pyspark-notebook\n</code></pre> <p>Opciones del comando: <pre><code>-it: modo interactivo, nos permite ver la salida del comando. En este caso ejecuta `jupyter notebook` por lo que podremos ver el resultado de la ejecuci\u00f3n y la url para acceder al notebook.\n\n-p: \"publicaci\u00f3n\", este par\u00e1metro asigna un puerto de red del contenedor a un puerto de red en la m\u00e1quina host al puerto 8888. La sintaxis es `- {host's port}:{container's port}\n\nEsto significa que si abrimos la m\u00e1quina en local, en el puerto 8888 veremos lo que hay en el mismo puerto en el contenedor.\n\n-v: \"volumen\", sirve para mapear vol\u00famenes, en nuestro caso mapeamos el directorio actual (donde lanzamos el comando) `{pwd}` con el directorio que hay en el contenedor `{/home/jovyan/work/projects}`\n</code></pre></p> <p>Ahora contamos con un entorno PySpark completamente funcional ejecut\u00e1ndose en Jupyter.</p> <p>Podemos crear un alias en .bashrc o .zshrc para realizar estos pasos de una manera m\u00e1s sencilla.</p> <pre><code># definir una funci\u00f3n contenedora para ejecutar el comando \nrun_spark() { \n   docker run -it -p 8888:8888 -v `(pwd)`:/home/jovyan/work/projects/ jupyter/pyspark-notebook \n}\n</code></pre>"},{"location":"UD05-Spark/3.pyspark/#conexion-con-vscode","title":"Conexi\u00f3n con VsCode","text":"<ol> <li>Arrancar contenedor docker y quedarse con la URL. http://127.0.0.1:8888/lab?token=e6db942bdb0acd0479c5e6b4e838f9a4602c079e38166beb</li> <li>Abrir VSCODE en la misma carpeta donde se encuentran los archivos.</li> <li>Abrir archivo ipynb desde VSCODE y a la derecha arriba SELECCIONAR KERNEL</li> <li>Poner como server la URL del paso 1.</li> </ol>"},{"location":"UD05-Spark/4.superhero/","title":"Super Hero Marvel","text":""},{"location":"UD05-Spark/4.superhero/#1-en-busca-del-superheroe-mas-popular","title":"1. En busca del superh\u00e9roe M\u00c1S popular","text":"<p>Este ejercicio consiste en analizar datos de superh\u00e9roes de Marvel utilizando Apache Spark para determinar cu\u00e1l es el superh\u00e9roe m\u00e1s popular.  Se trabaja con dos archivos:</p> <p>https://github.com/josepgarcia/datos/tree/main/superhero</p> <pre><code>$ egrep SPIDER Marvel+Names | head -n 5\n400 \"BEACH, SPIDER\"\n603 \"BLOOD SPIDER/\"\n3413 \"MAN-SPIDER | MUTANT \"\n3414 \"MAN-SPIDER CLONE | M\"\n5306 \"SPIDER-MAN/PETER PAR\"\n</code></pre> <pre><code>$ head -n 5 Marvel+Graph\n5988 748 1722 3752 4655 5743 1872 3413 5527 6368 6085 4319 4728 1636 2397 3364 4001 1614 1819 1585 732 2660 3952 2507 3891 2070 2239 2602 612 1352 5447 4548 1596 5488 1605 5517 11 479 2554 2043 17 865 4292 6312 473 534 1479 6375 4456\n5989 4080 4264 4446 3779 2430 2297 6169 3530 3272 4282 6432 2548 4140 185 105 3878 2429 1334 4595 2767 3956 3877 4776 4946 3407 128 269 5775 5121 481 5516 4758 4053 1044 1602 3889 1535 6038 533 3986\n5982 217 595 1194 3308 2940 1815 794 1503 5197 859 5096 6039 2664 651 2244 528 284 1449 1097 1172 1092 108 3405 5204 387 4607 4545 3705 4930 1805 4712 4404 247 4754 4427 1845 536 5795 5978 533 3984 6056\n5983 1165 3836 4361 1282 716 4289 4646 6300 5084 2397 4454 1913 5861 5485\n5980 2731 3712 1587 6084 2472 2546 6313 875 859 323 2664 1469 522 2506 2919 2423 3624 5736 5046 1787 5776 3245 3840 2399\n</code></pre> <p>Marvel+Names: Contiene un identificador \u00fanico y el nombre del superh\u00e9roe.</p> <p>Marvel+Graph:  Cada fila representa un c\u00f3mic distinto. En cada fila, el primer campo indica el protagonista del c\u00f3mic, seguido de los superh\u00e9roes que tambi\u00e9n aparecen en ese c\u00f3mic. Un superh\u00e9roe puede ser el protagonista y puede aparecer en N c\u00f3mics.</p>"},{"location":"UD05-Spark/4.superhero/#ejercicio","title":"Ejercicio","text":"<p>Debes averiguar qui\u00e9n es el superh\u00e9roe m\u00e1s popular teniendo en cuenta el total de colaboraciones que haya tenido en cada uno de sus c\u00f3mics. Cuantas m\u00e1s colaboraciones m\u00e1s popular</p>"},{"location":"UD05-Spark/4.superhero/#2-en-busca-del-superheroe-menos-popular","title":"2. En busca del superh\u00e9roe MENOS popular","text":"<p>Utilizaremos los mismos archivos que en el caso anterior. - Buscaremos los superh\u00e9roes con solamente 1 conexi\u00f3n. - EXTRA: calcular n\u00famero menor de conexiones en el conjunto de datos en lugar de suponer que es uno.</p>"},{"location":"UD05-Spark/4.superhero/#estrategia-a-seguir","title":"Estrategia a seguir","text":"<ol> <li>Filtrar conexiones para buscar filas con una conexi\u00f3n.</li> <li>Hacer join con el DF de los nombres.</li> <li>Seleccionar y mostrar la columna \"names\".</li> </ol>"},{"location":"UD05-Spark/4.superhero/#snippets","title":"Snippets","text":"<pre><code>Dataframename.filter(func.col(\"columnname\")== valor)\nDafaframename.join(otroDF, \"columnaencomun\")\nagg(func.min(\"nombrecolumna\").first()[0])\n</code></pre>"},{"location":"UD05-Spark/4.superhero/#3-grados-de-separacion","title":"3. Grados de separaci\u00f3n","text":"<p>Implementaci\u00f3n iterativa con spark de Breadth-First-Search. https://www.youtube.com/watch?v=_no9DorK0ww</p> <p>Ejemplo:  SpiderMan est\u00e1 conectado a Hulk y este a IronMan, por lo que SpiderMan y IronMan est\u00e1n a 2 grados de separaci\u00f3n.</p> <p>Para averiguar cu\u00e1ntos grados de separaci\u00f3n hay entre 2 SuperH\u00e9roes se utiliza el algoritmo BFS Grafos: B\u00fasqueda en anchura</p>"},{"location":"UD05-Spark/4b.similarmovies/","title":"Similar Movies","text":"<p>Este ejercicio consiste en crear un sistema de recomendaci\u00f3n de pel\u00edculas. - Buscaremos un par de pel\u00edculas que hayan sido vistas por la misma persona. - Mediremos lo similares que son las calificaciones entre los usuarios que hayan visto estas dos pel\u00edculas. - Ordenaremos por pel\u00edcula y similaridad.</p> <p>https://github.com/josepgarcia/datos/tree/main/superhero</p> <pre><code>$ egrep SPIDER Marvel+Names | head -n 5\n400 \"BEACH, SPIDER\"\n603 \"BLOOD SPIDER/\"\n3413 \"MAN-SPIDER | MUTANT \"\n3414 \"MAN-SPIDER CLONE | M\"\n5306 \"SPIDER-MAN/PETER PAR\"\n</code></pre> <pre><code>$ head -n 5 Marvel+Graph\n5988 748 1722 3752 4655 5743 1872 3413 5527 6368 6085 4319 4728 1636 2397 3364 4001 1614 1819 1585 732 2660 3952 2507 3891 2070 2239 2602 612 1352 5447 4548 1596 5488 1605 5517 11 479 2554 2043 17 865 4292 6312 473 534 1479 6375 4456\n5989 4080 4264 4446 3779 2430 2297 6169 3530 3272 4282 6432 2548 4140 185 105 3878 2429 1334 4595 2767 3956 3877 4776 4946 3407 128 269 5775 5121 481 5516 4758 4053 1044 1602 3889 1535 6038 533 3986\n5982 217 595 1194 3308 2940 1815 794 1503 5197 859 5096 6039 2664 651 2244 528 284 1449 1097 1172 1092 108 3405 5204 387 4607 4545 3705 4930 1805 4712 4404 247 4754 4427 1845 536 5795 5978 533 3984 6056\n5983 1165 3836 4361 1282 716 4289 4646 6300 5084 2397 4454 1913 5861 5485\n5980 2731 3712 1587 6084 2472 2546 6313 875 859 323 2664 1469 522 2506 2919 2423 3624 5736 5046 1787 5776 3245 3840 2399\n</code></pre> <p>Marvel+Names: Contiene un identificador \u00fanico y el nombre del superh\u00e9roe.</p> <p>Marvel+Graph:  Cada fila representa un c\u00f3mic distinto. En cada fila, el primer campo indica el protagonista del c\u00f3mic, seguido de los superh\u00e9roes que tambi\u00e9n aparecen en ese c\u00f3mic. Un superh\u00e9roe puede ser el protagonista y puede aparecer en N c\u00f3mics.</p>"},{"location":"UD05-Spark/4b.similarmovies/#el-mas-popular","title":"El m\u00e1s popular","text":"<p>Debes averiguar qui\u00e9n es el superh\u00e9roe m\u00e1s popular teniendo en cuenta el total de colaboraciones que haya tenido en cada uno de sus c\u00f3mics. Cuantas m\u00e1s colaboraciones m\u00e1s popular</p>"},{"location":"UD05-Spark/4b.similarmovies/#en-busca-del-superheroe-menos-popular","title":"En busca del superh\u00e9roe MENOS popular","text":"<p>Utilizaremos los mismos archivos que en el caso anterior. - Buscaremos los superh\u00e9roes con solamente 1 conexi\u00f3n. - EXTRA: calcular n\u00famero menor de conexiones en el conjunto de datos en lugar de suponer que es uno.</p>"},{"location":"UD05-Spark/4b.similarmovies/#estrategia-a-seguir","title":"Estrategia a seguir","text":"<ol> <li>Filtrar conexiones para buscar filas con una conexi\u00f3n.</li> <li>Hacer join con el DF de los nombres.</li> <li>Seleccionar y mostrar la columna \"names\".</li> </ol>"},{"location":"UD05-Spark/4b.similarmovies/#snippets","title":"Snippets","text":"<pre><code>Dataframename.filter(func.col(\"columnname\")== valor)\nDafaframename.join(otroDF, \"columnaencomun\")\nagg(func.min(\"nombrecolumna\").first()[0])\n</code></pre>"},{"location":"UD05-Spark/4b.similarmovies/#grados-de-separacion","title":"Grados de separaci\u00f3n","text":"<p>Implementaci\u00f3n iterativa con spark de Breadth-First-Search. https://www.youtube.com/watch?v=_no9DorK0ww</p> <p>Ejemplo:  SpiderMan est\u00e1 conectado a Hulk y este a IronMan, por lo que SpiderMan y IronMan est\u00e1n a 2 grados de separaci\u00f3n.</p> <p>Para averiguar cu\u00e1ntos grados de separaci\u00f3n hay entre 2 SuperH\u00e9roes se utiliza el algoritmo BFS Grafos: B\u00fasqueda en anchura</p>"},{"location":"UD05-Spark/5.formatodatos/","title":"Formato de datos","text":"<p>https://aitor-medrano.github.io/iabd/de/formatos.html</p>"},{"location":"UD05-Spark/6.streaming/","title":"Streaming","text":"<p>https://aitor-medrano.github.io/iabd/spark/streaming.html</p> <p>Apache Spark Streaming es una extensi\u00f3n de Apache Spark que permite el procesamiento de flujos de datos en tiempo real. A diferencia del procesamiento por lotes tradicional, que maneja grandes cantidades de datos en bloques, Spark Streaming permite procesar datos de forma continua y en tiempo real a medida que se van generando. Esto lo hace ideal para aplicaciones que requieren procesamiento inmediato de datos, como monitoreo en tiempo real, an\u00e1lisis de logs, y sistemas de recomendaci\u00f3n en tiempo real.</p> <p>Los datos se pueden ingestar desde diversas fuentes de datos, como\u00a0Kafka,\u00a0sockets\u00a0TCP, etc.. y se pueden procesar mediante funciones de alto nivel, ya sea mediante el uso de RDD y algoritmos\u00a0MapReduce, o utilizando\u00a0DataFrames\u00a0y la sintaxis SQL. Finalmente, los datos procesados se almacenan en sistemas de ficheros, bases de datos o cuadros de mandos. </p>"},{"location":"UD05-Spark/6.streaming/#comando-ncat-nc","title":"Comando ncat (nc)","text":"<p>Permite acceder a puertos TCP o UDP de la propia m\u00e1quina o de otras m\u00e1quinas remotas. Tambi\u00e9n permite quedar a la escucha en un puerto dado (TCP o UDP) de la m\u00e1quina local.</p> <p>Podemos utilizar a ncat como herramienta de escaneo de puertos, de seguridad o de monitoreo; adem\u00e1s de como proxy TCP simple. Es muy \u00fatil para auditar la seguridad de sistemas, de servidores web, de servidores de correo, entre otros.</p> <p>IMPORTANTE: Aunque se asemejan, ncat y netcat son programas diferentes. https://access.redhat.com/documentation/es-es/red_hat_enterprise_linux/7/html/migration_planning_guide/ch04s07s04</p> <pre><code>TLDR\n\n   Redirect I/O into a network stream through this versatile tool. More information: [https://manned.org/man/nc.1](https://manned.org/man/nc.1).\n\n   - Start a listener on the specified TCP port and send a file into it:\n   nc -l -p port &lt; filename\n   - Connect to a target listener on the specified port and receive a file from it:\n   nc host port &gt; received_filename\n   - Scan the open TCP ports of a specified host:\n   nc -v -z -w timeout_in_seconds host start_port-end_port\n   - Start a listener on the specified TCP port and provide your local shell access to the connected party (this is dangerous and can be abused):\n   nc -l -p port -e shell_executable\n   - Connect to a target listener and provide your local shell access to the remote party (this is dangerous and can be abused):\n   nc host port -e shell_executable\n   - Act as a proxy and forward data from a local TCP port to the given remote host:\n   nc -l -p local_port | nc host remote_port\n   - Send an HTTP GET request:\n   echo -e \"GET / HTTP/1.1\\nHost: host\\n\\n\" | nc host 80\n</code></pre> <p>Funcionamiento</p> <p>Cuando funciona como cliente, nc crea un socket para conectarse al puerto indicado de la m\u00e1quina destino.</p> <pre><code>$ nc maquina_destino puerto_destino\n</code></pre> <p>La conexi\u00f3n permenecer\u00e1 abierta mientras no la finalice el servidor o el cliente nc (CONTROL+C)</p> <p>Cuando funciona como servidor (modo escucha [opci\u00f3n -l, listen]), abre un socket en la m\u00e1quina local que queda a la escucha en el puerto indicado</p> <pre><code>$ nc -l -p puerto_escucha\n</code></pre> <p>En ambos casos, una vez establecida la conexi\u00f3n, nc env\u00eda a trav\u00e9s del socket creado todo lo que reciba por la entrada est\u00e1ndar y env\u00eda a la salida est\u00e1ndar lo que le llegue por el socket.</p> <p>Opciones interesantes:</p> <p>-u usa el modo UDP (por defecto son conexiones TCP) -c comando / -e ejecutable ejecuta un comando/programa una vez iniciada la conexi\u00f3n cuyas entrada y salida est\u00e1ndar est\u00e1n redirigidas a la conexi\u00f3n establecida</p> <p>Ejemplo: Enviar - Recibir </p> <pre><code># En una terminal escuchamos\nnc -l -p 999 # En mac sin -p\n\n# En otra terminal enviamos\nnc localhost 999 \n</code></pre> <p>Ejemplo: Copia de ficheros</p> <p>* Intentar con linux *</p> <pre><code># Abrimos un terminal, se queda a la escucha\nnc localhost 88 &gt; destino.txt\n\n# Abrimos otro termina, creamos un fichero y lo mandamos al primer terminal\necho \"aa\\nb\\ncc\\ndd\" &gt; origen.txt\nnc localhost 88 &lt; origen.txt\n</code></pre> <p>Ejemplo con python:</p> <pre><code># En un terminal\n# Opci\u00f3n k -&gt; keep open, mantiene la conexi\u00f3n abierta incluso despu\u00e9s de recibir datos.\nnc -lk 88\n</code></pre> <pre><code>import socket\nimport time\nimport random\n\ndef enviar_palabra(palabra, host='localhost', puerto=88):\n    # Crear un socket TCP/IP\n    cliente_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n    try:\n        # Conectar al servidor netcat\n        cliente_socket.connect((host, puerto))\n\n        # Enviar la palabra al servidor\n        palabra = palabra + '\\n'\n        cliente_socket.sendall(palabra.encode())\n\n        # Esperar X segundos\n        intervalo = random.uniform(0.5, 2)\n        time.sleep(intervalo)\n    finally:\n        # Cerrar la conexi\u00f3n\n        cliente_socket.close()\n\nif __name__ == \"__main__\":\n    palabras = [\"hola\", \"mundo\", \"python\", \"netcat\", \"socket\"]\n    palabras = palabras * 2\n    for palabra in palabras:\n        enviar_palabra(palabra)\n</code></pre> <p>La l\u00ednea \"if name == \"main\":\" es una forma de asegurarse de que cierto c\u00f3digo se ejecute solo cuando el archivo Python se ejecuta directamente, y no cuando se importa como un m\u00f3dulo en otro script.</p> <pre><code>chatGPT\n\n    \u2022   __name__: Es una variable especial en Python. Cuando un archivo se ejecuta como un script, __name__ toma el valor \"__main__\". Pero si el archivo es importado como un m\u00f3dulo en otro script, __name__ tomar\u00e1 el nombre del archivo (sin la extensi\u00f3n .py).\n    \u2022   __main__: Es un valor especial asignado a __name__ cuando el script se ejecuta directamente. Esto significa que el bloque de c\u00f3digo dentro de if __name__ == \"__main__\": solo se ejecutar\u00e1 si el archivo es ejecutado directamente (por ejemplo, con python archivo.py).\n</code></pre> <p>La funci\u00f3n encode() se utiliza para convertir una cadena de caracteres (string) en su representaci\u00f3n en bytes, utilizando un cierto esquema de codificaci\u00f3n de caracteres. En el contexto de la comunicaci\u00f3n a trav\u00e9s de sockets o de la lectura/escritura de archivos, es importante trabajar con cadenas de bytes en lugar de cadenas de caracteres directamente, ya que muchos protocolos y sistemas de archivos operan a nivel de bytes. https://www.w3schools.com/python/ref_string_encode.asp</p> <p>Beginner\u2019s Guide To Netcat for Hackers</p> <p>https://www.youtube.com/watch?v=8oGm4pEAsd8</p>"},{"location":"UD05-Spark/6.streaming/#caso-1-hola-spark-streaming","title":"Caso 1: Hola Spark Streaming","text":"<p>Abrimos un terminal desde donde enviaremos los mensajes:</p> <pre><code>nc -lk 9999\n</code></pre> <p>Tras arrancar\u00a0Netcat, ya podemos crear nuestra aplicaci\u00f3n\u00a0Spark\u00a0(vamos a indicar que cree 2 hilos, lo cual es el m\u00ednimo necesario para realizar\u00a0streaming, uno para recibir y otro para procesar), en la cual tenemos diferenciadas:</p> <ul> <li>la fuente de datos: creaci\u00f3n del flujo de lectura mediante\u00a0<code>readStream</code>\u00a0que devuelve un\u00a0DataStreamReader\u00a0que utilizaremos para cargar un\u00a0DataFrame.</li> <li>la l\u00f3gica de procesamiento, ya sea mediante\u00a0DataFrames API\u00a0o\u00a0Spark SQL.</li> <li>la persistencia de los datos mediante\u00a0writeStream\u00a0que devuelve un\u00a0DataStreamWriter\u00a0donde indicamos el modo de salida, el cual, al iniciarlo con\u00a0<code>start</code>\u00a0nos devuelve un\u00a0StreamingQuery</li> <li>y finalmente el cierre del flujo de datos a partir de la consult en\u00a0streaming\u00a0mediante\u00a0<code>awaitTermination</code>.</li> </ul> <p>En un cuaderno jupyter\u2026.</p> <pre><code>''' Streaming1.ipynb '''\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession \\\n        .builder \\\n        .appName(\"Streaming IABD WordCount\") \\\n        .master(\"local[2]\") \\\n        .getOrCreate()\n\n# Creamos un flujo de escucha sobre netcat en localhost:9999\n# En Spark Streaming, la lectura se realiza mediante readStream\nlineasDF = spark.readStream \\\n        .format(\"socket\") \\\n        .option(\"host\", \"IP_MAQUINA\") \\ # Ponemos IP local de la m\u00e1quina\n        .option(\"port\", \"9999\") \\\n        .load()\n\n# Leemos las l\u00edneas y las pasamos a palabras.\n# Sobre ellas, realizamos la agrupaci\u00f3n count (transformaci\u00f3n)\nfrom pyspark.sql.functions import explode, split\npalabrasDF = lineasDF.select(explode(split(lineasDF.value, ' ')).alias('palabra'))\ncantidadDF = palabrasDF.groupBy(\"palabra\").count()\n\n# Mostramos las palabras por consola (sink)\n# En Spark Streaming, la persistencia se realiza mediante writeStream\n#  y en vez de realizar un save, ahora utilizamos start\nwordCountQuery = cantidadDF.writeStream \\\n        .format(\"console\") \\\n        .outputMode(\"complete\") \\\n        .start()\n\n# dejamos Spark a la escucha\nwordCountQuery.awaitTermination()\n</code></pre>"},{"location":"UD05-Spark/6.streaming/#elementos","title":"Elementos","text":"<p>La idea b\u00e1sica al trabajar los datos en\u00a0streaming\u00a0es similar a tener una tabla de entrada de tama\u00f1o ilimitado, y conforme llegan nuevos datos, tratarlos como un nuevo conjunto de filas que se adjuntan a la tabla. </p>"},{"location":"UD05-Spark/6.streaming/#fuentes-de-datos","title":"Fuentes de Datos","text":"<p>Mientras que en el procesamiento\u00a0batch\u00a0las fuentes de datos son\u00a0datasets\u00a0est\u00e1ticos que residen en un almacenamiento como pueda ser un sistema local, HDFS o S3, al hablar de procesamiento en\u00a0streaming\u00a0las fuentes de datos generan los datos de forma continuada, por lo que necesitamos otro tipo de fuentes.</p> <p>Structured Streaming\u00a0ofrece un conjunto predefinido de\u00a0fuentes de datos\u00a0que se leen a partir de un\u00a0DataStreamReader. Los tipos existentes son:</p> <p>Fichero: permite leer ficheros desde un directorio como un flujo de datos, con soporte para ficheros de texto, CSV, JSON, Parquet, ORC, etc\u2026</p> <pre><code># Lee todos los ficheros csv de un directorio\nesquemaUsuario = StructType() \\\n    .add(\"nombre\", \"string\").add(\"edad\", \"integer\")\n\ncsvDF = spark.readStream \\\n    .option(\"sep\", \";\") \\\n    .schema(esquemaUsuario) \\\n    .csv(\"/path/al/directorio\")  # equivalente a format(\"csv\").load(\"/path/al/directorio\")\n</code></pre> <p>Socket: lee texto UTF8 desde una conexi\u00f3n\u00a0socket\u00a0(es el que hemos utilizado en el caso de uso 1). S\u00f3lo se debe utilizar para pruebas ya que no ofrece garant\u00eda de tolerancia de fallos de punto a punto.</p> <pre><code>socketDF = spark.readStream \\\n    .format(\"socket\") \\\n    .option(\"host\", \"localhost\") \\\n    .option(\"port\", 9999) \\\n    .load()\n</code></pre> <p>Rate: Genera datos indicando una cantidad de filas por segundo, donde cada fila contiene un\u00a0timestamp\u00a0y el valor de un contador secuencial (la primera fila contiene el 0). Esta fuente tambi\u00e9n se utiliza para la realizaci\u00f3n de pruebas y\u00a0benchmarking.</p> <pre><code>socketDF = spark.readStream \\\n    .format(\"rate\") \\\n    .option(\"rowsPerSecond\", 1)\n    .load()\n</code></pre> <p>Tabla\u00a0(desde\u00a0Spark 3.1): Carga los datos desde una tabla temporal de\u00a0SparkSQL, la cual podemos utilizar tanto para cargar como para persistir los c\u00e1lculos realizados. M\u00e1s informaci\u00f3n en la\u00a0documentaci\u00f3n oficial.</p> <pre><code>tablaDF = spark.readStream \\\n    .table(\"clientes\")\n</code></pre>"},{"location":"UD05-Spark/6.streaming/#sinks","title":"Sinks","text":"<p>El t\u00e9rmino \"sinks\" se refiere a las operaciones de escritura de datos que se realizan al final de un proceso de streaming o transformaci\u00f3n de datos. Un \"sink\" (o \"destino\" en espa\u00f1ol) en PySpark es el lugar donde se env\u00edan los datos procesados para su almacenamiento o uso posterior.</p> <p>Escriben a partir de un\u00a0DataStreamWriter\u00a0mediante el interfaz\u00a0<code>writeStream</code></p> <p>Fichero: Podemos almacenar los resultados en un sistema de archivos, HDFS o S3, con soporte para los formatos CSV, JSON, ORC y Parquet.</p> <pre><code># Otros valores pueden ser \"json\", \"csv\", etc...\ndf.writeStream.format(\"parquet\") \\        \n    .option(\"path\", \"/path/al/directorio\") \\ \n    .start()\n</code></pre> <p>Kafka: Env\u00eda los datos a un cl\u00faster de\u00a0Kafka:</p> <pre><code>df.writeStream.format(\"kafka\") \\        \n    .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n    .option(\"topic\", \"miTopic\")\n    .start()\n</code></pre> <p>Foreach\u00a0y\u00a0ForeachBatch: permiten realizar operaciones y escribir l\u00f3gica sobre la salida de una consulta de\u00a0streaming, ya sea a nivel de fila (foreach) como a nivel de micro-batch (foreachBatch). M\u00e1s informaci\u00f3n en la\u00a0documentaci\u00f3n oficial.</p> <p>Consola: se emplea para pruebas y depuraci\u00f3n y permite mostrar el resultado por consola.</p> <p>Admite las opciones\u00a0<code>numRows</code>\u00a0para indicar las filas a mostrar y\u00a0<code>truncate</code>\u00a0para truncar los datos si las filas son muy largas.</p> <pre><code>df.writeStream.format(\"console\") \\        \n    .start()\n</code></pre> <p>Memoria: se emplea para pruebas y depuraci\u00f3n, ya que s\u00f3lo permite un volumen peque\u00f1o de datos para evitar un problema de falta de memoria en el driver para almacenar la salida. Los datos se almacenan en una tabla temporal a la cual podemos acceder desde\u00a0SparkSQL:</p> <pre><code>df.writeStream.format(\"memory\") \\  \n    .queryName(\"nombreTabla\")  \n    .start()\n</code></pre>"},{"location":"UD05-Spark/6.streaming/#modos-de-salida","title":"Modos de salida","text":"<p>El\u00a0modo de salida\u00a0determina c\u00f3mo salen los datos a un sumidero de datos. Existen tres opciones:</p> <ul> <li>A\u00f1adir (<code>*append*</code>): para insertar los datos, cuando sabemos que no vamos a modificar ninguna salida anterior, y que cada\u00a0batch\u00a0\u00fanicamente escribir\u00e1 nuevos registros. Es el modo por defecto.</li> <li>Modificar (<code>*update*</code>): similar a un\u00a0upsert, donde veremos solo registros que, bien son nuevos, bien son valores antiguos que debemos modificar.</li> <li>Completa (<code>*complete*</code>): para sobrescribir completamente el resultado, de manera que siempre recibimos la salida completa.</li> </ul> <p>En el caso 1 hemos utilizado el modo de salida completa, de manera que con cada dato nuevo, se mostraba como resultado todas las palabras y su cantidad. Si hubi\u00e9semos elegido el modo\u00a0update, en cada micro-batch solo se mostrar\u00eda el resultado acumulado de cada\u00a0batch.</p>"},{"location":"UD05-Spark/6.streaming/#ejercicio","title":"Ejercicio","text":"<ol> <li>Abrimos nc -lk -p 9999</li> <li>Duplicamos Streaming1.ipynb a Streaming2.ipynb</li> <li>Iniciamos</li> <li> <p>Mandamos dos frases </p> <p>Esta es la primera frase</p> <p>Enviamos segunda linea primera</p> </li> </ol> <pre><code>-------------------------------------------\nBatch: 2\n-------------------------------------------\n+--------+-----+\n| palabra|count|\n+--------+-----+\n|   frase|    1|\n| segunda|    1|\n|      es|    1|\n|   linea|    1|\n|    Esta|    1|\n|      la|    1|\n|Enviamos|    1|\n| primera|    1|\n+--------+-----+\n</code></pre> <ol> <li>Modificamos el c\u00f3digo del ejercicio, detenemos todo, volvemos a iniciar</li> </ol> <pre><code>wordCountQuery = cantidadDF.writeStream \\\n    .format(\"console\") \\\n    .outputMode(\"update\") \\\n    .start()\n</code></pre> <ol> <li>Mandamos las mismas frases</li> </ol> <pre><code>-------------------------------------------\nBatch: 2\n-------------------------------------------\n+--------+-----+\n| palabra|count|\n+--------+-----+\n| segunda|    1|\n|   linea|    1|\n|      la|    2|\n|Enviamos|    1|\n+--------+-----+\n</code></pre> <ol> <li>Modificamos el c\u00f3digo del ejercicio, detenemos todo, volvemos a iniciar</li> </ol> <pre><code>wordCountQuery = cantidadDF.writeStream \\\n    .format(\"console\") \\\n    .outputMode(\"append\") \\\n    .start()\n</code></pre> <p>Con este ejemplo, el modo\u00a0append\u00a0no tiene sentido (ya que para contar las palabras necesitamos las anteriores), y\u00a0Spark\u00a0es tan listo que cuando realizamos agregaciones no permite su uso y lanza una excepci\u00f3n del tipo\u00a0<code>AnalysisException</code>: <code>AnalysisException: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;</code></p> <p>En resumen, el modo append es s\u00f3lo para inserciones, update para modificaciones e inserciones y finalmente complete sobrescribe los resultados previos.</p> <p>Adem\u00e1s, no todos los tipos de salida se pueden aplicar siempre, va a depender del tipo de operaciones que realicemos.</p>"},{"location":"UD05-Spark/6.streaming/#transformaciones","title":"Transformaciones","text":"<ul> <li> <p>Sin estado (stateless): los datos de cada\u00a0micro-batch\u00a0son independientes de los anteriores, y por tanto, podemos realizar las transformaciones\u00a0<code>select</code>,\u00a0<code>filter</code>,\u00a0<code>map</code>,\u00a0<code>flatMap</code>,\u00a0<code>explode</code>. Es importante destacar que estas transformaciones no soportan el modo de salida\u00a0complete, por lo que s\u00f3lo podemos utilizar los modos\u00a0append\u00a0o\u00a0update.</p> </li> <li> <p>Con estado (stateful): aquellas que implica realizar agrupaciones, agregaciones,\u00a0windowing\u00a0y/o\u00a0joins, ya que mantienen el estado entre los diferentes\u00a0micro-batches. Destacar que un abuso del estado puede causar problemas de falta de memoria, ya que el estado se almacena en la memoria de los ejecutores (executors). Por ello,\u00a0Spark\u00a0ofrece dos tipos de operaciones con estado:</p> <ul> <li>Gestionadas (managed):\u00a0Spark\u00a0gestiona el estado y libera la memoria conforme sea necesario.</li> <li>Sin gestionar (unmanaged): permite que el desarrollador defina las pol\u00edticas de limpieza del estado (y su liberaci\u00f3n de memoria), por ejemplo, a partir de pol\u00edticas basadas en el tiempo. A d\u00eda de hoy, las transformaci\u00f3n sin gestionar s\u00f3lo est\u00e1n disponibles mediante\u00a0Java\u00a0o\u00a0Scala.</li> </ul> </li> </ul> <p>Adem\u00e1s, hay que tener en cuenta que no todas las operaciones que realizamos con\u00a0DataFrames\u00a0est\u00e1n soportadas al trabajar en\u00a0streaming, como pueden ser\u00a0<code>show</code>,\u00a0<code>describe</code>,\u00a0<code>count</code>\u00a0(aunque s\u00ed que podemos contar sobre agregaciones/funciones ventana),\u00a0<code>limit</code>,\u00a0<code>distinct</code>,\u00a0<code>cube</code>\u00a0o\u00a0<code>sort</code>\u00a0(podemos ordenar en algunos casos despu\u00e9s de haber realizado una agregaci\u00f3n), ya que los datos no est\u00e1n acotados y provocar\u00e1 una excepci\u00f3n del tipo\u00a0<code>AnalysisException</code>.</p>"},{"location":"UD05-Spark/6.streaming/#triggers","title":"Triggers","text":"<p>Un\u00a0trigger\u00a0define el intervalo (timing) temporal de procesamiento de los datos en\u00a0streaming, indicando si la consulta se ejecutar\u00e1 como un\u00a0micro-batch\u00a0mediante un intervalo fijo o con una consulta con procesamiento continuo.</p> <p>As\u00ed pues, un trigger es un mecanismo para que el motor de\u00a0Spark SQL\u00a0determine cuando ejecutar la computaci\u00f3n en\u00a0streaming.</p> <p>Los posibles tipos son: - Sin especificar, de manera que cada micro-batch se va a ejecutar tan pronto como lleguen datos. - Por intervalo de tiempo, mediante la propiedad\u00a0<code>processingTime</code>. Si indicamos un intervalo de un minuto, una vez finalizado un job, si no ha pasado un minuto, se esperar\u00e1 a ejecutarse. Si el micro-batch tardase m\u00e1s de un minuto, el siguiente se ejecutar\u00eda inmediatamente. As\u00ed pues, de esta manera,\u00a0Spark\u00a0permite colectar datos de entrada y procesarlos de manera conjunta (en vez de procesar individualmente cada registro de entrada). - Un intervalo, mediante la propiedad\u00a0<code>once</code>, de manera que funciona como un proceso\u00a0batch\u00a0est\u00e1ndar, creando un \u00fanico proceso\u00a0micro-batch, o con la propiedad\u00a0<code>availableNow</code>\u00a0para leer todos los datos disponibles hasta el momento mediante m\u00faltiples\u00a0batches. - Continuo, mediante la propiedad\u00a0<code>continuous</code>, para permitir latencias del orden de milisegundos mediante\u00a0Continuous Processing. Se trata de una opci\u00f3n experimental desde la versi\u00f3n 2.3 de Spark.</p> <p>Los triggers se configuran al persistir el\u00a0DataFrame, tras indicar el modo de salida mediante el m\u00e9todo\u00a0trigger:</p> <p>Ejercicio: Modifica Streaming2.ipynb</p> <pre><code>wordCountQuery = cantidadDF.writeStream \\\n    .format(\"console\") \\\n    .outputMode(\"complete\") \\\n    .trigger(processingTime=\"1 minute\") \\\n    .start()\n</code></pre>"},{"location":"UD05-Spark/6b.streamingfacturas/","title":"Facturas: Spark streaming con ficheros","text":"<p>Este ejercicio tiene como objetivo procesar archivos JSON de facturas generados por diferentes sucursales de una empresa cada 5 minutos. Cada factura puede contener m\u00faltiples l\u00edneas de art\u00edculos, y el prop\u00f3sito es descomponerlas en facturas individuales.</p> <p>https://github.com/josepgarcia/datos/raw/main/invoices/invoices.zip</p> <p>ENTRADA: Cada archivo JSON incluye detalles de la factura, como el n\u00famero de factura, la hora de creaci\u00f3n, el ID de la tienda, el ID del punto de venta, el tipo de cliente, el m\u00e9todo de pago, el tipo de entrega y una lista de art\u00edculos (InvoiceLineItems). SALIDA: Por cada l\u00ednea de art\u00edculo en InvoiceLineItems, se generar\u00e1 un nuevo documento con los detalles de la factura y del art\u00edculo correspondiente.</p> <p>EJEMPLO Estructura de una de las facturas almacenadas en JSON   A partir de la estructura anterior generar\u00edamos 4 l\u00edneas como la siguiente: Estructura de directorios para el proyecto: <pre><code>invoices/\n\u251c\u2500\u2500 invoices.ipynb\n\u251c\u2500\u2500 DATOS_TMP/\n\u2502   \u251c\u2500\u2500 Invoice-set2.json\n\u2502   \u2514\u2500\u2500 Invoice-set3.json\n\u2514\u2500\u2500 ENTRADA/\n    \u2514\u2500\u2500 Invoice-set1.json\n</code></pre></p> <p>Funcionamiento del programa:</p> <ol> <li>El programa monitorea continuamente la carpeta ENTRADA, donde las sucursales depositan los archivos JSON de facturas.</li> <li>Para simular la llegada de nuevos archivos, se mover\u00e1n manualmente los archivos desde DATOS_TMP a ENTRADA.</li> <li>Cada 5 minutos se buscar\u00e1 un nuevo archivo en la carpeta ENTRADA, el programa lo procesa y guarda los datos transformados en una carpeta llamada SALIDA.</li> </ol>"},{"location":"UD05-Spark/6b.streamingfacturas/#version-inicial","title":"Versi\u00f3n inicial","text":"<p>La primera versi\u00f3n ser\u00e1 sin \"streaming\", leeremos los datos de <code>ENTRADA</code>los transformaremos y los escribiremos en <code>SALIDA</code></p>"},{"location":"UD05-Spark/6b.streamingfacturas/#lectura-de-datos","title":"Lectura de datos","text":"<p>Creamos el archivo invoices.ipynb</p> <p>Iniciar una sesi\u00f3n de Spark y leer todos los archivos JSON de la carpeta ENTRADA. <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate() \nraw_df = spark.read.json(\"./invoices/ENTRADA/*.json\")\n\nraw_df.printSchema()\nraw_df.show(5)\n</code></pre></p>"},{"location":"UD05-Spark/6b.streamingfacturas/#transformacion","title":"Transformaci\u00f3n","text":"<p>Seleccionar las columnas relevantes y usar la funci\u00f3n explode para descomponer el array InvoiceLineItems en filas individuales.</p> <pre><code>explode_df = raw_df.selectExpr(\"InvoiceNumber\", \"CreatedTime\", \"StoreID\",\n                                 \"PosID\", \"CustomerType\",\n                                 \"PaymentMethod\", \"DeliveryType\",\n                                 \"explode(InvoiceLineItems) as LineItem\")\nexplode_df.printSchema()\nexplode_df.show(5)\n</code></pre> <p>Renombrar las columnas anidadas para obtener una estructura plana y eliminar la columna original LineItem. <pre><code>from pyspark.sql.functions import expr\nlimpio_df = explode_df \\\n    .withColumn(\"ItemCode\", expr(\"LineItem.ItemCode\")) \\\n    .withColumn(\"ItemDescription\", expr(\"LineItem.ItemDescription\")) \\\n    .withColumn(\"ItemPrice\", expr(\"LineItem.ItemPrice\")) \\\n    .withColumn(\"ItemQty\", expr(\"LineItem.ItemQty\")) \\\n    .withColumn(\"TotalValue\", expr(\"LineItem.TotalValue\")) \\\n    .drop(\"LineItem\")\nlimpio_df.printSchema()\nlimpio_df.show(5)\n</code></pre></p>"},{"location":"UD05-Spark/6b.streamingfacturas/#guardando-el-resultado","title":"Guardando el resultado","text":"<p>Escribir el DataFrame transformado en la carpeta SALIDA en el formato deseado (por ejemplo, CSV, json o Parquet). <pre><code>facturaWriterQuery = limpio_df.write \\\n    .format(\"json\") \\\n    .mode(\"overwrite\") \\\n    .option(\"path\", \"./invoices/salidatmp.json\") \\\n    .save()\n\nfacturaWriterQuery = limpio_df.write.mode(\"overwrite\").parquet(\"./invoices/SALIDA/\")\n</code></pre></p>"},{"location":"UD05-Spark/6b.streamingfacturas/#modificando-para-streaming","title":"Modificando para streaming","text":"<p>\u2705 Ejercicio: Adapta el programa para el procesamiento en tiempo real.</p> <p>Es necesario realizar ajustes que permitan que el programa permanezca a la espera de nuevos archivos en la carpeta ENTRADA. Esto se logra modificando las operaciones de lectura y escritura para que funcionen en modo streaming.</p> <p>Modificaciones en la lectura (READ) - Uso de readStream: Para leer datos en modo streaming, se debe utilizar la funci\u00f3n readStream en lugar de read.</p> <p>Modificaciones en la escritura (WRITE)  - Uso de writeStream: Para escribir datos en modo streaming, se debe utilizar la funci\u00f3n writeStream en lugar de write.  - Cambiar el atributo <code>mode(overwrite)</code>por <code>.outputMode(\u2753\u2753)</code>   - writeStream no soporta save()  - La salida ser\u00e1 una carpeta, no un archivo.  - Hay que a\u00f1adir un \"trigger\" que se dispare cada 5 minutos (para pruebas podemos poner 10 segundos) <code>.trigger(processingTime=\"5 minutes\")</code> - Finalizamos el writer con <code>.start()</code> - Despu\u00e9s del writer a\u00f1adimos las siguientes l\u00edneas: <pre><code>facturaWriterQuery.explain(True)\nfacturaWriterQuery.awaitTermination()\n</code></pre></p> <p>Ampliaci\u00f3n - Configuraci\u00f3n de maxFilesPerTrigger: Esta opci\u00f3n limita el n\u00famero de archivos que se procesan en cada micro-lote, permitiendo controlar la cantidad de datos ingeridos. Limitar a 1 archivo por vez. - Archivado o eliminaci\u00f3n de archivos procesados: Para gestionar los archivos una vez procesados, se pueden utilizar las opciones cleanSource y sourceArchiveDir.     - cleanSource: Define la acci\u00f3n a realizar con los archivos procesados. Puede tomar los valores archive (archivar) o delete (eliminar).     - sourceArchiveDir: Especifica el directorio donde se mover\u00e1n los archivos si se elige la opci\u00f3n de archivado. - Al writer le a\u00f1adimos la opci\u00f3n: <code>.option(\"checkpointLocation\", \"chk-point-dir\")</code> \u00bfPara qu\u00e9 sirve?</p> <p>Es importante considerar que archivar o eliminar archivos procesados puede impactar en el rendimiento de los micro-lotes. Si los micro-lotes son largos, se puede utilizar la opci\u00f3n cleanSource. En caso de que los micro-lotes sean cortos y la demora introducida por cleanSource no sea aceptable, es recomendable implementar un proceso de limpieza separado que se ejecute peri\u00f3dicamente para gestionar el directorio de entrada. Al implementar estas modificaciones, el programa podr\u00e1 procesar de manera continua los archivos de facturas que se a\u00f1adan a la carpeta ENTRADA, transformarlos y guardarlos en la carpeta SALIDA, manteniendo la eficiencia y la tolerancia a fallos en el procesamiento de datos en tiempo real.</p> <pre><code>### AYUDA\ninvoice_schema = StructType([\n    StructField(\"InvoiceNumber\", StringType(), True),\n    StructField(\"CreatedTime\", StringType(), True),\n    StructField(\"StoreID\", StringType(), True),\n    StructField(\"PosID\", StringType(), True),\n    StructField(\"CustomerType\", StringType(), True),\n    StructField(\"PaymentMethod\", StringType(), True),\n    StructField(\"DeliveryType\", StringType(), True),\n    StructField(\"InvoiceLineItems\", ArrayType(StructType([\n        StructField(\"ItemCode\", StringType(), True),\n        StructField(\"ItemDescription\", StringType(), True),\n        StructField(\"ItemPrice\", DoubleType(), True),\n        StructField(\"ItemQty\", IntegerType(), True),\n        StructField(\"TotalValue\", DoubleType(), True)\n    ])), True)\n])\n</code></pre>"},{"location":"UD05-Spark/6b.streamingfacturas/#codigo-anterior","title":"C\u00f3digo anterior","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate() \nraw_df = spark.read.json(\"./invoices/ENTRADA/*.json\")\n\nexplode_df = raw_df.selectExpr(\"InvoiceNumber\", \"CreatedTime\", \"StoreID\",\n                                 \"PosID\", \"CustomerType\",\n                                 \"PaymentMethod\", \"DeliveryType\",\n                                 \"explode(InvoiceLineItems) as LineItem\")\n\nfrom pyspark.sql.functions import expr\nlimpio_df = explode_df \\\n    .withColumn(\"ItemCode\", expr(\"LineItem.ItemCode\")) \\\n    .withColumn(\"ItemDescription\", expr(\"LineItem.ItemDescription\")) \\\n    .withColumn(\"ItemPrice\", expr(\"LineItem.ItemPrice\")) \\\n    .withColumn(\"ItemQty\", expr(\"LineItem.ItemQty\")) \\\n    .withColumn(\"TotalValue\", expr(\"LineItem.TotalValue\")) \\\n    .drop(\"LineItem\")\n\nfacturaWriterQuery = limpio_df.write \\\n    .format(\"json\") \\\n    .mode(\"overwrite\") \\\n    .option(\"path\", \"./invoices/salidatmp2.json\") \\\n    .save()\n</code></pre>"},{"location":"UD05-Spark/6b.streamingfacturas/#monitorizacion","title":"Monitorizaci\u00f3n","text":"<p>Una vez hemos realizado una consulta, podemos obtener\u00a0informaci\u00f3n\u00a0sobre la misma de forma programativa: <pre><code>facturaWriterQuery.explain() # muestra una explicaci\u00f3n detalla del plan de ejecuci\u00f3n\n# == Physical Plan ==\n# *(1) Project [InvoiceNumber#314, CreatedTime#308L, StoreID#319, PosID#317, CustomerType#310, PaymentMethod#316, DeliveryType#312, _extract_City#339 AS City#52, _extract_State#340 AS State#53, _extract_PinCode#341 AS PinCode#54, LineItem#55.ItemCode AS ItemCode#67, LineItem#55.ItemDescription AS ItemDescription#81, LineItem#55.ItemPrice AS ItemPrice#96, LineItem#55.ItemQty AS ItemQty#112L, LineItem#55.TotalValue AS TotalValue#129]\n# ...\nfacturaWriterQuery.recentProgress # muestra una lista de los \u00faltimos progresos de la consulta\n# [{'id': '3b6d37cf-6a3c-405e-a715-1dc787f34b00',\n#   'runId': '3dc7c478-626a-4558-87ea-4912da55114d',\n#   'name': 'Facturas Writer',\n#   'timestamp': '2024-05-11T08:20:49.058Z',\n#   'batchId': 0,\n#   'numInputRows': 500,\n#   'inputRowsPerSecond': 0.0,\n#   'processedRowsPerSecond': 113.55893708834887,\n#   'durationMs': {'addBatch': 2496,\n# ...\nfacturaWriterQuery.lastProgress # muestra el \u00faltimo progreso\n# {'id': '3b6d37cf-6a3c-405e-a715-1dc787f34b00',\n#  'runId': '3dc7c478-626a-4558-87ea-4912da55114d',\n#  'name': 'Facturas Writer',\n#  'timestamp': '2024-05-11T08:33:00.001Z',\n#  'batchId': 3,\n#  'numInputRows': 0,\n#  'inputRowsPerSecond': 0.0,\n#  'processedRowsPerSecond': 0.0,\n#  'durationMs': {'latestOffset': 5, 'triggerExecution': 8},\n# ...\n</code></pre></p> <p>Estas mismas estad\u00edsticas las podemos obtener de forma gr\u00e1fica. Al ejecutar procesos en Streaming, si accedemos a Spark UI, ahora podremos ver la pesta\u00f1a\u00a0Structured Streaming\u00a0con informaci\u00f3n detallada de la cantidad datos de entrada, tiempo procesado y duraci\u00f3n de los micro-batches: </p> <p>Adem\u00e1s, podemos iniciar tantas consultas como queramos en una \u00fanica sesi\u00f3n de Spark, las cuales se ejecutar\u00e1n de forma concurrente utilizando los recursos del cl\u00faster de Spark.</p> <p>Para acceder a la webUI deber\u00edamos de mapear los puertos tal y como lo hicimos anteriormente</p> <p>docker run -it -p 8888:8888 -p 4040:4040 -p 4041:4041 -p 4042:4042 -v $(pwd):/home/jovyan/work/projects/ jupyter/pyspark-notebook</p>"},{"location":"UD05-Spark/6b.streamingfacturas/#tolerancia-a-fallos","title":"Tolerancia a fallos","text":"<p>Un aplicaci\u00f3n en\u00a0streaming\u00a0se espera que se ejecute de forma ininterrumpida mediante un bucle infinito de micro-batches.</p> <p>Realmente, un escenario de ejecuci\u00f3n infinita no es posible, ya que la aplicaci\u00f3n se detendr\u00e1 por: - un fallo, ya sea por un dato mal formado o un error de red. - mantenimiento del sistema, para actualizar la aplicaci\u00f3n o el hardware donde corre.</p> <p>Para tratar la tolerancia a fallos, existen tres escenarios posibles: - Una vez como mucho (at most once): no se entrega m\u00e1s de una copia de un dato. Es decir, puede darse el caso de que no llegue, pero no habr\u00e1 repetidos. - Una vez al menos (at least once): en este caso no habr\u00e1 p\u00e9rdidas, pero un dato puede llegar m\u00e1s de una vez. - Una vez exacta (exactly once): se garantiza que cada dato se entrega una \u00fanica vez, sin p\u00e9rdidas ni duplicados.</p> <p></p> <p>Por ello, una aplicaci\u00f3n\u00a0Spark Streaming\u00a0se debe reiniciar de forma transparente para mantener la caracter\u00edstica de\u00a0exactly-once\u00a0la cual implica que: 1. No se pierde ning\u00fan registro 2. No crea registros duplicados.</p> <p>Para ello,\u00a0Spark Structured Streaming\u00a0mantiene el estado de los micro-batches mediante\u00a0checkpoints\u00a0que se almacenan en la carpeta indicada por la opci\u00f3n\u00a0<code>checkpointLocation</code>:</p> <pre><code>facturaWriterQuery = limpio_df.writeStream \\\n    .format(\"json\") \\\n    .queryName(\"Facturas Writer\") \\\n    .outputMode(\"append\") \\\n    .option(\"path\", \"salida\") \\\n    .option(\"checkpointLocation\", \"chk-point-dir\") \\\n    .trigger(processingTime=\"1 minute\") \\\n    .start()\n</code></pre> <p>La localizaci\u00f3n de esta carpeta deber\u00eda ser un sistema de archivo confiable y tolerante a fallos, como HDFS o Amazon S3.</p> <p>Esta carpeta contiene dos elementos: - Posici\u00f3n de lectura, que realiza la misma funci\u00f3n que los\u00a0offset\u00a0en Kafka, y representa el inicio y el final del rango de datos procesados por el actual\u00a0micro-batch, de manera que\u00a0Spark\u00a0conoce el progreso exacto del procesamiento. Una vez ha finalizado el\u00a0micro-batch,\u00a0Spark\u00a0realiza un\u00a0commit\u00a0para indicar que se han procesado los datos de forma exitosa. - Informaci\u00f3n del estado, que contiene los datos intermedios del\u00a0micro-batch, como la cantidad total de palabras contadas.</p> <p>De esta manera,\u00a0Spark\u00a0mantiene toda la informaci\u00f3n necesaria para reiniciar un micro-batch que no ha finalizado. Sin embargo, la capacidad de reiniciarse no tiene por qu\u00e9 garantizar la pol\u00edtica\u00a0exactly-once. Para ello, es necesario cumplir los siguientes requisitos:</p> <ol> <li>Reiniciar la aplicaci\u00f3n con el mismo\u00a0<code>checkpointLocation</code>. Si se elimina la carpeta o se ejecuta la misma consulta sobre otro directorio de\u00a0checkpoint\u00a0es como si realiz\u00e1semos una consulta desde 0.</li> <li>Utilizar una fuente de datos que permita volver a leer los datos incompletos del\u00a0micro-batch, por ejemplo, tanto los ficheros de texto como\u00a0Kafka\u00a0permiten volver a leer los datos desde un punto determinado. Sin embargo, los datos que provienen de un socket no permite volver a leerlos.</li> <li>Asegurar que la l\u00f3gica de aplicaci\u00f3n, dados los mismos datos de entrada, produce siempre el mismo resultado (aplicaci\u00f3n determinista). Si por ejemplo, nuestra l\u00f3gica de aplicaci\u00f3n utilizar\u00e1 alguna dependencia basada en fechas o el tiempo, ya no obtendr\u00edamos el mismo resultado.</li> <li>El destino (sink) debe ser capaz de identificar los elementos duplicados e ignorarlos o actualizar la copia antigua con el mismo registro, es decir, son idempotentes.</li> </ol>"},{"location":"UD05-Spark/6c.streamingbizum/","title":"Proyecto Bizum","text":"<p>\u2705 Entregar AULES</p> <p>Descomprime y ejecuta el script. <code>bizum_dist.zip</code></p> <p>Al ejecutarlo, este script crear\u00e1 autom\u00e1ticamente una simulaci\u00f3n de BIZUMs entregados y con ERROR. <pre><code>\u2705 Bizum entregado: Victoria Flores;272;Compra productos pesca\n\u2705 Bizum entregado: Victoria Flores;195;Pago clases baile\n\u26a0\ufe0f Error entregando el bizum:  Marta D\u00edaz;453;Compra videojuegos\n\u2705 Bizum entregado: Pablo Cano;119;Pago servicio dise\u00f1o industrial\n</code></pre></p> <p>Los Bizums que se entregan correctamente se guardan en la capeta <code>ENTREGADO</code>y los que fallan se guardan en la carpeta <code>ERROR</code> <pre><code>\u2502 .\n\u2502 \u251c\u2500\u2500 ENTREGADO\n\u2502 \u2502   \u251c\u2500\u2500 1743764835_52011.csv\n\u2502 \u2502   \u251c\u2500\u2500 1743764836_79627.csv\n\u2502 \u2502   \u251c\u2500\u2500 1743764838_81538.csv\n\u2502 \u2502   \u251c\u2500\u2500 1743764840_99141.csv\n\u2502 \u2502   \u251c\u2500\u2500 1743764845_5786.csv\n\u2502 \u2502   \u2514\u2500\u2500 1743764846_4374.csv\n\u2502 \u251c\u2500\u2500 ERROR\n\u2502 \u2502   \u2514\u2500\u2500 1743764842_6549.csv\n</code></pre></p> <p>El contenido de cada BIZUM es un <code>csv</code> con el siguiente formato: <pre><code>$ cat ENTREGADO/1743764835_52011.csv \nJos\u00e9 G\u00f3mez;441;Pago m\u00e9dico\n</code></pre></p>"},{"location":"UD05-Spark/6c.streamingbizum/#ejercicio","title":"Ejercicio","text":"<p>Crea una aplicaci\u00f3n de\u00a0Spark Streaming\u00a0que quede a la escucha de nuevos ficheros en los directorios <code>ENTREGADO</code>y <code>ERROR</code> y muestre por consola conforme vaya encontrando nuevos ficheros: - Cu\u00e1l es el bizum ENTREGADO m\u00e1s alto de cada persona. - Cu\u00e1l es el bizum con m\u00e1s importe que ha dado ERROR.</p> <p>Al finalizar, el script proporcionado muestra la misma salida que deber\u00eda mostrar tu aplicaci\u00f3n.</p> <p>Mostrar mensajes por consoa: https://www.youtube.com/watch?v=7bKX4YOaET0</p>"},{"location":"UD05-Spark/6d.streamingbizumSOL/","title":"Ejercicio Bizum","text":"<p>Trabajar con m\u00faltiples streams: https://medium.com/@kiranvutukuri/working-with-multiple-streams-in-apache-spark-part-2-46c381e31079</p> <pre><code>from pyspark.sql import SparkSession  \nfrom pyspark.sql.functions import col  \n\n# Initialize Spark Session  \nspark = SparkSession.builder \\  \n.appName(\"MultipleStreamsExample\") \\  \n.getOrCreate()  \n\n# Stream 1: Kafka Stream (Transaction Data)  \ntransaction_stream = spark.readStream \\  \n.format(\"kafka\") \\  \n.option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\  \n.option(\"subscribe\", \"transactions\") \\  \n.load() \\  \n.selectExpr(\"CAST(value AS STRING) as transaction_data\")  \n\n# Stream 2: File Stream (User Data)  \nuser_stream = spark.readStream \\  \n.format(\"csv\") \\  \n.option(\"header\", \"true\") \\  \n.schema(\"user_id STRING, user_name STRING\") \\  \n.load(\"/path/to/user/files\")  \n\n# Process each stream independently  \ntransaction_query = transaction_stream.writeStream \\  \n.format(\"console\") \\  \n.outputMode(\"append\") \\  \n.start()\n\nuser_query = user_stream.writeStream \\  \n.format(\"console\") \\  \n.outputMode(\"append\") \\  \n.start()  \n\ntransaction_query.awaitTermination()  \nuser_query.awaitTermination()\n</code></pre>"},{"location":"UD05-Spark/9.funcionesLambda/","title":"Funciones lambda","text":"<p>https://ellibrodepython.com/lambda-python#funciones-lambda</p> <p>\u201cPython lambdas are only a shorthand notation if you\u2019re too lazy to define a function.\u201d</p> <p>Lo que ser\u00eda una funci\u00f3n que suma dos n\u00fameros como la siguiente. <pre><code>def suma(a, b):\n    return a+b\n</code></pre></p> <p>Se podr\u00eda expresar en forma de una funci\u00f3n lambda de la siguiente manera. <pre><code>lambda a, b : a + b\n</code></pre></p> <p>La primera diferencia es que una funci\u00f3n lambda no tiene un nombre, y por lo tanto salvo que sea asignada a una variable, es totalmente in\u00fatil. Para ello debemos. <pre><code>suma = lambda a, b: a + b\n</code></pre></p> <p>Una vez tenemos la funci\u00f3n, es posible llamarla como si de una funci\u00f3n normal se tratase. <pre><code>suma(2, 4)\n</code></pre></p> <p>Si es una funci\u00f3n que solo queremos usar una vez, tal vez no tenga sentido almacenarla en una variable. Es posible declarar la funci\u00f3n y llamarla en la misma l\u00ednea. <pre><code>(lambda a, b: a + b)(2, 4)\n</code></pre></p>"},{"location":"UD05-Spark/9.funcionesLambda/#ejercicios-convertir-a-funcion-lambda","title":"Ejercicios, convertir a funci\u00f3n lambda","text":"<ol> <li>Crear un listado con los n\u00fameros que sean mayores o iguales a 80 <pre><code>numbers = [70, 60, 80, 90, 50,82,90,91,84,82,94,99,78,65,61,45,89,87,49,76,81,94]\n #function to check scores above 80\ndef check_score(number):\n    if number &gt;=80:\n          return True  \n\n    return False\n\n# Extract elements from the numbers list for which check_score() returns True\n#using filter function on list numbers to extract scores above 80\npercentage_score = filter(check_score, numbers)\n\n# converting to list\nscores = list(percentage_score)\nprint(scores)\n</code></pre></li> </ol> <p>SOLUCI\u00d3N <pre><code>   numbers = [70, 60, 80, 90, 50,82,90,91,84,82,94,99,78,65,61,45,89,87,49,76,81,94]\n\n   # Utilizando una funci\u00f3n lambda para verificar puntuaciones por encima de 80\n   check_score = lambda number: number &gt;= 80\n\n   # Extrayendo elementos de la lista 'numbers' para los cuales 'check_score()' devuelve True\n   # usando la funci\u00f3n filter en la lista 'numbers' para extraer puntuaciones por encima de 80\n   percentage_score = filter(check_score, numbers)\n\n   # Convirtiendo a lista\n   scores = list(percentage_score)\n   print(scores)\n</code></pre></p> <ol> <li>Devuelve la suma de todas las edades</li> </ol> <p>https://www.toppr.com/guides/python-guide/references/methods-and-functions/methods/sum/built-in/python-sum/#:~:text=Python sum () function is,floating-point numbers as well.</p> <p>https://www.geeksforgeeks.org/python-map-function/</p> <pre><code>personas = [(\"Pedro\",50),(\"Ana\",25),(\"Marta\",24),(\"Ana\",50)]\n</code></pre> <p>SOLUCI\u00d3N <pre><code>  personas = [(\"Pedro\",50),(\"Ana\",25),(\"Marta\",24),(\"Ana\",50)]\n  suma_edades = sum(map(lambda persona: persona[1], personas))\n  print(suma_edades)\n</code></pre></p>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/","title":"PySpark. Contexto, sesi\u00f3n y RDDs.","text":"<p>Fuentes: https://aitor-medrano.github.io/iabd/spark/rdd.html https://spark.apache.org/docs/latest/rdd-programming-guide.html</p>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#hola-spark","title":"Hola Spark","text":"<p>Creamos un directorio llamado pyspark</p> <p>Clonamos el repositorio https://github.com/josepgarcia/datos.git</p> <pre><code>cd ## \u00bfCreamos el directorio en nuestra home?\nmkdir pyspark\ncd pyspark\ngit clone https://github.com/josepgarcia/datos.git\n</code></pre> <p>Spark se puede ejecutar de diferentes modos: - Desde la consola (pyspark) - Desde la creaci\u00f3n de un script en python y lanz\u00e1ndolo contra un server spark (spark-submit) - Un cuaderno jupyter desde docker - ...</p> <p>Nosotros utilizaremos la tercera opci\u00f3n. Para trabajar con spark vamos a utilizar jupyther notebook a trav\u00e9s de un contenedor docker</p> <p>Iniciamos la imagen docker y ejecutamos el siguiente c\u00f3digo sobre jupyter. <pre><code>docker run -it -p 8888:8888 -v `(cmd)`:/home/jovyan/work/projects/ jupyter/pyspark-notebook\n</code></pre></p> <pre><code>docker run -it -p 8888:8888 -p 4040:4040 -p 4041:4041 -p 4042:4042 -v `(cmd)`:/home/jovyan/work/projects/ jupyter/pyspark-notebook\n</code></pre> <p>Creamos nuestro primer programa con spark, vamos a sumar los 100 primeros n\u00fameros naturales. <pre><code>from pyspark import SparkContext\nsc = SparkContext.getOrCreate()\n\n# Creamos un RDD con los 100 primeros n\u00fameros naturales\nrdd = sc.parallelize(range(100+1))\n\n# Los sumamos\nprint(\"El total es: \",rdd.sum())\n</code></pre></p> <p>https://www.w3schools.com/python/ref_func_range.asp</p> <p></p>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#crear-sesion-en-spark","title":"Crear sesi\u00f3n en Spark","text":"<p>SparkContext es el punto de entrada a Spark desde las versiones 1.x y se utiliza para crear de forma programativa RDD, acumuladores y variables broadcast en el cl\u00faster. Desde Spark 2.0, la mayor\u00eda de funcionalidades (m\u00e9todos) disponibles en SparkContext tambi\u00e9n los est\u00e1n en SparkSession. Su objeto sc est\u00e1 disponible en el spark-shell y se puede crear de forma programativa mediante la clase SparkContext.</p>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#creacion-de-un-contexto-en-spark","title":"Creaci\u00f3n de un contexto en Spark","text":"<pre><code>from pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\n</code></pre>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#creacion-de-una-sesion-en-spark","title":"Creaci\u00f3n de una sesi\u00f3n en Spark","text":"<p>SparkSession se introdujo en la versi\u00f3n 2.0 y es el punto de entrada para crear RDD, DataFrames y DataSets. El objeto spark se encuentra disponible por defecto en el spark-shell y se puede crear de forma programativa mediante el patr\u00f3n builder de SparkSession.</p> <pre><code>from pyspark.sql import SparkSession\n\n# M\u00ednimo para crear una sesi\u00f3n\nspark = SparkSession.builder.getOrCreate() \n\n# Con par\u00e1metros\nspark = SparkSession.builder \\\n.appName(\"MyAPP\") \\\n.config(\"spark.executor.memory\",\"1g\") \\\n.config(\"spark.sql.shuffle.partitions\", \"4\") \\\n.getOrCreate()\n</code></pre> <p>A trav\u00e9s de una sesi\u00f3n podemos obtener un contexto.</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n**sc = spark.sparkContext**\n</code></pre>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#finalizar","title":"Finalizar","text":"<pre><code># Terminamos con la sesi\u00f3n activa\nspark.stop()\n</code></pre>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#sparksession-vs-sparkcontext","title":"SparkSession vs SparkContext","text":"<p>SparkSession \u2192 Trabajo con Dataframes, datasets\u2026 proporciona interfaz m\u00e1s simple.</p> <p>SparkContext \u2192 Operaciones de nivel inferior como RDDs, acumuladores\u2026</p> <p>SparkContext es un singleton y solo se puede crear una vez en una aplicaci\u00f3n Spark, mientras que SparkSession se puede crear varias veces dentro de una aplicaci\u00f3n.</p> <p>SparkContext se crea utilizando SparkConf, que permite establecer varias configuraciones de Spark. SparkSession, por otro lado, no tiene un objeto de configuraci\u00f3n correspondiente, pero se pueden establecer configuraciones utilizando el m\u00e9todo <code>.config</code> de SparkSession.</p>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#hola-spark-webui","title":"Hola Spark - webUI","text":"<p>Copiar el siguiente c\u00f3digo en jupyter</p> <pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nspark\n</code></pre> <p></p> <pre><code>sc = spark.sparkContext\n\nrdd = sc.textFile(\"./DATOS/pdi_sales_small.csv\")\npaisesUnidades = rdd.map(lambda x: (x.split(\";\")[-1].strip(), x.split(\";\")[3]))\n# Le quitamos el encabezado\nheader = paisesUnidades.first()\npaisesUnidadesSinHeader = paisesUnidades.filter(lambda linea: linea != header)\n# Pasamos las unidades a un n\u00famero entero\npaisesUnidadesInt = paisesUnidadesSinHeader.map(lambda x: (x[0], int(x[1])))\n# Reducimos por el pa\u00eds y sumamos las unidades\npaisesTotalUnidades = paisesUnidadesInt.reduceByKey(lambda a,b: a+b)\npaisesTotalUnidades.collect()\n# Creamos un RDD de pares con el nombre del pa\u00eds como clave, y una lista con los valores\nventas = rdd.map(lambda x: (x.split(\";\")[-1].strip(), x.split(\";\")))\n# Quitamos el primer elemento que es el encabezado del CSV\nheader = paisesUnidades.first()\npaisesUnidadesSinHeader = paisesUnidades.filter(lambda linea: linea != header)\n# Agrupamos las ventas por nombre del pa\u00eds\npaisesAgrupados = ventas.groupByKey()\npaisesAgrupados.collect()\n</code></pre> <p>Accedemos a la UI</p> <p>Detenemos spark</p> <pre><code>spark.stop()\n</code></pre>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#rdd","title":"RDD","text":"<p>Un RDD (Resilient Distributed Datasets) es una estructura de datos que abstrae los datos para su procesamiento en paralelo.</p> <p>Antes de Spark 2.0, los RDD eran el interfaz principal para interactuar con los datos.</p> <p>Se trata de una colecci\u00f3n de elementos tolerantes a fallos que son immutables (una vez creados, no se pueden modificar) y dise\u00f1ados para su procesamiento distribuido. Cada conjunto de datos en los RDD se divide en particiones l\u00f3gicas, que se pueden calcular en diferentes nodos del cl\u00faster.</p> <p>Hay dos formas de crear un RDD:</p> <ul> <li>Paralelizando una colecci\u00f3n ya existente en nuestra aplicaci\u00f3n\u00a0Spark.</li> <li>Referenciando un dataset de un sistema externo como\u00a0HDFS,\u00a0HBase, etc...</li> </ul> <p>Sobre los RDD se pueden realizar dos tipos de operaciones:</p> <ul> <li>Acci\u00f3n: devuelven un valor tras ejecutar una computaci\u00f3n sobre el conjunto de datos.</li> <li>Transformaci\u00f3n: es una operaci\u00f3n perezosa que crea un nuevo conjunto de datos a partir de otro RDD/Dataset, tras realizar un filtrado,\u00a0join, etc...</li> </ul> <p>\u26a0\ufe0f \u00bfRDD obsoleto? Antes de la versi\u00f3n 2.0, el principal interfaz para programar en\u00a0Spark\u00a0eran los RDD. Tras la versi\u00f3n 2.0, fueron sustituidos por los\u00a0Dataset, que son RDD fuertemente tipados que adem\u00e1s est\u00e1n optimizados a bajo nivel. Aunque el interfaz RDD todav\u00eda tiene soporte, sin embargo, se recomienda el uso de los\u00a0Dataset\u00a0por su mejor rendimiento. A lo largo de estas sesiones iremos combinando ambos interfaces para conocer las similitudes y diferencias.</p>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#ejemplos","title":"Ejemplos","text":"<p>Jupyter (02-RDD.ipynb)</p> <p></p>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#actions","title":"Actions","text":"<p>Acci\u00f3n: devuelven un valor tras ejecutar una computaci\u00f3n sobre el conjunto de datos.</p> <p>A continuaci\u00f3n vamos a revisar las acciones m\u00e1s comunes. Puedes consultar todas las acciones disponibles en la documentaci\u00f3n oficial:</p> <p>https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions</p> Action Meaning reduce(func) Aggregate the elements of the dataset using a function\u00a0func\u00a0(which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. collect() Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. count() Return the number of elements in the dataset. first() Return the first element of the dataset (similar to take(1)). take(n) Return an array with the first\u00a0n\u00a0elements of the dataset. takeSample(withReplacement,\u00a0num, [seed]) Return an array with a random sample of\u00a0num\u00a0elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed. takeOrdered(n,\u00a0[ordering]) Return the first\u00a0n\u00a0elements of the RDD using either their natural order or a custom comparator. saveAsTextFile(path) Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. saveAsSequenceFile(path)(Java and Scala) Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc). saveAsObjectFile(path)(Java and Scala) Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using\u00a0<code>SparkContext.objectFile()</code>. countByKey() Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. foreach(func) Run a function\u00a0func\u00a0on each element of the dataset. This is usually done for side effects such as updating an\u00a0Accumulator\u00a0or interacting with external storage systems.Note: modifying variables other than Accumulators outside of the\u00a0<code>foreach()</code>\u00a0may result in undefined behavior. See\u00a0Understanding closures\u00a0for more details. <p></p>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#transformations","title":"Transformations","text":"<p>Transformaci\u00f3n: es una operaci\u00f3n perezosa que crea un nuevo conjunto de datos a partir de otro RDD/Dataset, tras realizar un filtrado,\u00a0join, etc\u2026</p> <p>Una transformaci\u00f3n no se \u201cejecuta\u201d hasta que se realiza una acci\u00f3n sobre ella.</p> <p>Funciones Lambda</p> <p></p> <p>flatMap</p> <p>Devuelve una lista con un elemento por cada entrada deshaciendo todas las colecciones y creando elementos individuales:</p> <p>Entrada: array de arrays \u2192 Salida: array de elementos individuales</p> <p></p>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#guardar-y-leer-rdds","title":"Guardar y leer RDDs","text":"<p> Cerramos sesi\u00f3n spark</p> <pre><code>spark.stop()\n</code></pre>"},{"location":"UD05-Spark/ZZ4.pysparkRDD/#ejercicios-rdds","title":"Ejercicios RDDs","text":"<p>\u26a0\ufe0f Creamos un RDD con los distintos datos</p> <ol> <li> <p>Filtra los elementos pares del siguiente listado <pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n</code></pre></p> </li> <li> <p>Multiplica los elementos (todos entre s\u00ed). <pre><code>[1, 2, 3, 4, 5]\n\nSol: 120\n</code></pre></p> </li> <li> <p>Ordenar de mayor a menor y de menor a mayor</p> </li> </ol> <pre><code>[5, 2, 8, 1, 9, 3]\n</code></pre> <ol> <li>Calcula el promedio de una lista de n\u00fameros</li> </ol> <pre><code>[1, 2, 3, 4, 5]\n</code></pre> <ol> <li>Duplica los elementos de una lista</li> </ol> <pre><code>[1, 2, 3, 4, 5]\n\nSalida: [1, 1, 2, 2, 3, 3, 4, 4, 5, 5]\n</code></pre> <ol> <li>Cuenta la longitud de cada palabra</li> </ol> <pre><code>[\"hola\", \"mundo\", \"en\", \"pySpark\"]\n\nSalida: [('hola', 4), ('mundo', 5), ('en', 2), ('pySpark', 7)]\n</code></pre> <ol> <li>Contar las palabras \u00fanicas de un fichero y el n\u00famero de ocurrencias (n\u00fameros, espacios, tabs, etc\u2026 no deben ser contados).     Las palabras deben ser case-insensitive (Hola y hola son la misma palabra)</li> </ol> <pre><code>file = './DATOS/magna_carta.txt'\n</code></pre> <p>PASOS:</p> <ul> <li>Dividir en palabras</li> <li>Asignar el valor 1 a cada palabra<ul> <li>[(hola, 1), (adios, 1), (dentro,1)\u2026.]</li> </ul> </li> <li>reduceByKey</li> </ul>"},{"location":"UD05-Spark/ZZ8.varios/","title":"Varios","text":""},{"location":"UD05-Spark/ZZ8.varios/#scala-hive","title":"Scala &amp;&amp; hive","text":"<pre><code>#Verifica que HADOOP_HOME y HIVE_HOME est\u00e9n correctamente configurados:\necho $HADOOP_HOME\necho $HIVE_HOME\n\n## Trabajaremos en esta carpeta\ncd $HIVE_HOME/bin \n\n#Aseg\u00farate de que el directorio de trabajo actual en HDFS sea accesible para Hive, especialmente si est\u00e1s cargando datos en tablas desde HDFS:\nhdfs dfs -mkdir -p /user/hive/warehouse\nhdfs dfs -chmod -R 777 /user/hive/warehouse\n</code></pre> <pre><code>/usr/local/hive/bin/schematool -dbType derby -initSchema\n\nwget -O /tmp/users.csv https://github.com/josepgarcia/datos/raw/refs/heads/main/ml-100k/u.user\n</code></pre> <p>Dentro de hive, creamos una tabla para los usuarios: <pre><code>1|24|M|technician|85711\n2|53|F|other|94043\n3|23|M|writer|32067\n4|24|M|technician|43537\n5|33|F|other|15213\n6|42|M|executive|98101\n7|57|M|administrator|91344\n8|36|M|administrator|05201\n9|29|M|student|01002\n</code></pre></p> <p>Ejecutamos hive y creamos la BD asociada al archivo: <code>Directorio de trabajo: $HIVE_HOME/bin</code></p> <pre><code>hive&gt; CREATE DATABASE ejemplodb;\nhive&gt; USE ejemplodb;\nhive&gt; \nCREATE TABLE users (\n    id INT,\n    age INT,\n    sex STRING,\n    profession STRING,\n    code INT)\n    ROW FORMAT DELIMITED\n    FIELDS TERMINATED BY '|'\n    STORED AS TEXTFILE;\n\n\nhive&gt; load data local inpath '/tmp/users.csv' into table users;\n\nhive&gt; select * from users;\n\nhive&gt; select * from users where sex like \"M\";\n</code></pre> <p>La base de datos se ha creado en HDFS en la carpeta /user/hive/warehouse </p> <p>Configurar Spark para acceder a las tablas de Hive: \u2022 Copia el archivo hive-site.xml al directorio de configuraci\u00f3n de Spark para que Spark pueda conectarse al metastore de Hive:</p> <pre><code>cd /usr/local/spark/conf\nln -s /usr/local/hive/conf/hive-site.xml .\n</code></pre> <p>Iniciamos spark-shell con la compatibilidad de hive: (con --packages importamos las dependencias autom\u00e1ticamente) <pre><code># Volvemos al directorio de trabajo\ncd $HIVE_HOME/bin\n\n# Iniciamos spark-shell\nspark-shell --packages org.apache.spark:spark-hive_2.12:3.3.0 \n\u23f3\n\n\nscala&gt; import org.apache.spark.sql.SparkSession\n\nscala&gt;  val spark = SparkSession.builder().appName(\"Spark Hive Test\").config(\"spark.sql.catalogImplementation\", \"hive\").enableHiveSupport().getOrCreate()\n\nscala&gt; spark.sql(\"SHOW DATABASES\").show()\n\nscala&gt; spark.sql(\"USE ejemplodb\")\n\nscala&gt; spark.sql(\"SHOW TABLES\").show()\n\nscala&gt; spark.sql(\"SELECT * FROM users\").show()\n</code></pre></p> <p>Ejemplo de consultas: <pre><code>val professionCount = spark.sql(\"\"\"\n  SELECT profession, COUNT(*) AS total_users\n  FROM users\n  GROUP BY profession\n  ORDER BY total_users DESC\n\"\"\")\nprofessionCount.show()\n\n\nval genderCount = spark.sql(\"\"\"\n\u00a0 SELECT sex, COUNT(*) AS total_users\n\u00a0 FROM users\n\u00a0 GROUP BY sex\n\"\"\")\ngenderCount.show()\n</code></pre></p>"},{"location":"UD05-Spark/ZZ8.varios/#pyspark-hive","title":"Pyspark &amp;&amp; hive","text":"<p>Atenci\u00f3n</p> <p>Si lanzamos pyspark desde fuera del directorio de trabajo que hemos indicado en el principio ($HIVE_HOME/bin) solo nos encuentra la base de datos default.</p> <pre><code>pyspark --packages org.apache.spark:spark-hive_2.12:3.3.0\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n\u00a0 \u00a0 .appName(\"Consulta Hive desde PySpark\") \\\n\u00a0 \u00a0 .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n\u00a0 \u00a0 .enableHiveSupport() \\\n\u00a0 \u00a0 .getOrCreate()\n\nspark.sql(\"SHOW DATABASES\").show()\n\nspark.sql(\"USE ejemplodb\")\nspark.sql(\"SELECT * FROM users\").show()\n</code></pre>"},{"location":"UD05-Spark/ZZ8.varios/#spark-submit-hive","title":"Spark-submit &amp;&amp; hive","text":"<p>Atenci\u00f3n</p> <p>Si lanzamos spark-submit desde fuera del directorio de trabajo que hemos indicado en el principio ($HIVE_HOME/bin) solo nos encuentra la base de datos default.</p> <pre><code>'''consulta_hive.py'''\n\nfrom pyspark.sql import SparkSession\n\n# Crear una SparkSession con soporte para Hive\nspark = SparkSession.builder \\\n    .appName(\"Consulta Hive desde Spark Submit\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Mostrar las bases de datos disponibles\ndatabases = spark.sql(\"SHOW DATABASES\")\ndatabases.show()\n\n# Cambiar a la base de datos deseada y consultar\nspark.sql(\"USE ejemplodb\")\nusers = spark.sql(\"SELECT * FROM users\")\nusers.show()\n\n# Ejemplo de consulta avanzada: Contar usuarios por profesi\u00f3n\nprofession_count = spark.sql(\"\"\"\n    SELECT profession, COUNT(*) AS total_users\n    FROM users\n    GROUP BY profession\n    ORDER BY total_users DESC\n\"\"\")\nprofession_count.show()\n</code></pre> <pre><code># Submit\n\nspark-submit --master local --packages org.apache.spark:spark-hive_2.12:3.3.0 --conf spark.sql.catalogImplementation=hive consulta_hive.py\n</code></pre>"},{"location":"UD05-Spark/ZZ8.varios/#ejercicio","title":"Ejercicio","text":"<p>Realiza las configuraciones necesarias para que la base de datos \"ejemplodb\" sea accesible desde cualquier ubicaci\u00f3n (que no sea necesario estar en <code>$HIVE_HOME/bin</code>). Pista: problema de configuraci\u00f3n del metastore</p>"},{"location":"UD05-Spark/_TODO/","title":"TODO","text":"<p>Spark scala https://www.udemy.com/course/big-data-hadoop-spark-project/?couponCode=NEWYEARCAREER</p> <p>https://github.com/CarlosRyu98/Spark</p> <p>SPARK CURSO https://www.udemy.com/course/big-data-y-spark-ingenieria-de-datos-con-python-y-pyspark/?couponCode=NEWYEARCAREER</p> <p>Data Science:Hands-on Diabetes Prediction with Pyspark MLlib https://www.udemy.com/course/data-science-hands-on-diabetes-prediction-with-pyspark-mllib/?couponCode=NEWYEARCAREER</p> <p>Spark y Scala en Databricks: Big Data e ingenier\u00eda de datos https://www.udemy.com/course/spark-y-scala-en-databricks-big-data-e-ingenieria-de-datos/?couponCode=NEWYEARCAREER</p> <p>Spark https://www.udemy.com/course/pyspark-python-spark-hadoop-coding-framework-testing/?couponCode=NEWYEARCAREER</p> <p>Big Data Engineering Project: PySpark, Databricks and Azure https://www.udemy.com/course/pyspark-databricks-azure-projects/?couponCode=NEWYEARCAREER</p>"},{"location":"UD06-Monitoring/1.Prometheus/","title":"Prometheus","text":"<p> https://prometheus.io/</p>"},{"location":"UD06-Monitoring/1.Prometheus/#que-es-prometheus","title":"\u00bfQu\u00e9 es prometheus?","text":"<p>RESUMEN Prometheus es un sistema de monitoreo y alerta de c\u00f3digo abierto que recopila y almacena m\u00e9tricas de aplicaciones e infraestructuras. Es una base de datos de series temporales que permite monitorizar el estado de los sistemas, detectar problemas y alertar cuando ocurren.</p> <p>FUNCIONAMIENTO Prometheus va recorriendo \"escrapeando\" diferentes sistemas que le indiquemos los datos de m\u00e9tricas a trav\u00e9s de una API y los almacena. Con prometheus podemos consultar estas m\u00e9tricas.</p> <p>Recopilaci\u00f3n de m\u00e9tricas: Prometheus utiliza un modelo de extracci\u00f3n (pull) para recopilar m\u00e9tricas de los sistemas que supervisa. Esto significa que el servidor de Prometheus solicita peri\u00f3dicamente m\u00e9tricas a los objetivos (aplicaciones, servidores, etc.). </p> <p>Almacenamiento de series temporales: Las m\u00e9tricas recopiladas se almacenan en una base de datos de series temporales, lo que permite analizar los datos a lo largo del tiempo. </p> <p>Lenguaje de consulta (PromQL): Prometheus ofrece un lenguaje de consulta (PromQL) para consultar las m\u00e9tricas almacenadas, lo que permite crear gr\u00e1ficos, alertas y an\u00e1lisis ad hoc. </p> <p>Alertas: Prometheus incluye un sistema de alertas que permite configurar reglas para alertar cuando se detectan problemas o cambios en las m\u00e9tricas. </p>"},{"location":"UD06-Monitoring/1.Prometheus/#componentes","title":"Componentes","text":"<p>Prometheus est\u00e1 compuesto por varios componentes, incluyendo: - Servidor Prometheus: El componente central que almacena las m\u00e9tricas y proporciona la API de consulta.  - Exportadores: Agentes de software que recopilan m\u00e9tricas de diferentes sistemas y las exponen en un formato compatible con Prometheus.  - Alertmanager: Un servicio que gestiona las alertas y las notifica a los usuarios.  - Pushgateway: Un componente que permite a los servicios enviar m\u00e9tricas a Prometheus sin tener que implementar el modelo de extracci\u00f3n. </p>"},{"location":"UD06-Monitoring/1.Prometheus/#uso","title":"Uso","text":"<p>Prometheus se utiliza ampliamente en entornos de desarrollo, operaciones y DevOps para monitorizar la salud de los sistemas, detectar problemas de rendimiento, predecir fallas y mejorar la eficiencia. </p> <p>Hay todo un ecosistema de bibliotecas y clientes para exportar m\u00e9tricas compatibles con Prometheus desde multitud de sistemas. https://prometheus.io/docs/instrumenting/clientlibs/</p>"},{"location":"UD06-Monitoring/1.Prometheus/#metricas","title":"M\u00e9tricas","text":"<p>Ejemplo de m\u00e9tricas a las que accede prometheus.</p> <p><code>nombre_etiqueta{LABELS}</code></p> <p></p>"},{"location":"UD06-Monitoring/1.Prometheus/#tipos-de-metricas","title":"Tipos de m\u00e9tricas","text":"<p>https://prometheus.io/docs/tutorials/understanding_metric_types/</p>"},{"location":"UD06-Monitoring/1.Prometheus/#contador","title":"Contador","text":"<p>Es un valor m\u00e9trico que solo puede aumentar o restablecerse, es decir, el valor no puede ir a menos. Se puede utilizar para m\u00e9tricas como el n\u00famero de solicitudes, n\u00famero de errores, etc.</p> <p>Escribe la consulta a continuaci\u00f3n en la barra de consultas y haga clic en ejecutar. <code>go_gc_duration_seconds_count</code>  La funci\u00f3n rate() en PromQL toma el historial de m\u00e9tricas durante un per\u00edodo de tiempo y calcula qu\u00e9 tan r\u00e1pido aumenta el valor por segundo. La tasa es aplicable solo en valores de contador. <code>rate(go_gc_duration_seconds_count[5m])</code> </p>"},{"location":"UD06-Monitoring/1.Prometheus/#gauge-indicador","title":"Gauge (indicador)","text":"<p>Es un n\u00famero que puede subir o bajar. Se puede usar para m\u00e9tricas como el n\u00famero de eventos en una cola, etc. <code>go_memstats_heap_alloc_bytes</code>  Funciones promQL como max_over_time, min_over_time y avg_over_time se puede utilizar en m\u00e9tricas de gauge.</p>"},{"location":"UD06-Monitoring/1.Prometheus/#histograma","title":"Histograma","text":"<p>Se utilizan para ver distribuciones estad\u00edsticas.</p>"},{"location":"UD06-Monitoring/1.Prometheus/#summary","title":"Summary","text":"<p>Res\u00famenes, funcionan como los histogramas pero solamente te dan el valor total. Por ejemplo, nos pueden dar la mediana o m\u00e1ximo o m\u00ednimo de por ejemplo el uso de la CPU.</p>"},{"location":"UD06-Monitoring/1.Prometheus/#ejemplos","title":"Ejemplos","text":"<p>Clonamos el siguiente repositorio para los ejemplos:</p> <p><code>https://github.com/josepgarcia/monitoring-prometheus-graphana</code></p>"},{"location":"UD06-Monitoring/1.Prometheus/#ejemplo-1-instalacion-basica","title":"Ejemplo 1. Instalaci\u00f3n b\u00e1sica.","text":"<p>Abre y analiza los archivos:  <code>docker-compose.yaml</code>y <code>prometheus.yml</code></p> <p>Levanta el contenedor con: <code>docker compose up</code></p> <p>Accede a prometheus  http://localhost:9090/</p> <p>Las m\u00e9tricas que encontramos son las del propio prometheus: http://localhost:9090/metrics</p> <p>Vemos las m\u00e9tricas que tenemos disponibles.  Encontramos diferentes tipos de m\u00e9tricas: </p> <p>Ejemplos de consultas que podemos realizar:</p> <p>Intervalos entre los diferentes \"scrapeos\" <code>prometheus_target_interval_length_seconds</code></p> <p>Si estamos interesados en las de 0.99, las podemos filtrar con: <code>prometheus_target_interval_length_seconds{quantile=\"0.99\"}</code></p> <p>En graph: rate(prometheus_tsdb_head_chunks_created_total[1m])`</p>"},{"location":"UD06-Monitoring/1.Prometheus/#ejemplo-2-cadvisor","title":"Ejemplo 2. cadvisor.","text":"<p>https://www.apptio.com/topics/kubernetes/devops-tools/cadvisor/</p> <p>cAdvisor es un software de Google, escrito en Go, y programado espec\u00edficamente para la captura de m\u00e9tricas de contenedores; necesita acceso a varios directorios del host (que mapearemos con bind-mounts en Docker) para capturar los datos. cAdvisor expone el puerto 8080 (interfaz web y REST api) y por defecto permite a Prometheus acceder a varias m\u00e9tricas.</p> <p>cadvisor: http://localhost:8080/ cadvisor metrics: http://localhost:8080/metrics</p> <p>Podemos consultar m\u00e9tricas como: <code>cadvisor_version_info</code></p> <p><code>container_start_time_seconds{name=\"cadvisor\"}</code></p> <code>rate(container_cpu_usage_seconds_total{name=\"redis\"}[1m])</code> The\u00a0cgroup's CPU usage in the last minute The\u00a0<code>redis</code>\u00a0container <code>container_memory_usage_bytes{name=\"redis\"}</code> The cgroup's total memory usage (in bytes) The\u00a0<code>redis</code>\u00a0container <code>rate(container_network_transmit_bytes_total[1m])</code> Bytes transmitted over the network by the container per second in the last minute All containers <code>rate(container_network_receive_bytes_total[1m])</code> Bytes received over the network by the container per second in the last minute All containers"},{"location":"UD06-Monitoring/1.Prometheus/#ejemplo-3-graphana","title":"Ejemplo 3. graphana.","text":""},{"location":"UD06-Monitoring/1.Prometheus/#netdata","title":"netdata","text":"<p>https://github.com/netdata/netdata</p>"},{"location":"UD06-Monitoring/1.Prometheus/#promql","title":"promQL","text":""},{"location":"UD06-Monitoring/1.Prometheus/#enlaces","title":"Enlaces","text":"<p>https://mxulises.medium.com/simple-prometheus-setup-on-docker-compose-f702d5f98579 https://comacero.com/posts/notes_docker_mon/</p> <p>[^1]: </p>"},{"location":"UD08-cloud/_TODO/","title":"TODO","text":"<p>https://aitor-medrano.github.io/iabd/cloud/emr.html</p>"},{"location":"UD09-Herramientas/1.faker/","title":"Faker","text":""},{"location":"UD09-Herramientas/1.faker/#instalacion","title":"Instalaci\u00f3n","text":"<p>Si necesitamos generar muchos datos, es muy \u00fatil emplear una librer\u00eda como\u00a0Faker\u00a0para generar datos sint\u00e9ticos.</p> <p>Primero hemos de instalarla mediante pip:</p> <pre><code>pip3 install faker\n</code></pre>"},{"location":"UD09-Herramientas/1.faker/#ejemplo","title":"Ejemplo","text":"<pre><code>'''hola.py'''\nfrom faker import Faker\n\nfake = Faker()\nfake = Faker('es_ES')   # cambiamos el locale a espa\u00f1ol\n\nprint(\"Nombre:\", fake.name())\nprint(\"Direcci\u00f3n:\", fake.address())\nprint(\"Nombre de hombre:\", fake.first_name_male())\nprint(\"N\u00famero de tel\u00e9fono:\", fake.phone_number())\nprint(\"Color:\", fake.color_name())\nprint(\"Fecha:\", fake.date())\nprint(\"Email:\", fake.email())\nprint(\"Frase de 10 palabras\", fake.sentence(nb_words=10))\n</code></pre> <pre><code>$ python3 hola.py\n\nNombre: Yaiza Rico-Carbonell\nDirecci\u00f3n: Ca\u00f1ada de Maximiliano Gal\u00e1n 4\nSegovia, 08077\nNombre de hombre: Nando\nN\u00famero de tel\u00e9fono: +34 888 44 88 30\nColor: Amarillo dorado claro\nFecha: 1989-02-18\nEmail: pinolcalixto@example.com\nFrase de 10 palabras Labore nesciunt placeat nam soluta atque dolores dolor provident qui.\n</code></pre> <p>https://faker.readthedocs.io/en/master/providers.html</p> <p>Los diferentes grupos de datos que genera se agrupan en\u00a0Providers: de direcci\u00f3n, fechas, relacionados con internet, bancarios, c\u00f3digos de barra, isbn, etc...</p> <p>Al trabajar con el idioma en espa\u00f1ol, puede que algunos m\u00e9todos no funcionen (m\u00e1s que no funcionar, posiblemente tengan otro nombre). Es recomendable comprobar las opciones disponibles en\u00a0https://faker.readthedocs.io/en/master/locales/es_ES.html</p>"},{"location":"UD09-Herramientas/1.faker/#otros-lenguajes","title":"Otros lenguajes","text":"<p>https://fakerjs.dev/guide/ https://www.npmjs.com/package/@faker-js/faker https://fakerphp.org</p>"},{"location":"UD09-Herramientas/1.faker/#ejercicios","title":"Ejercicios","text":"<ol> <li>Modifica el ejemplo anterior para generar un CSV con 100 personas.</li> <li>Modifica el ejemplo anterior para generar un JSON con 100 personas.</li> <li>Genera un JSON con los siguientes datos: Nombre | Apellido1 | Apellido2 | NIF | N\u00famero de cuenta (iban) | nombre empresa | pa\u00eds empresa | ciudad empresa</li> </ol> <p>Ejercicio 1 <pre><code>from faker import Faker\nimport csv\n\noutput = open('datosFaker.csv', 'w')\n\nfake = Faker('es_ES')   # cambiamos el locale a espa\u00f1ol\nheader = ['nombre', 'edad', 'calle', 'ciudad',\n        'provincia', 'cp', 'longitud', 'latitud']\nmywriter = csv.writer(output)\nmywriter.writerow(header)\n\nfor r in range(1000):\n    mywriter.writerow([fake.name(),\n                    fake.random_int(min=18, max=80, step=1),\n                    fake.street_address(),\n                    fake.city(),\n                    fake.state(),\n                    fake.postcode(),\n                    fake.longitude(),\n                    fake.latitude()])\noutput.close()\n</code></pre></p> <p>Ejercicio 2 <pre><code>from faker import Faker\nimport json\n\nfake = Faker('es_ES')   # cambiamos el locale a espa\u00f1ol\n\n# Preparamos los datos\ndatos = {}\ndatos['registros'] = []\n\nfor x in range(1000):\n    persona = {\"datos\": fake.name(),\n            \"edad\": fake.random_int(min=18, max=80, step=1),\n            \"calle\": fake.street_address(),\n            \"ciudad\": fake.city(),\n            \"provincia\": fake.state(),\n            \"cp\": fake.postcode(),\n            \"longitud\": float(fake.longitude()),\n            \"latitud\": float(fake.latitude())}\n    datos['registros'].append(persona)\n\n# Los metemos en el fichero\noutput = open('datosFaker.json', 'w')\njson.dump(datos, output)\n</code></pre></p>"},{"location":"UD09-Herramientas/2.domotica/","title":"2.domotica","text":"<p>https://hackmd.io/@phdunimed/rJUh4GlZu</p> <p>https://wokwi.com/projects/322577683855704658</p>"},{"location":"UD09-Herramientas/3.kafka/","title":"Procesamiento de datos en tiempo real","text":""},{"location":"UD09-Herramientas/3.kafka/#procesamiento-en-lote-vs-procesamiento-en-tiempo-real","title":"Procesamiento en Lote vs. Procesamiento en Tiempo Real","text":"<p>Procesamiento en lote (Batch Processing):     \u2022 Se env\u00eda un trabajo (por ejemplo, MapReduce, Spark o Hive) usando Yarn.     \u2022 El sistema ejecuta el trabajo y devuelve una respuesta en pantalla o escribe la salida en HDFS.     \u2022 Se ejecuta en intervalos regulares (cada hora, cada d\u00eda, etc.).     \u2022 Es ideal para comenzar con Big Data, pero no es adecuado cuando se requiere procesar datos en intervalos muy cortos.</p> <p>Procesamiento en tiempo real (Real-Time Processing):         \u2022 Se utiliza cuando hay necesidad de procesar los datos casi inmediatamente despu\u00e9s de que llegan.         \u2022 Requiere tecnolog\u00edas y arquitecturas espec\u00edficas para manejar flujos constantes de datos.</p>"},{"location":"UD09-Herramientas/3.kafka/#tecnologias-necesarias-para-procesamiento-en-tiempo-real","title":"Tecnolog\u00edas Necesarias para Procesamiento en Tiempo Real","text":"<p>Ingesta y almacenamiento temporal de datos:     \u2022 Los datos entrantes deben almacenarse temporalmente antes de ser procesados. Problema con HDFS:     \u2022 HDFS no es eficiente para leer eventos individuales (como una l\u00ednea o un registro).     \u2022 Est\u00e1 dise\u00f1ado para leer bloques completos de datos.</p> <p>SOLUCI\u00d3N Kafka:     \u2022 Kafka es un sistema de mensajer\u00eda basado en el modelo publicaci\u00f3n-suscripci\u00f3n (pub-sub).     \u2022 Act\u00faa como un log distribuido que almacena eventos temporalmente en un buffer mientras esperan para ser procesados.</p>"},{"location":"UD09-Herramientas/3.kafka/#procesamiento-de-eventos-en-tiempo-real","title":"Procesamiento de Eventos en Tiempo Real","text":"<p>Los servicios que procesan los datos deben estar en ejecuci\u00f3n continua dentro del cl\u00faster de Hadoop y deben recuperar datos desde sistemas de colas como Kafka. Diferentes enfoques para el procesamiento de eventos: \u2022 Storm: Procesa cada evento de manera individual. \u2022 Storm + Trident: Permite agrupar eventos (batching) y procesar m\u00faltiples eventos a la vez. \u2022 Spark Streaming: Utiliza un enfoque de micro-lotes (micro-batching), agrupando eventos dentro de intervalos de tiempo cortos (por ejemplo, cada pocos segundos).</p>"},{"location":"UD09-Herramientas/3.kafka/#almacenamiento-persistente-de-datos-procesados","title":"Almacenamiento Persistente de Datos Procesados","text":"<p>Despu\u00e9s de procesar los datos, deben almacenarse para consultas posteriores.</p> <p>Problema con HDFS: \u2022 No es adecuado para manejar muchos archivos peque\u00f1os. \u2022 Una posible soluci\u00f3n ser\u00eda anexar eventos a un \u00fanico archivo grande, pero esto hace que la lectura de un evento espec\u00edfico sea dif\u00edcil.</p> <p>Soluciones ideales: \u2022 Utilizar almacenes de datos distribuidos como: \u2022 HBase: Dise\u00f1ado para escritura r\u00e1pida y lectura eficiente cuando se conoce la clave de fila. \u2022 Accumulo: Similar a HBase, pero con funcionalidades adicionales como control de acceso granular. </p> <p>https://aitor-medrano.github.io/iabd/dataflow/kafka1.html</p>"},{"location":"UD09-Herramientas/3.kafka/#kafka","title":"Kafka","text":"<p>Apache Kafka\u00a0es, en pocas palabras, un\u00a0middleware\u00a0de mensajer\u00eda entre sistemas heterog\u00e9neos, el cual, mediante un sistema de colas (topics, para ser concreto) facilita la comunicaci\u00f3n as\u00edncrona, desacoplando los flujos de datos de los sistemas que los producen o consumen. Funciona como un\u00a0broker\u00a0de mensajes, encargado de enrutar los mensajes entre los clientes de un modo muy r\u00e1pido.</p> <p>Supongamos que tenemos m\u00faltiples generadores de datos, ya sean servidores web, de bases de datos, un servidor de chat y que todos ellos tienen que almacenar sus datos en m\u00faltiples destinos, como pueden ser logs, m\u00e9tricas de rendimiento y monitorizaci\u00f3n, el carrito de la compra o los fallos ocurridos, lo que puede provocar una serie de dependencias de unos con otros. Para evitarlo,\u00a0Kafka\u00a0viene al rescate conectando todos los generadores de datos (productores) a Kafka y a su vez, a todos los consumidores de estos datos.</p> <p>En concreto, se trata de una plataforma\u00a0open source distribuida\u00a0de\u00a0transmisi\u00f3n de eventos/mensajes\u00a0en tiempo real con almacenamiento duradero y que proporciona de base un alto rendimiento (capaz de manejar billones de peticiones al d\u00eda, con una latencia inferior a 10ms), tolerancia a fallos, disponibilidad y escalabilidad horizontal (mediante cientos de nodos).</p> <p>M\u00e1s del 80% de las 100 compa\u00f1\u00edas m\u00e1s importantes de EEUU utilizan Kafka: Uber, Twitter, Netflix, Spotify, Blizzard, LinkedIn, Spotify, y PayPal procesan cada d\u00eda sus mensajes con Kafka.</p> <p>https://kafka.apache.org/powered-by</p> <p>Como sistema de mensajes, sigue un modelo publicador-suscriptor. Su arquitectura tiene dos directivas claras:</p> <ul> <li>No bloquear los productores (para poder gestionar la\u00a0back pressure, la cual sucede cuando un publicador produce m\u00e1s elementos de los que un suscriptor puede consumir).</li> <li>Aislar los productores y los consumidores, de manera que los productores y los consumidores no se conocen.</li> </ul> <p>A d\u00eda de hoy,\u00a0Apache Kafka\u00a0se utiliza, adem\u00e1s de como un sistema de mensajer\u00eda, para ingestar datos, realizar procesado de datos en streaming y anal\u00edtica de datos en tiempo real, as\u00ed como en arquitectura de microservicios y sistemas IOT.</p>"},{"location":"UD09-Herramientas/3.kafka/#publicador-suscriptor-productor-consumidor","title":"Publicador / Suscriptor (Productor / consumidor)","text":"<p>Antes de entrar en detalle sobre\u00a0Kafka, hay que conocer el modelo publicador/suscriptor. Este patr\u00f3n tambi\u00e9n se conoce como\u00a0publish / subscribe\u00a0o\u00a0productor / consumidor.</p> <p>Hay tres elementos que hay que tener realmente claros:</p> <ul> <li>Publicador (publisher\u00a0/ productor / emisor): genera un dato y lo coloca en un\u00a0topic\u00a0como un mensaje.</li> <li>Topic (tema): almac\u00e9n temporal/duradero que guarda los mensajes funcionando como una cola.</li> <li>Suscriptor (subscriber\u00a0/ consumidor / receptor): recibe el mensaje.</li> </ul> <p>Cabe destacar que un productor no se comunica nunca directamente con un consumidor, siempre lo hace a trav\u00e9s de un\u00a0topic: https://www.youtube.com/watch?v=wO6DCLU4uxE</p> <p>Kafka garantiza que... - Los mensajes se a\u00f1aden a una partici\u00f3n/topic\u00a0en el orden en el que se env\u00edan - Los consumidores leen los mensajes en el orden en que se almacenaron en la partici\u00f3n/topic - Con un factor de replicaci\u00f3n N, los productores y consumidores pueden soportar que se caigan N-1 brokers.    - Por ejemplo, con un factor de replicaci\u00f3n de 3 (el cual es un valor muy apropiado), podemos tener un nodo detenido para mantenimiento y podemos permitirnos que otro de los nodos se caiga de forma inesperada. - Mientras el n\u00famero de particiones de un\u00a0topic\u00a0permanezca constante (no se hayan creado nuevas particiones), la misma clave implicar\u00e1 que los mensajes vayan a la misma partici\u00f3n.</p>"},{"location":"UD09-Herramientas/3.kafka/#zookeeper","title":"Zookeeper","text":"<p>Es un servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas dentro del ecosistema de\u00a0Apache. No s\u00f3lo se utiliza en\u00a0Hadoop, pero es muy \u00fatil ya que elimina la complejidad de la gesti\u00f3n distribuida de la plataforma.</p> <p>En el caso de\u00a0Kafka,\u00a0Zookeeper: - gestiona los\u00a0brokers\u00a0(manteniendo una lista de ellos). - ayuda en la elecci\u00f3n de la partici\u00f3n l\u00edder - env\u00eda notificaciones a\u00a0Kafka\u00a0cuando hay alg\u00fan cambio (por ejemplo, se crea un\u00a0topic, se cae un broker, se recupera un\u00a0broker, al eliminar un\u00a0topic, etc...).</p> <p>Por todo ello,\u00a0Kafka\u00a0no puede funcionar sin\u00a0Zookeeper.</p> <p>En un entorno real, se instalan un n\u00famero impar de servidores\u00a0Zookeeper\u00a0(3, 5, 7). Para su gesti\u00f3n,\u00a0Zookeeper\u00a0define un l\u00edder (gestiona las escrituras) y el resto de los servidores funcionan como r\u00e9plicas de lectura. </p> <p>Pese a su dependencia, los productores y consumidores no interact\u00faan nunca con\u00a0Zookeeper, s\u00f3lo lo hacen con\u00a0Kafka.</p> <p>Aunque podemos decir que todav\u00eda no es la opci\u00f3n m\u00e1s recomendable en producci\u00f3n, tambi\u00e9n podemos utilizar una instalaci\u00f3n de Kafka sin Zookeeper haciendo uso de\u00a0Kraft, el cual ofrece un nuevo protocolo de consenso y evita tener una infraestructura extra para Zookeeper.</p>"},{"location":"UD09-Herramientas/3.kafka/#instalacion","title":"Instalaci\u00f3n","text":""},{"location":"UD09-Herramientas/3.kafka/#en-local","title":"En local","text":"<p>https://kafka.apache.org/quickstart</p>"},{"location":"UD09-Herramientas/3.kafka/#contenedores","title":"Contenedores","text":"<p>Apache Kafka con docker compose (producer y consumer)</p> <p>docker-compose.yml <pre><code>version: '3.8'\n\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    container_name: zookeeper\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - 2181:2181\n\n  kafka-broker-1:\n    image: confluentinc/cp-kafka:latest\n    container_name: kafka-broker-1\n    depends_on:\n      - zookeeper\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://broker:29092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n    ports:\n      - \"9092:9092\"\n</code></pre></p> <pre><code>docker-compose up\n</code></pre> <p>Abrimos 2 terminales:</p> <pre><code># Terminal 1\ndocker exec -it kafka-broker-1 bash\n# Terminal 2\ndocker exec -it kafka-broker-1 bash\n\n# Terminal 1, creamos topic test1\n$ kafka-topics --bootstrap-server kafka-broker-1:9092 --create --topic test1\n## (productor) Escribimos mensajes dentros de este topic\n$ kafka-console-producer  --bootstrap-server kafka-broker-1:9092 --topic test1\n&gt; hola\n&gt; uno \n&gt; dos \n&gt; tres\n\n# Terminal 2, creamos consumidor\n## (consumidor)\nkafka-console-consumer --bootstrap-server kafka-broker-1:9092 --topic test1 --from-beginning\n</code></pre> <p>Abrimos otro terminal</p> <pre><code>docker exec -it kafka-broker-1 bash\n\n# Creamos nuevo topic\n$ kafka-topics --bootstrap-server kafka-broker-1:9092 --create --topic test2\n\n# Listamos los topics disponibles\n$ kafka-topics --list --bootstrap-server kafka-broker-1:9092\n\n# Mandamos mensajes al topic \"test2\"\nkafka-console-producer  --bootstrap-server kafka-broker-1:9092 --topic test2\n\n# No se muestran por la terminal2, ya que se encuentra escuchando\n# otra \"categor\u00eda\"\n</code></pre>"},{"location":"UD09-Herramientas/3.kafka/#ejemplos","title":"Ejemplos","text":"<p>Posibles ERRORES se solucionan con la versi\u00f3n *-ng :</p> <pre><code>pip install kafka-python-ng\n</code></pre>"},{"location":"UD09-Herramientas/3.kafka/#producer-desde-python","title":"Producer desde python","text":"<pre><code>'''producer.py'''\n'''pip install kafka-python'''\n\nfrom kafka import KafkaProducer\nimport time\n\n# Configuraci\u00f3n del servidor Kafka y el tema al que enviaremos mensajes\nbootstrap_servers = ['127.0.0.1:9092']\ntopic_name = 'test1'\n\n# Crear un productor de Kafka\nproducer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n\n# Enviar mensajes al tema\nfor i in range(1, 6):\n        mensaje = f\"Mensaje {i}\"\n        producer.send(topic_name, mensaje.encode('utf-8'))\n        print(f\"Mensaje enviado: {mensaje}\")\n        time.sleep(1)  # Esperar 1 segundo entre cada mensaje\n\nproducer.close()\n</code></pre>"},{"location":"UD09-Herramientas/3.kafka/#consumer-desde-python","title":"Consumer desde python","text":"<pre><code>'''consumer.py'''\n'''pip install kafka-python'''\n\nfrom kafka import KafkaConsumer\n\n# Configuraci\u00f3n del servidor Kafka y el tema al que nos conectaremos\nbootstrap_servers = ['localhost:9092']\ntopic_name = 'mi_tema'\n\n# Crear un consumidor de Kafka y configurarlo para leer desde el inicio del tema\nconsumer = KafkaConsumer(topic_name,\n                         group_id='grupo1',\n                         bootstrap_servers=bootstrap_servers,\n                         auto_offset_reset='earliest')\n\n# Leer mensajes del tema desde el inicio\nfor mensaje in consumer:\n    print(f\"Mensaje recibido: {mensaje.value.decode('utf-8')}\")\n</code></pre>"},{"location":"UD09-Herramientas/3.kafka/#producer-consumer-desde-node","title":"Producer - Consumer desde node","text":"<pre><code>npm init\n\nnpm install kafka-node\n</code></pre> <pre><code>const kafka = require('kafka-node');\n\nconst client = new kafka.KafkaClient({kafkaHost: '127.0.0.1:9092'})\n\n/* Consumer */\nvar consumer = new kafka.Consumer(client, [{topic:'test1'}])\nconsumer.on('message', function(message) {\n  console.log(message)\n});\n\n/* Producer */\nvar producer = new kafka.Producer(client);\nproducer.on('ready', function () {\n    setInterval(function() {\n            producer.send( [ { topic: \"test1\", messages: \"Mensaje autom\u00e1tico cada 5 seg.\" } ], function (err,data) {} );\n          }, 5000);\n});\n</code></pre> <pre><code>node index.js\n\n{\n  topic: 'test1',\n  value: 'hola',\n  offset: 0,\n  partition: 0,\n  highWaterOffset: 7,\n  key: null\n}\n{\n  topic: 'test1',\n  value: 'uno dos',\n  offset: 1,\n  partition: 0,\n  highWaterOffset: 7,\n  key: null\n}\n</code></pre>"},{"location":"UD09-Herramientas/3.kafka/#actividades","title":"Actividades","text":"\u2705 **Entregar AULES**   <p>A partir de un\u00a0topic\u00a0denominado\u00a0<code>iabd-python-topic</code>\u00a0y utilizando\u00a0Python\u00a0crea: 1. un productor que env\u00ede datos de personas cada 10 segundos al\u00a0topic. (5 datos por persona) 2. un consumidor que reciba las personas y, mediante\u00a0PyMongo, las inserte en\u00a0MongoDB\u00a0en una colecci\u00f3n llamada\u00a0<code>kafka_personas</code>.</p> <p>Utiliza la librer\u00eda FAKER para generar los datos en python.</p>"},{"location":"UD09-Herramientas/3.kafka/#pipeline-basico-de-kafka-y-spark-streaming-para-procesar-datos-en-tiempo-real","title":"Pipeline b\u00e1sico de Kafka y Spark Streaming para procesar datos en tiempo real","text":"<pre><code># Iniciar Zookeeper\nbin/zookeeper-server-start.sh config/zookeeper.properties\n# Iniciar Kafka\nbin/kafka-server-start.sh config/server.properties\n\n2. **Crear un tema (topic):**\n\u2022 Un **topic** es un canal donde Kafka almacena mensajes.\nbin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n\u2022 Usa el productor de Kafka para enviar mensajes.\nbin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092\n</code></pre> <p>Paso 2: Crear un Programa de Spark Streaming Este programa en Python (usando PySpark) consumir\u00e1 mensajes de Kafka y los procesar\u00e1 en micro-lotes. <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StringType, StructType\n\n# Crear la sesi\u00f3n de Spark\nspark = SparkSession.builder \\\n    .appName(\"KafkaSparkStreaming\") \\\n    .getOrCreate()\n\n# Leer datos en streaming desde Kafka\ndf = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"test-topic\") \\\n    .load()\n\n# Definir el esquema de los mensajes\nschema = StructType().add(\"key\", StringType()).add(\"value\", StringType())\n\n# Procesar los mensajes\nmessages = df.selectExpr(\"CAST(value AS STRING)\").select(from_json(col(\"value\"), schema).alias(\"data\"))\n\n# Escribir la salida en consola\nquery = messages.writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"console\") \\\n    .start()\n\nquery.awaitTermination()\n</code></pre> Paso 3: Enviar Datos al Pipeline \u2022 Usa el productor de Kafka para enviar mensajes. <pre><code>bin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092\n</code></pre> Escribe mensajes como: {\"key\": \"sensor1\", \"value\": \"temperature:25\"}</p> <p>\u2022 Observa c\u00f3mo Spark Streaming procesa estos mensajes en la consola.</p> <p>Diagrama Arquitect\u00f3nico: Pipeline de Procesamiento en Tiempo Real Data Sources (IoT devices, APIs, etc.)       \u2193 [ Kafka (buffer)]       \u2193 [ Spark Streaming (micro-batching) ]       \u2193 [ Processed Data Stored in HBase / Accumulo / Data Warehouse ]</p> <ol> <li>Kafka: Act\u00faa como un buffer que recibe y almacena temporalmente los datos.</li> <li>Spark Streaming: Procesa los datos de Kafka en micro-lotes y ejecuta l\u00f3gica de transformaci\u00f3n.</li> <li>HBase/Accumulo: Persiste los datos procesados para consultas r\u00e1pidas.</li> </ol>"},{"location":"UD09-Herramientas/3b.kafkaSOL/","title":"Faker (SOLUCIONES)","text":""},{"location":"UD09-Herramientas/3b.kafkaSOL/#generando-csv","title":"Generando CSV","text":"<pre><code>from faker import Faker\nimport csv\n\noutput = open('datosFaker.csv', 'w')\n\nfake = Faker('es_ES')   # cambiamos el locale a espa\u00f1ol\nheader = ['nombre', 'edad', 'calle', 'ciudad',\n        'provincia', 'cp', 'longitud', 'latitud']\nmywriter = csv.writer(output)\nmywriter.writerow(header)\n\nfor r in range(1000):\n    mywriter.writerow([fake.name(),\n                    fake.random_int(min=18, max=80, step=1),\n                    fake.street_address(),\n                    fake.city(),\n                    fake.state(),\n                    fake.postcode(),\n                    fake.longitude(),\n                    fake.latitude()])\noutput.close()\n</code></pre>"},{"location":"UD09-Herramientas/3b.kafkaSOL/#generando-json","title":"Generando JSON","text":"<pre><code>from faker import Faker\nimport json\n\nfake = Faker('es_ES')   # cambiamos el locale a espa\u00f1ol\n\n# Preparamos los datos\ndatos = {}\ndatos['registros'] = []\n\nfor x in range(1000):\n    persona = {\"datos\": fake.name(),\n            \"edad\": fake.random_int(min=18, max=80, step=1),\n            \"calle\": fake.street_address(),\n            \"ciudad\": fake.city(),\n            \"provincia\": fake.state(),\n            \"cp\": fake.postcode(),\n            \"longitud\": float(fake.longitude()),\n            \"latitud\": float(fake.latitude())}\n    datos['registros'].append(persona)\n\n# Los metemos en el fichero\noutput = open('datosFaker.json', 'w')\njson.dump(datos, output)\n</code></pre>"},{"location":"UD09-Herramientas/3b.kafkaSOL/#ejercicio","title":"Ejercicio","text":"<p>Genera un JSON con los siguientes datos:</p> <p>Nombre | Apellido1 | Apellido2 | NIF | N\u00famero de cuenta (iban) | nombre empresa | pa\u00eds empresa | ciudad empresa</p>"},{"location":"UD09-Herramientas/4.mqtt/","title":"UD08 3. Mosquitto (mqtt)","text":""},{"location":"UD09-Herramientas/4.mqtt/#que-es-mqtt","title":"\u00bfQu\u00e9 es mqtt?","text":"<p>MQTT, o Message Queueing Telemetry Transport, es un protocolo de mensajer\u00eda ligero dise\u00f1ado para la comunicaci\u00f3n entre dispositivos con recursos limitados, como los de Internet de las Cosas (IoT). Es un protocolo de publicaci\u00f3n/suscripci\u00f3n, lo que significa que los dispositivos (clientes) pueden enviar mensajes a un tema espec\u00edfico (publicar) y otros dispositivos pueden suscribirse a ese tema para recibir los mensajes (suscripci\u00f3n). </p> <ul> <li> <p>Ligero:     MQTT est\u00e1 dise\u00f1ado para funcionar en dispositivos con poca memoria y bajo consumo de energ\u00eda, lo que lo hace ideal para dispositivos IoT.\u00a0</p> </li> <li> <p>Eficiente:      La arquitectura de publicaci\u00f3n/suscripci\u00f3n de MQTT permite una comunicaci\u00f3n bidireccional eficiente y escalable.\u00a0</p> </li> <li> <p>Flexible:     MQTT puede funcionar sobre TCP/IP y otros protocolos de red.\u00a0</p> </li> <li> <p>Seguro:     MQTTs, que es MQTT con SSL/TLS, proporciona una capa de seguridad adicional para proteger la comunicaci\u00f3n.\u00a0</p> </li> <li> <p>Escalable:     MQTT puede manejar grandes cantidades de dispositivos y mensajes, lo que lo convierte en una buena opci\u00f3n para proyectos IoT a gran escala.\u00a0</p> </li> </ul> <p>Aplicaciones de MQTT: - IoT:     MQTT es ampliamente utilizado en aplicaciones IoT para conectar sensores, dispositivos de control y otros dispositivos con la nube.\u00a0</p> <ul> <li> <p>Automatizaci\u00f3n industrial:     MQTT se utiliza para la comunicaci\u00f3n entre dispositivos de automatizaci\u00f3n industrial, como robots y sistemas de control.\u00a0</p> </li> <li> <p>Dom\u00f3tica:     MQTT se utiliza para controlar dispositivos del hogar inteligente, como luces, term\u00f3statos y otros.\u00a0</p> </li> <li> <p>Telemetr\u00eda:     MQTT se utiliza para transmitir datos de telemetr\u00eda de dispositivos a un sistema central.\u00a0</p> </li> </ul> <p>En resumen, MQTT es un protocolo de mensajer\u00eda ligero, eficiente y flexible que se utiliza para conectar dispositivos en una variedad de aplicaciones, especialmente en el \u00e1mbito del IoT.</p>"},{"location":"UD09-Herramientas/4.mqtt/#como-funciona-mqtt","title":"\u00bfC\u00f3mo funciona MQTT?","text":"<p>https://aws.amazon.com/es/what-is/mqtt/</p> <ol> <li>Un cliente MQTT establecer una conexi\u00f3n con el agente MQTT.</li> <li>Una vez conectado, el cliente puede publicar mensajes, suscribirse a mensajes espec\u00edficos o hacer ambas cosas.</li> <li>Cuando el agente MQTT recibe un mensaje, lo reenv\u00eda a los suscriptores que est\u00e1n interesados.</li> </ol>"},{"location":"UD09-Herramientas/4.mqtt/#tema-de-mqtt","title":"Tema de MQTT","text":"<p>El t\u00e9rmino \u201ctema\u201d se refiere a las palabras clave que utiliza el agente MQTT a fin de filtrar mensajes para los clientes de MQTT. Los temas est\u00e1n organizados jer\u00e1rquicamente, de forma similar a un directorio de archivos o carpetas. Por ejemplo, considere un sistema dom\u00e9stico inteligente que opera en una casa de varios pisos que tiene diferentes dispositivos inteligentes en cada uno de ellos. En ese caso, es posible que el agente MQTT organice temas como:</p> <p><code>_ourhome/groundfloor/livingroom/light_</code></p> <p><code>_ourhome/firstfloor/kitchen/temperature_</code></p>"},{"location":"UD09-Herramientas/4.mqtt/#publicacion-mqtt","title":"Publicaci\u00f3n MQTT","text":"<p>Los clientes MQTT publican mensajes que contienen el tema y los datos en formato de bytes. El cliente determina el formato de los datos, como datos de texto, datos binarios, archivos XML o JSON. Por ejemplo, es posible que una l\u00e1mpara del sistema dom\u00e9stico inteligente publique un mensaje\u00a0sobre el tema\u00a0sal\u00f3n o luz.</p>"},{"location":"UD09-Herramientas/4.mqtt/#suscripcion-mqtt","title":"Suscripci\u00f3n MQTT","text":"<p>Los clientes MQTT env\u00edan un mensaje\u00a0SUBSCRIBE\u00a0(SUBSCRIBIRSE) al agente MQTT para recibir mensajes sobre temas de inter\u00e9s. Este mensaje contiene un identificador \u00fanico y una lista de suscripciones. Por ejemplo, la aplicaci\u00f3n de hogar inteligente en su tel\u00e9fono quiere mostrar cu\u00e1ntas luces est\u00e1n encendidas en casa. Se suscribir\u00e1 a la\u00a0luz\u00a0del tema y aumentar\u00e1 el contador para todos los mensajes\u00a0activados.</p>"},{"location":"UD09-Herramientas/4.mqtt/#codigo-ejemplo","title":"C\u00f3digo ejemplo","text":"<p>Podemos utilizar mqtt desde nuestro propio servidor (a trav\u00e9s de docker por ejemplo) o utilizando servidores p\u00fablicos, en este ejemplo utilizaremos un servidor p\u00fablico <code>broker.emqx.io</code> En \u00e9l crearemos un topic \"propio\" para poder capturar los mensajes que llegan. Este servidor p\u00fablico no tiene autenticaci\u00f3n, por lo que no necesitamos user - pass.</p> <p>Crea los siguientes archivos en una carpeta llamada mqtt.</p> <p>Vamos a trabajar en un venv (hay un archivo que lo indica, requrements.txt)</p> <p>ATENCI\u00d3N</p> <p>Modifica el topic con tu nombre, as\u00ed acceder\u00e1s solamente a tus mensajes. iabdEduardoPrimoMINOMBRE</p> <pre><code>'''config.py'''\n\nbroker = 'broker.emqx.io'\nport = 1883\ntopic = \"iabdEduardoPrimoMINOMBRE/mqtt\"\n</code></pre> <pre><code>'''requirements.txt'''\ndocopt==0.6.2\nmqtt-client==1.6.1\npaho-mqtt==1.6.1\nterminaltables==3.1.10\n</code></pre> <pre><code>'''consumer.py'''\n\nimport random\n\nfrom paho.mqtt import client as mqtt_client\nfrom config import broker, port, topic\n\nclient_id = f'subscribe-{random.randint(0, 10000)}'\n# username = 'emqx'\n# password = 'public'\n\ndef connect_mqtt() -&gt; mqtt_client:\n    def on_connect(client, userdata, flags, rc):\n        if rc == 0:\n            print(\"Connected to MQTT Broker!\")\n        else:\n            print(\"Failed to connect, return code %d\\n\", rc)\n\n    client = mqtt_client.Client(client_id)\n    # client.username_pw_set(username, password)\n    client.on_connect = on_connect\n    client.connect(broker, port)\n    return client\n\ndef subscribe(client: mqtt_client):\n    def on_message(client, userdata, msg):\n        print(f\"Received `{msg.payload.decode()}` from `{msg.topic}` topic\")\n\n    client.subscribe(topic)\n    client.on_message = on_message\n\ndef run():\n    client = connect_mqtt()\n    subscribe(client)\n    client.loop_forever()\n\nif __name__ == '__main__':\n    run()\n</code></pre> <pre><code>'''publisher.py'''\n# python 3.11\n\nimport random\nimport time\n\nfrom paho.mqtt import client as mqtt_client\nfrom config import broker, port, topic\n\nclient_id = f'publisher-{random.randint(0, 1000)}'\n# username = 'josep'\n# password = '123123'\n\ndef connect_mqtt():\n    def on_connect(client, userdata, flags, rc):\n        if rc == 0:\n            print(\"Connected to MQTT Broker!\")\n        else:\n            print(\"Failed to connect, return code %d\\n\", rc)\n\n    client = mqtt_client.Client(client_id)\n    client.enable_logger()\n    # client.username_pw_set(username, password)\n    client.on_connect = on_connect\n    client.connect(broker, port)\n    return client\n\ndef publish(client):\n    msg_count = 1\n    while True:\n        time.sleep(1)\n        msg = f\"messages: {msg_count}\"\n        result = client.publish(topic, msg)\n        # result: [0, 1]\n        status = result[0]\n        if status == 0:\n            print(f\"Send `{msg}` to topic `{topic}`\")\n        else:\n            print(f\"Failed to send message to topic {topic}\")\n        msg_count += 1\n        if msg_count &gt; 5:\n            break\n\ndef run():\n    client = connect_mqtt()\n    client.loop_start()\n    publish(client)\n    client.loop_stop()\n\nif __name__ == '__main__':\n    run()\n</code></pre> <p>Ejecuta en diferentes terminales el consumer y despu\u00e9s el publisher.</p> <p>Ejercicio 1, vamos a a\u00f1adir a nuestro sistema dom\u00f3tico un sensor de temperatura y humedad. Modifica el proyecto, cambia el TOPIC. https://wokwi.com/projects/430378144021081089</p> <p>Ejercicio 2, a\u00f1adimos a nuestro sistema dom\u00f3tico, al pulsar el bot\u00f3n se conectar\u00e1 y mandar\u00e1 un mensaje por mqtt del tipo \"Alarma activada\". Extra, al pulsarlo otra vez el mensaje cambiar\u00e1 a \"Alarma desactivada\" https://wokwi.com/projects/430378792170160129</p>"},{"location":"UD09-Herramientas/4.mqtt/#flashear-esp32-real","title":"Flashear esp32 \"real\"","text":""},{"location":"UD09-Herramientas/5.databricks/","title":"Databricks","text":"<p>Fuente: https://aitor-medrano.github.io/iabd/spark/spark.html#uso-en-la-nube</p> <p>Databricks\u00a0es una plataforma anal\u00edtica de datos basada en Apache Spark desarrollada por la compa\u00f1\u00eda con el mismo nombre. La empresa, creada en el 2013 por los desarrolladores principales de\u00a0Spark, permite realizar anal\u00edtica Big Data e Inteligencia Artificial con\u00a0Spark\u00a0de una forma sencilla y colaborativa.</p> <p>Databricks\u00a0se integra de forma transparente con\u00a0AWS,\u00a0Azure\u00a0y\u00a0Google Cloud. En una\u00a0entrada del blog de la empresa de noviembre de 2021\u00a0anuncian un nuevo record de procesamiento que implica que su rendimiento es 3 veces superior a la competencia y con un coste menor.</p> <p>Para poder trabajar con\u00a0Databricks\u00a0de forma gratuita, podemos hacer uso de\u00a0Databricks Community Edition, donde podemos crear nuestros propios cuadernos\u00a0Jupyter\u00a0y trabajar con\u00a0Spark\u00a0sin necesidad de instalar nada.</p> <p>Para crear una cuenta gratuita, clickando sobre\u00a0Sign up, tras rellenar los datos personales, antes de seleccionar el proveedor\u00a0cloud, en la parte inferior derecha, hemos de pulsar sobre\u00a0Get started with Community Edition\u00a0(bot\u00f3n en negro):</p>"},{"location":"UD09-Herramientas/5.databricks/#creacion-cluster","title":"Creaci\u00f3n cl\u00faster","text":"<p>El \u00fanico paso inicial tras registrarnos, es crear un cl\u00faster b\u00e1sico (con 15.3GB de memoria y dos n\u00facleos) desde la opci\u00f3n\u00a0Create\u00a0del men\u00fa de la izquierda:</p> <p></p> <p>Tras un par de minutos se habr\u00e1 creado y lanzado el cl\u00faster, ya estaremos listos para crear un nuevo\u00a0notebook\u00a0y tener acceso a\u00a0Spark\u00a0directamente desde el objeto\u00a0<code>spark</code>:</p> <p></p> <p>Si queremos, podemos hacer p\u00fablico el cuaderno y compartirlo con la comunidad.</p> <p>\u2139\ufe0f INFO Por defecto, el navegador del sistema de archivos de DataBricks est\u00e1 oculto. Para facilitar el acceso a los datos y visualizar la estructura y ruta de los mismos, lo podemos activar desde el men\u00fa superior derecho: Admin Settings \u2192 Workspace Settings \u2192 Advanced, y ponemos la opci\u00f3n DBFS File Browser a Enabled.</p>"},{"location":"UD09-Herramientas/5.databricks/#sparkui-en-databricks","title":"SparkUI en databricks","text":"<p>Para acceder a la herramienta de monitorizaci\u00f3n en Databricks, una vez creado un cl\u00faster, en la opci\u00f3n\u00a0Compute/Calcular podremos seleccionar el cl\u00faster creado y en la pesta\u00f1a\u00a0IU de Spark\u00a0acceder al mismo interfaz gr\u00e1fico:</p> <p></p>"},{"location":"UD09-Herramientas/5.databricks/#trabajando-con-databricks","title":"Trabajando con\u00a0Databricks\u00b6","text":"<p>https://aitor-medrano.github.io/iabd/spark/dataframeAPI.html#trabajando-con-databricks</p> <p>Una vez creado de nuevo el cluster, vamos a cargar los datos mediante la opci\u00f3n Data, subiendo el archivo <code>pdi_sales_small.csv</code></p> <p></p> <p>Una vez cargado el archivo, pulsamos sobre el bot\u00f3n Create table in notebook de manera que nos crea un cuaderno Jupyter donde podemos consultar los datos y crear una vista temporal:</p> <p></p> <p>Para que funcione correctamente con nuestro datos, vamos a modificar el c\u00f3digo:</p> <pre><code>infer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \";\"\n</code></pre> <p>Y tras cargar el\u00a0dataset, antes de crear la vista, vamos a limpiar los pa\u00edses:</p> <pre><code>from pyspark.sql.functions import trim\ndf = df.withColumn(\"Country\", trim(df.Country))\n</code></pre>"},{"location":"UD09-Herramientas/5.databricks/#datos-visuales","title":"Datos visuales\u00b6","text":"<p>Si volvemos a ejecutar el cuaderno, ahora s\u00ed que cargar\u00e1 correctamente los datos. Si nos vamos a la celda que realiza una consulta sobre todos los datos, podemos ver en la parte superior derecha como el lenguaje empleado en la celda es SQL, por ello la primera l\u00ednea comienza con %sql, y a continuaci\u00f3n ya podemos introducir directamente c\u00f3digo SQL, teniendo la opci\u00f3n de visualizar los datos tanto en modo texto como mediante gr\u00e1ficos:</p> <p></p> <p>CONTINUAR\u2026..</p> <p>https://aitor-medrano.github.io/iabd/spark/dataframeAPI.html#cuadro-de-mandos</p> <p>Adem\u00e1s, con las tablas y/o gr\u00e1ficos que generamos dentro de\u00a0Databricks, podemos generar un sencillo cuadro de mandos.</p> <p>Vamos a crear un par de consultas, una para obtener las ventas medias por pa\u00eds:</p> <pre><code>%sql\nselect Country, avg(Revenue) as ventas\nfrom pdi_sales_small_csv\ngroup by Country\norder by ventas desc\n</code></pre> <p>Y otra para las unidas pedidas por cada pa\u00eds:</p> <pre><code>%sql\nselect Country, sum(Units) as pedidos\nfrom pdi_sales_small_csv\ngroup by Country\norder by pedidos desc\n</code></pre> <p>Si pulsamos sobre el icono del gr\u00e1fico de barras de la esquina superior derecha de una celda SQL, podemos a\u00f1adir el resultado de la celda a un\u00a0dashboard:</p> <p></p> <p>Una vez creado, s\u00f3lo tenemos que seleccionar las celdas que queramos, e ir a\u00f1adi\u00e9ndolas al cuadro de mandos creado. Posteriormente, podemos abrirlo, resituar los elementos y visualizarlo:</p> <p></p>"},{"location":"UD09-Herramientas/_TODO/","title":"TODO","text":"<p>kafka https://www.udemy.com/course/python-kafka-streaming-handson/?referralCode=0DB1416B58D5527FD168&amp;couponCode=NEWYEARCAREER</p> <p>Databricks - Master Azure Databricks for Data Engineers https://www.udemy.com/course/complete-hadoop-framework-including-kafka-spark-and-mongo-db/?couponCode=NEWYEARCAREER</p> <p>https://www.udemy.com/course/azure-databricks-end-to-end-project-with-unity-catalog-cicd/?couponCode=NEWYEARCAREER</p>"},{"location":"UD09-Herramientas/27c8a05e-9516-44af-b1a9-4a2af1fac089_Export-deaed103-6353-41af-a78a-be12228dd6e2/UD%2008%20-%20Otras%20herramientas%20fffe913de6c481f3a815ca61f406ba9e/","title":"UD 08 - Otras herramientas","text":"<p>UD08 1a. Faker</p> <p>UD08 1b. Faker (SOLUCIONES)</p> <p>UD08 2. Kafka</p> <p>UD08 3. Kafka II</p> <p>UD08 3. Mosquitto (mqtt)</p> <p>UD08 4. Databricks</p> <p>Nifi docker</p> <pre><code>docker run --platform linux/amd64 --name nifi -e NIFI_WEB_HTTP_PORT='8080' -p 8080:8080 -d apache/nifi:1.18.0 --restart=always\n</code></pre>"},{"location":"UD09-Herramientas/27c8a05e-9516-44af-b1a9-4a2af1fac089_Export-deaed103-6353-41af-a78a-be12228dd6e2/UD%2008%20-%20Otras%20herramientas%20fffe913de6c481f3a815ca61f406ba9e/UD08%201a%20Faker%20fffe913de6c481a3ba13f25e6ccb4ef5/","title":"UD08 1a. Faker","text":""},{"location":"UD09-Herramientas/27c8a05e-9516-44af-b1a9-4a2af1fac089_Export-deaed103-6353-41af-a78a-be12228dd6e2/UD%2008%20-%20Otras%20herramientas%20fffe913de6c481f3a815ca61f406ba9e/UD08%201a%20Faker%20fffe913de6c481a3ba13f25e6ccb4ef5/#1-libreria-faker","title":"1. Librer\u00eda faker","text":""},{"location":"UD09-Herramientas/27c8a05e-9516-44af-b1a9-4a2af1fac089_Export-deaed103-6353-41af-a78a-be12228dd6e2/UD%2008%20-%20Otras%20herramientas%20fffe913de6c481f3a815ca61f406ba9e/UD08%201a%20Faker%20fffe913de6c481a3ba13f25e6ccb4ef5/#11-instalacion","title":"1.1. Instalaci\u00f3n","text":"<p>Si necesitamos generar muchos datos, es muy \u00fatil emplear una librer\u00eda como\u00a0Faker\u00a0para generar datos sint\u00e9ticos.</p> <p>Primero hemos de instalarla mediante pip:</p> <pre><code>pip3 install faker\n</code></pre>"},{"location":"UD09-Herramientas/27c8a05e-9516-44af-b1a9-4a2af1fac089_Export-deaed103-6353-41af-a78a-be12228dd6e2/UD%2008%20-%20Otras%20herramientas%20fffe913de6c481f3a815ca61f406ba9e/UD08%201a%20Faker%20fffe913de6c481a3ba13f25e6ccb4ef5/#12-ejemplo","title":"1.2. Ejemplo","text":"<pre><code>'''hola.py'''\nfrom faker import Faker\n\nfake = Faker()\nfake = Faker('es_ES')   # cambiamos el locale a espa\u00f1ol\n\nprint(\"Nombre:\", fake.name())\nprint(\"Direcci\u00f3n:\", fake.address())\nprint(\"Nombre de hombre:\", fake.first_name_male())\nprint(\"N\u00famero de tel\u00e9fono:\", fake.phone_number())\nprint(\"Color:\", fake.color_name())\nprint(\"Fecha:\", fake.date())\nprint(\"Email:\", fake.email())\nprint(\"Frase de 10 palabras\", fake.sentence(nb_words=10))\n</code></pre> <pre><code>$ python3 hola.py\n\nNombre: Yaiza Rico-Carbonell\nDirecci\u00f3n: Ca\u00f1ada de Maximiliano Gal\u00e1n 4\nSegovia, 08077\nNombre de hombre: Nando\nN\u00famero de tel\u00e9fono: +34 888 44 88 30\nColor: Amarillo dorado claro\nFecha: 1989-02-18\nEmail: pinolcalixto@example.com\nFrase de 10 palabras Labore nesciunt placeat nam soluta atque dolores dolor provident qui.\n</code></pre> <p>https://faker.readthedocs.io/en/master/providers.html</p> <p>Los diferentes grupos de datos que genera se agrupan en\u00a0Providers: de direcci\u00f3n, fechas, relacionados con internet, bancarios, c\u00f3digos de barra, isbn, etc...</p> <p>Al trabajar con el idioma en espa\u00f1ol, puede que algunos m\u00e9todos no funcionen (m\u00e1s que no funcionar, posiblemente tengan otro nombre). Es recomendable comprobar las opciones disponibles en\u00a0https://faker.readthedocs.io/en/master/locales/es_ES.html</p>"},{"location":"UD09-Herramientas/27c8a05e-9516-44af-b1a9-4a2af1fac089_Export-deaed103-6353-41af-a78a-be12228dd6e2/UD%2008%20-%20Otras%20herramientas%20fffe913de6c481f3a815ca61f406ba9e/UD08%201a%20Faker%20fffe913de6c481a3ba13f25e6ccb4ef5/#2-otros-lenguajes","title":"2. Otros lenguajes","text":"<p>Getting Started | Faker</p> <p>npm: @faker-js/faker</p> <p>FakerPHP / Faker</p>"},{"location":"UD09-Herramientas/27c8a05e-9516-44af-b1a9-4a2af1fac089_Export-deaed103-6353-41af-a78a-be12228dd6e2/UD%2008%20-%20Otras%20herramientas%20fffe913de6c481f3a815ca61f406ba9e/UD08%201a%20Faker%20fffe913de6c481a3ba13f25e6ccb4ef5/#3-ejercicios","title":"3. Ejercicios","text":"\u2705 **ENTREGAR AULES**   <ol> <li>Modifica el ejemplo anterior para generar un CSV con 100 personas.</li> <li>Modifica el ejemplo anterior para generar un JSON con 100 personas.</li> <li>Genera un JSON con los siguientes datos:</li> </ol> <p>Nombre | Apellido1 | Apellido2 | NIF | N\u00famero de cuenta (iban) | nombre empresa | pa\u00eds empresa | ciudad empresa</p>"},{"location":"UD09-Herramientas/27c8a05e-9516-44af-b1a9-4a2af1fac089_Export-deaed103-6353-41af-a78a-be12228dd6e2/UD%2008%20-%20Otras%20herramientas%20fffe913de6c481f3a815ca61f406ba9e/UD08%203%20Kafka%20II%20fffe913de6c4813e82abcf00fa8f7c76/","title":"UD08 3. Kafka II","text":"<p>https://www.youtube.com/watch?v=AM4hxuGmEBU</p> <pre><code>git clone [git@github.com](mailto:git@github.com):streamthoughts/kafka-monitoring-stack-docker-compose.git\n</code></pre>"}]}