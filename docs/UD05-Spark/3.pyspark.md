# PySpark. Entornos de ejecución

**PySpark** es la interfaz de **Apache Spark** para **Python**. Apache Spark es un motor de procesamiento de datos de código abierto, diseñado para procesar y analizar grandes volúmenes de datos de manera distribuida y en paralelo. PySpark permite a los desarrolladores utilizar Spark con el lenguaje Python, lo que facilita la manipulación de datos, el análisis y la implementación de algoritmos de aprendizaje automático a gran escala.

PySpark vs Pandas
![](<./images/pyspark1.png>)

## docker

**Fuente:**

[https://towardsdatascience.com/stuck-trying-to-get-pyspark-to-work-in-your-data-science-environment-here-is-another-way-fb80a4bb7d8f](https://towardsdatascience.com/stuck-trying-to-get-pyspark-to-work-in-your-data-science-environment-here-is-another-way-fb80a4bb7d8f)

Utilizaremos la siguiente imagen:

[https://hub.docker.com/r/jupyter/pyspark-notebook](https://hub.docker.com/r/jupyter/pyspark-notebook)

![](<./images/pyspark2.png>)

```bash
docker pull jupyter/pyspark-notebook
```

Arrancamos el contenedor con:

```bash
# if running on Windows (cmd.exe):
docker run -it -p 8888:8888 -v %cd%:/home/jovyan/work/projects/ jupyter/pyspark-notebook

# if running on a mac or linux computer:
docker run -it -p 8888:8888 
-v `(pwd)`:/home/jovyan/work/projects/ 
jupyter/pyspark-notebook
```

```docker
### Arrancar contenedor desde mac (josep), 
### cambiamos puerto para que no coinicida con apache
docker run -it -p 7777:8888 -v /Users/josepgarcia/Webs/spark:/home/jovyan/work/projects/ jupyter/pyspark-notebook
```

Opciones del comando:
```txt
-it: modo interactivo, nos permite ver la salida del comando. En este caso ejecuta `jupyter notebook` por lo que podremos ver el resultado de la ejecución y la url para acceder al notebook.

-p: "publicación", este parámetro asigna un puerto de red del contenedor a un puerto de red en la máquina host al puerto 8888. La sintaxis es `- {host's port}:{container's port}

Esto significa que si abrimos la máquina en local, en el puerto 8888 veremos lo que hay en el mismo puerto en el contenedor.

-v: "volumen", sirve para mapear volúmenes, en nuestro caso mapeamos el directorio actual (donde lanzamos el comando) `{pwd}` con el directorio que hay en el contenedor `{/home/jovyan/work/projects}`
```

Ahora contamos con un entorno PySpark completamente funcional ejecutándose en Jupyter.

Podemos crear un alias en .bashrc o .zshrc para realizar estos pasos de una manera más sencilla.

```bash
# definir una función contenedora para ejecutar el comando 
run_spark() { 
   docker run -it -p 8888:8888 -v `(pwd)`:/home/jovyan/work/projects/ jupyter/pyspark-notebook 
}
```

## Conexión con VsCode
1. Arrancar contenedor docker y quedarse con la URL.
http://127.0.0.1:8888/lab?token=e6db942bdb0acd0479c5e6b4e838f9a4602c079e38166beb
2. Abrir VSCODE en la misma carpeta donde se encuentran los archivos.
3. Abrir archivo ipynb desde VSCODE y a la derecha arriba SELECCIONAR KERNEL
4. Poner como server la URL del paso 1.
