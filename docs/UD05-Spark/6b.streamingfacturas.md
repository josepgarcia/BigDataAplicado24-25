# Streaming

## 3. Caso 2: Facturas

En este caso de uso vamos a poner en práctica algunos de los conceptos que acabamos de ver.

Vamos a suponer que tenemos un empresa compuesta de diferentes sucursales. Cada una de ellas, **cada 5 minutos** genera un fichero con los datos de las facturas ([*invoices.zip*](https://aitor-medrano.github.io/iabd/spark/resources/invoices.zip)) que han generado. Cada una de las facturas contiene una o más líneas de factura, las cuales queremos separar en facturas simples.

Así pues, vamos a partir de documentos JSON con la siguiente estructura:
![](<./images/Untitled 1.png>)
 Y a partir de él, generaremos 4 documentos (uno por cada línea de factura) con la siguiente estructura:

![](<./images/Untitled 2.png>)
## Cargando los datos

[https://github.com/josepgarcia/datos/raw/main/invoices/invoices.zip](https://github.com/josepgarcia/datos/raw/main/invoices/invoices.zip)

Creamos el archivo invoices.ipynb

1. Iniciar sesión spark, leer todas las facturas (formato json).
2. Imprimir el schema (lo inferimos, no hace falta indicarlo).
- **SOL**
    
    ```python
    from pyspark.sql import SparkSession
    spark = SparkSession \
            .builder \
            .appName("Streaming de Ficheros") \
            .master("local[2]") \
            .config("spark.streaming.stopGracefullyOnShutdown", "true") \
            .config("spark.sql.shuffle.partitions", 3) \
            .config("spark.sql.streaming.schemaInference", "true") \
            .getOrCreate()
    
    raw_df = spark.readStream \
            .format("json") \
            .option("path", "PATH_CORRESPONDIENTE_INVOICES") \
            .load()
    
    raw_df.printSchema()
    # root
    #  |-- CESS: double (nullable = true)
    #  |-- CGST: double (nullable = true)
    #  |-- CashierID: string (nullable = true)
    #  |-- CreatedTime: long (nullable = true)
    #  |-- CustomerCardNo: string (nullable = true)
    #  |-- CustomerType: string (nullable = true)
    #  |-- DeliveryAddress: struct (nullable = true)
    #  |    |-- AddressLine: string (nullable = true)
    #  |    |-- City: string (nullable = true)
    #  |    |-- ContactNumber: string (nullable = true)
    #  |    |-- PinCode: string (nullable = true)
    #  |    |-- State: string (nullable = true)
    #  |-- DeliveryType: string (nullable = true)
    #  |-- InvoiceLineItems: array (nullable = true)
    #  |    |-- element: struct (containsNull = true)
    #  |    |    |-- ItemCode: string (nullable = true)
    #  |    |    |-- ItemDescription: string (nullable = true)
    #  |    |    |-- ItemPrice: double (nullable = true)
    #  |    |    |-- ItemQty: long (nullable = true)
    #  |    |    |-- TotalValue: double (nullable = true)
    #  |-- InvoiceNumber: string (nullable = true)
    #  |-- NumberOfItems: long (nullable = true)
    #  |-- PaymentMethod: string (nullable = true)
    #  |-- PosID: string (nullable = true)
    #  |-- SGST: double (nullable = true)
    #  |-- StoreID: string (nullable = true)
    #  |-- TaxableAmount: double (nullable = true)
    #  |-- TotalAmount: double (nullable = true)
    ```
    
    Aunque en este caso hemos realizado la inferencia de la estructura de los datos de entrada, lo normal es indicar el esquema de los datos.
    

## Proyectando

El siguiente paso es seleccionar los datos que nos interesan. Para ello, tras revisar la estructura de salida que deseamos, realizamos una selección de las columnas y utilizaremos la función `explode` para desenrollar el array de facturas `InvoiceLineItems`:

```python
explode_df = raw_df.selectExpr("InvoiceNumber", "CreatedTime", "StoreID",
                                 "PosID", "CustomerType",
                                 "PaymentMethod", "DeliveryType",
                                 "explode(InvoiceLineItems) as LineItem")
explode_df.printSchema()
```

Tras ello, vamos a renombrar los campos para quitar los campos anidados (creando columnas nuevas con el nombre deseando y eliminando la columna `LineItem`):

```python
from pyspark.sql.functions import expr
limpio_df = explode_df \
    .withColumn("ItemCode", expr("LineItem.ItemCode")) \
    .withColumn("ItemDescription", expr("LineItem.ItemDescription")) \
    .withColumn("ItemPrice", expr("LineItem.ItemPrice")) \
    .withColumn("ItemQty", expr("LineItem.ItemQty")) \
    .withColumn("TotalValue", expr("LineItem.TotalValue")) \
    .drop("LineItem")
limpio_df.printSchema()
```

## Guardando el resultado

Una vez tenemos el proceso de transformación de datos, sólo nos queda crear el *WriterQuery* para escribir el resultado del flujo de datos. En este caso, vamos a almacenarlo también como ficheros en la carpeta `salida` en formato *JSON* a intervalos de un minuto:

(writeStream)

.trigger(processingTime="1 minute")

- **SOL**
    
    ```python
    facturaWriterQuery = limpio_df.writeStream \
        .format("json") \
        .queryName("Facturas Writer") \
        .outputMode("append") \
        .option("path", "./salida") \
        .option("checkpointLocation", "chk-point-dir-caso2") \
        .trigger(processingTime="1 minute") \
        .start()
    ```
    

## Refinando el resultado

Una vez que vemos que todo funciona, podemos realizar unos ajustes de configuración.

Por ejemplo, vamos a configurar que sólo consuma un fichero cada vez. Para ello, en el *reader* configuramos la opción `maxFilesPerTrigger`, la cual permite limitar la cantidad de ficheros de cada micro-batch.

Otras opciones que se usan de manera conjunta son `cleanSource` y `sourceArchiveDir`, que permiten archivar los ficheros procesados de forma automática. La opción `cleanSource` puede tomar los valores `archive` o `delete`. Si decidimos archivar, mediante `sourceArchiveDir` indicamos el destino donde se moverán.

```python
raw_df = spark.readStream \
    .format("json") \
    .option("path", "../datos/invoices") \
    .option("maxFilesPerTrigger", 1) \
    .option("cleanSource", "delete") \
    .load()
```

Hay que tener en cuenta, que tanto archivar como eliminar van a impactar negativamente en el rendimiento de cada micro-batch. Nosotros hemos de limpiar el directorio de entrada, eso es un hecho. Si ejecutamos *micro-batch* largos, podemos usar la opción `cleanSource`. En cambio, si nuestros *batches* son muy cortos y el utilizar `cleanSource` no es factible por la demora que introduce, debemos crear un proceso de limpieza separado que se ejecute cada X horas y que limpie nuestro directorio de entrada.

## Monitorización

Una vez hemos realizado una consulta, podemos obtener [información](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#managing-streaming-queries) sobre la misma de forma programativa:

```python
facturaWriterQuery.explain() # muestra una explicación detalla del plan de ejecución
# == Physical Plan ==
# *(1) Project [InvoiceNumber#314, CreatedTime#308L, StoreID#319, PosID#317, CustomerType#310, PaymentMethod#316, DeliveryType#312, _extract_City#339 AS City#52, _extract_State#340 AS State#53, _extract_PinCode#341 AS PinCode#54, LineItem#55.ItemCode AS ItemCode#67, LineItem#55.ItemDescription AS ItemDescription#81, LineItem#55.ItemPrice AS ItemPrice#96, LineItem#55.ItemQty AS ItemQty#112L, LineItem#55.TotalValue AS TotalValue#129]
# ...
facturaWriterQuery.recentProgress # muestra una lista de los últimos progresos de la consulta
# [{'id': '3b6d37cf-6a3c-405e-a715-1dc787f34b00',
#   'runId': '3dc7c478-626a-4558-87ea-4912da55114d',
#   'name': 'Facturas Writer',
#   'timestamp': '2024-05-11T08:20:49.058Z',
#   'batchId': 0,
#   'numInputRows': 500,
#   'inputRowsPerSecond': 0.0,
#   'processedRowsPerSecond': 113.55893708834887,
#   'durationMs': {'addBatch': 2496,
# ...
facturaWriterQuery.lastProgress # muestra el último progreso
# {'id': '3b6d37cf-6a3c-405e-a715-1dc787f34b00',
#  'runId': '3dc7c478-626a-4558-87ea-4912da55114d',
#  'name': 'Facturas Writer',
#  'timestamp': '2024-05-11T08:33:00.001Z',
#  'batchId': 3,
#  'numInputRows': 0,
#  'inputRowsPerSecond': 0.0,
#  'processedRowsPerSecond': 0.0,
#  'durationMs': {'latestOffset': 5, 'triggerExecution': 8},
# ...
```

Estas mismas estadísticas las podemos obtener de forma gráfica. Al ejecutar procesos en Streaming, si accedemos a Spark UI, ahora podremos ver la pestaña *Structured Streaming* con información detallada de la cantidad datos de entrada, tiempo procesado y duración de los micro-batches:
![](<./images/Untitled 3.png>)

Además, podemos iniciar tantas consultas como queramos en una única sesión de Spark, las cuales se ejecutarán de forma concurrente utilizando los recursos del clúster de Spark.

<aside>
✅ Para acceder a la webUI deberíamos de mapear los puertos tal y como lo hicimos en:

[UD05 4. PySpark. Contexto, sesión y RDDs.](<./ZZ4.pysparkRDD.md>)

docker run -it -p 8888:8888 -p **4040:4040 -p 4041:4041 -p 4042:4042** -v $(pwd):/home/jovyan/work/projects/ jupyter/pyspark-notebook

</aside>

## Tolerancia a fallos

Un aplicación en *streaming* se espera que se ejecute de forma ininterrumpida mediante un bucle infinito de micro-batches.

Realmente, un escenario de ejecución infinita no es posible, ya que la aplicación se detendrá por:

- un fallo, ya sea por un dato mal formado o un error de red.
- mantenimiento del sistema, para actualizar la aplicación o el hardware donde corre.

Para tratar la tolerancia a fallos, existen tres escenarios posibles:

- Una vez como mucho (*at most once*): no se entrega más de una copia de un dato. Es decir, puede darse el caso de que no llegue, pero no habrá repetidos.
- Una vez al menos (*at least once*): en este caso no habrá pérdidas, pero un dato puede llegar más de una vez.
- Una vez exacta (*exactly once*): se garantiza que cada dato se entrega una única vez, sin pérdidas ni duplicados.

![](<./images/Untitled 4.png>)

Por ello, una aplicación *Spark Streaming* se debe reiniciar de forma transparente para mantener la característica de ***exactly-once*** la cual implica que:

1. No se pierde ningún registro
2. No crea registros duplicados.

Para ello, *Spark Structured Streaming* mantiene el estado de los micro-batches mediante [*checkpoints*](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing) que se almacenan en la carpeta indicada por la opción `checkpointLocation`:

```python
facturaWriterQuery = limpio_df.writeStream \
    .format("json") \
    .queryName("Facturas Writer") \
    .outputMode("append") \
    .option("path", "salida") \
    **.option("checkpointLocation", "chk-point-dir-caso2") \**
    .trigger(processingTime="1 minute") \
    .start()
```

La localización de esta carpeta debería ser un sistema de archivo confiable y tolerante a fallos, como HDFS o Amazon S3.

Esta carpeta contiene dos elementos:

- Posición de lectura, que realiza la misma función que los *offset* en Kafka, y representa el inicio y el final del rango de datos procesados por el actual *micro-batch*, de manera que *Spark* conoce el progreso exacto del procesamiento. Una vez ha finalizado el *micro-batch*, *Spark* realiza un *commit* para indicar que se han procesado los datos de forma exitosa.
- Información del estado, que contiene los datos intermedios del *micro-batch*, como la cantidad total de palabras contadas.

De esta manera, *Spark* mantiene toda la información necesaria para reiniciar un micro-batch que no ha finalizado. Sin embargo, la capacidad de reiniciarse no tiene por qué garantizar la política *exactly-once*. Para ello, es necesario cumplir los siguientes requisitos:

1. Reiniciar la aplicación con el mismo `checkpointLocation`. Si se elimina la carpeta o se ejecuta la misma consulta sobre otro directorio de *checkpoint* es como si realizásemos una consulta desde 0.
2. Utilizar una fuente de datos que permita volver a leer los datos incompletos del *micro-batch*, por ejemplo, tanto los ficheros de texto como *Kafka* permiten volver a leer los datos desde un punto determinado. Sin embargo, los datos que provienen de un socket no permite volver a leerlos.
3. Asegurar que la lógica de aplicación, dados los mismos datos de entrada, produce siempre el mismo resultado (aplicación determinista). Si por ejemplo, nuestra lógica de aplicación utilizará alguna dependencia basada en fechas o el tiempo, ya no obtendríamos el mismo resultado.
4. El destino (*sink*) debe ser capaz de identificar los elementos duplicados e ignorarlos o actualizar la copia antigua con el mismo registro, es decir, son idempotentes.

# 4. Ejercicio Bizum

<aside>
✅ **Entregar AULES**

</aside>

El archivo bizums.zip contiene una simulación de datos de *bizum* que llegan a nuestra cuenta. 

[https://github.com/josepgarcia/datos/raw/main/bizums/bizums.zip](https://github.com/josepgarcia/datos/raw/main/bizums/bizums.zip)

1. Crea un *script* *Python* que, al ejecutarlo, se encargue de simular el envío de *bizums.*

```python
import os
import shutil
import shutil, os
from random import randint, uniform,random

import time

ruta = os.getcwd() + os.sep

#bizum = input('Bizum: ')

i = 1
while i <= 20:
	num_aleatorio = randint(0,20)
	origen = ruta + 'Sin_llegar' + os.sep + 'Bizum' +str(num_aleatorio) + '.csv'
	destino = ruta + 'recibidos' + os.sep + 'Bizum' +str(num_aleatorio) + '.csv'
	try:
	    shutil.copyfile(origen, destino)
	    print("Bizum recibido")
	    i=i+1
	except:
		print("Error al enviar bizum")
		i=i+1
	time.sleep(10)%
```

1. Crea una aplicación de *Spark Streaming* que muestre para cada persona, cual **es el *bizum* más alto**.

El formato de estos datos es CSV formado por el `Nombre;Cantidad;Concepto` (dos nombres en mayúsculas y en minúsculas son de la misma persona, convertir a lowercase). Un ejemplo de un *bizum* recibido sería similar a:

```bash
Aitor;25;Cena restaurante
```

Muestra el resultado completo por consola.