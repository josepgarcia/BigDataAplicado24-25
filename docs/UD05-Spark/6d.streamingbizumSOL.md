# Ejercicio Bizum

Trabajar con m√∫ltiples streams:
https://medium.com/@kiranvutukuri/working-with-multiple-streams-in-apache-spark-part-2-46c381e31079

```python
from pyspark.sql import SparkSession  
from pyspark.sql.functions import col  
  
# Initialize Spark Session  
spark = SparkSession.builder \  
.appName("MultipleStreamsExample") \  
.getOrCreate()  
  
# Stream 1: Kafka Stream (Transaction Data)  
transaction_stream = spark.readStream \  
.format("kafka") \  
.option("kafka.bootstrap.servers", "localhost:9092") \  
.option("subscribe", "transactions") \  
.load() \  
.selectExpr("CAST(value AS STRING) as transaction_data")  
  
# Stream 2: File Stream (User Data)  
user_stream = spark.readStream \  
.format("csv") \  
.option("header", "true") \  
.schema("user_id STRING, user_name STRING") \  
.load("/path/to/user/files")  
  
# Process each stream independently  
transaction_query = transaction_stream.writeStream \  
.format("console") \  
.outputMode("append") \  
.start()
  
user_query = user_stream.writeStream \  
.format("console") \  
.outputMode("append") \  
.start()  
  
transaction_query.awaitTermination()  
user_query.awaitTermination()
```
