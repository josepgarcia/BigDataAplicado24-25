# UD05 8. Streaming

[https://aitor-medrano.github.io/iabd/spark/streaming.html](https://aitor-medrano.github.io/iabd/spark/streaming.html)

# 1. Comando ncat (nc)

Permite acceder a puertos TCP o UDP de la propia máquina o de otras máquinas remotas. También permite quedar a la escucha en un puerto dado (TCP o UDP) de la máquina local.

Podemos utilizar a ncat como herramienta de escaneo de puertos, de seguridad o de monitoreo; además de como proxy TCP simple. Es muy útil para auditar la seguridad de sistemas, de servidores web, de servidores de correo, entre otros.

IMPORTANTE: Aunque se asemejan, ncat y netcat son programas diferentes. [https://access.redhat.com/documentation/es-es/red_hat_enterprise_linux/7/html/migration_planning_guide/ch04s07s04](https://access.redhat.com/documentation/es-es/red_hat_enterprise_linux/7/html/migration_planning_guide/ch04s07s04)

- **TLDR**
    
    Redirect I/O into a network stream through this versatile tool. More information: [https://manned.org/man/nc.1](https://manned.org/man/nc.1).
    
    - Start a listener on the specified TCP port and send a file into it:
    nc -l -p port < filename
    - Connect to a target listener on the specified port and receive a file from it:
    nc host port > received_filename
    - Scan the open TCP ports of a specified host:
    nc -v -z -w timeout_in_seconds host start_port-end_port
    - Start a listener on the specified TCP port and provide your local shell access to the connected party (this is dangerous and can be abused):
    nc -l -p port -e shell_executable
    - Connect to a target listener and provide your local shell access to the remote party (this is dangerous and can be abused):
    nc host port -e shell_executable
    - Act as a proxy and forward data from a local TCP port to the given remote host:
    nc -l -p local_port | nc host remote_port
    - Send an HTTP GET request:
    echo -e "GET / HTTP/1.1\nHost: host\n\n" | nc host 80

**Funcionamiento**

Cuando funciona como cliente, nc crea un socket para conectarse al puerto indicado de la máquina destino.

```bash
$ nc maquina_destino puerto_destino
```

La conexión permenecerá abierta mientras no la finalice el servidor o el cliente nc (CONTROL+C)

Cuando funciona como servidor (modo escucha [opción -l, listen]), abre un socket en la máquina local que queda a la escucha en el puerto indicado

```bash
$ nc -l -p puerto_escucha
```

En ambos casos, una vez establecida la conexión, nc envía a través del socket creado todo lo que reciba por la entrada estándar y envía a la salida estándar lo que le llegue por el socket.

Opciones interesantes:

-u usa el modo UDP (por defecto son conexiones TCP)
-c comando / -e ejecutable ejecuta un comando/programa una vez iniciada la conexión cuyas entrada y salida estándar están redirigidas a la conexión establecida

**Ejemplo: Enviar - Recibir** 

```bash
# En una terminal escuchamos
nc -l -p 999 # En mac sin -p

# En otra terminal enviamos
nc localhost 999 
```

**Ejemplo: Copia de ficheros**

**** Intentar con linux ****

```bash
# Abrimos un terminal, se queda a la escucha
nc localhost 88 > destino.txt

# Abrimos otro termina, creamos un fichero y lo mandamos al primer terminal
echo "aa\nb\ncc\ndd" > origen.txt
nc localhost 88 < origen.txt
```

**Ejemplo con python:**

```bash
# En un terminal
# Opción k -> keep open, mantiene la conexión abierta incluso después de recibir datos.
nc -lk 88
```

```python

import socket
import time
import random

def enviar_palabra(palabra, host='localhost', puerto=88):
    # Crear un socket TCP/IP
    cliente_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    try:
        # Conectar al servidor netcat
        cliente_socket.connect((host, puerto))

        # Enviar la palabra al servidor
        palabra = palabra + '\n'
        cliente_socket.sendall(palabra.encode())

        # Esperar X segundos
        intervalo = random.uniform(0.5, 2)
        time.sleep(intervalo)
    finally:
        # Cerrar la conexión
        cliente_socket.close()

if __name__ == "__main__":
    palabras = ["hola", "mundo", "python", "netcat", "socket"]
    palabras = palabras * 2
    for palabra in palabras:
        enviar_palabra(palabra)

```

La función encode() se utiliza para convertir una cadena de caracteres (string) en su representación en bytes, utilizando un cierto esquema de codificación de caracteres. En el contexto de la comunicación a través de sockets o de la lectura/escritura de archivos, es importante trabajar con cadenas de bytes en lugar de cadenas de caracteres directamente, ya que muchos protocolos y sistemas de archivos operan a nivel de bytes.

[Beginner’s Guide To Netcat for Hackers](https://medium.com/@HackTheBridge/beginners-guide-to-netcat-for-hackers-55abe449991d)

[https://www.youtube.com/watch?v=8oGm4pEAsd8](https://www.youtube.com/watch?v=8oGm4pEAsd8)

# **2. Caso 1: Hola Spark Streaming**

Escuchamos en un terminal:

```bash
nc -lk 9999
```

Tras arrancar *Netcat*, ya podemos crear nuestra aplicación *Spark* (vamos a indicar que cree 2 hilos, lo cual es el mínimo necesario para realizar *streaming*, uno para recibir y otro para procesar), en la cual tenemos diferenciadas:

- la fuente de datos: creación del flujo de lectura mediante [`readStream`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.readStream.html) que devuelve un [DataStreamReader](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html) que utilizaremos para cargar un *DataFrame*.
- la lógica de procesamiento, ya sea mediante *DataFrames API* o *Spark SQL*.
- la persistencia de los datos mediante [writeStream](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.writeStream.html) que devuelve un [DataStreamWriter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html) donde indicamos el modo de salida, el cual, al iniciarlo con `start` nos devuelve un [StreamingQuery](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQuery.html)
- y finalmente el cierre del flujo de datos a partir de la consult en *streaming* mediante [`awaitTermination`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQuery.awaitTermination.html?highlight=awaittermination).

**En un cuaderno jupyter….**

```python
''' Streaming1.ipynb '''

from pyspark.sql import SparkSession
spark = SparkSession \
        .builder \
        .appName("Streaming IABD WordCount") \
        .master("local[2]") \
        .getOrCreate()

# Creamos un flujo de escucha sobre netcat en localhost:9999
# En Spark Streaming, la lectura se realiza mediante readStream
lineasDF = spark.readStream \
        .format("socket") \
        .option("host", "IP_MAQUINA") \ # Ponemos IP local de la máquina
        .option("port", "9999") \
        .load()

# Leemos las líneas y las pasamos a palabras.
# Sobre ellas, realizamos la agrupación count (transformación)
from pyspark.sql.functions import explode, split
palabrasDF = lineasDF.select(explode(split(lineasDF.value, ' ')).alias('palabra'))
cantidadDF = palabrasDF.groupBy("palabra").count()

# Mostramos las palabras por consola (sink)
# En Spark Streaming, la persistencia se realiza mediante writeStream
#  y en vez de realizar un save, ahora utilizamos start
wordCountQuery = cantidadDF.writeStream \
        .format("console") \
        .outputMode("complete") \
        .start()

# dejamos Spark a la escucha
wordCountQuery.awaitTermination()
```

# 3. Elementos

La idea básica al trabajar los datos en *streaming* es similar a tener una tabla de entrada de tamaño ilimitado, y conforme llegan nuevos datos, tratarlos como un nuevo conjunto de filas que se adjuntan a la tabla.

![Untitled](<./UD05 8 Streaming fffe913de6c4819a9593edd001b23257/Untitled.png>)

## Fuentes de Datos

Mientras que en el procesamiento *batch* las fuentes de datos son *datasets* estáticos que residen en un almacenamiento como pueda ser un sistema local, HDFS o S3, al hablar de procesamiento en *streaming* las fuentes de datos generan los datos de forma continuada, por lo que necesitamos otro tipo de fuentes.

*Structured Streaming* ofrece un conjunto predefinido de [fuentes de datos](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources) que se leen a partir de un [DataStreamReader](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html). Los tipos existentes son:

**Fichero**: permite leer ficheros desde un directorio como un flujo de datos, con soporte para ficheros de texto, CSV, JSON, Parquet, ORC, etc…

```python
# Lee todos los ficheros csv de un directorio
esquemaUsuario = StructType() \
    .add("nombre", "string").add("edad", "integer")
    
csvDF = spark.readStream \
    .option("sep", ";") \
    .schema(esquemaUsuario) \
    .csv("/path/al/directorio")  # equivalente a format("csv").load("/path/al/directorio")
```

**Socket**: lee texto UTF8 desde una conexión *socket* (es el que hemos utilizado en el caso de uso 1). Sólo se debe utilizar para pruebas ya que no ofrece garantía de tolerancia de fallos de punto a punto.

```python
socketDF = spark.readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()
```

**Rate**: Genera datos indicando una cantidad de filas por segundo, donde cada fila contiene un *timestamp* y el valor de un contador secuencial (la primera fila contiene el 0). Esta fuente también se utiliza para la realización de pruebas y *benchmarking*.

```python
socketDF = spark.readStream \
    .format("rate") \
    .option("rowsPerSecond", 1)
    .load()
```

**Tabla** (desde *Spark 3.1*): Carga los datos desde una tabla temporal de *SparkSQL*, la cual podemos utilizar tanto para cargar como para persistir los cálculos realizados. Más información en la [documentación oficial](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-table-apis).

```python
tablaDF = spark.readStream \
    .table("clientes")
```

## Sinks

El término "sinks" se refiere a las operaciones de escritura de datos que se realizan al final de un proceso de streaming o transformación de datos. Un "sink" (o "destino" en español) en PySpark es el lugar donde se envían los datos procesados para su almacenamiento o uso posterior.

Escriben a partir de un [DataStreamWriter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html) mediante el interfaz [`writeStream`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.writeStream.html)

**Fichero**: Podemos almacenar los resultados en un sistema de archivos, HDFS o S3, con soporte para los formatos CSV, JSON, ORC y Parquet.

```python
# Otros valores pueden ser "json", "csv", etc...
df.writeStream.format("parquet") \        
    .option("path", "/path/al/directorio") \ 
    .start()
```

**Kafka**: Envía los datos a un clúster de *Kafka*:

```python
df.writeStream.format("kafka") \        
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
    .option("topic", "miTopic")
    .start()
```

**Foreach y ForeachBatch**: permiten realizar operaciones y escribir lógica sobre la salida de una consulta de *streaming*, ya sea a nivel de fila (*foreach*) como a nivel de micro-batch (*foreachBatch*). Más información en la [documentación oficial](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch).

**Consola**: se emplea para pruebas y depuración y permite mostrar el resultado por consola.

Admite las opciones `numRows` para indicar las filas a mostrar y `truncate` para truncar los datos si las filas son muy largas.

```python
df.writeStream.format("console") \        
    .start()
```

**Memoria**: se emplea para pruebas y depuración, ya que sólo permite un volumen pequeño de datos para evitar un problema de falta de memoria en el driver para almacenar la salida. Los datos se almacenan en una tabla temporal a la cual podemos acceder desde *SparkSQL*:

```python
df.writeStream.format("memory") \  
    .queryName("nombreTabla")  
    .start()
```

## Modos de salida

El [modo de salida](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes) determina cómo salen los datos a un sumidero de datos. Existen tres opciones:

- Añadir (`*append*`): para insertar los datos, cuando sabemos que no vamos a modificar ninguna salida anterior, y que cada *batch* únicamente escribirá nuevos registros. Es el modo por defecto.
- Modificar (`*update*`): similar a un *upsert*, donde veremos solo registros que, bien son nuevos, bien son valores antiguos que debemos modificar.
- Completa (`*complete*`): para sobrescribir completamente el resultado, de manera que siempre recibimos la salida completa.

En el caso 1 hemos utilizado el modo de salida completa, de manera que con cada dato nuevo, se mostraba como resultado todas las palabras y su cantidad. Si hubiésemos elegido el modo *update*, en cada micro-batch solo se mostraría el resultado acumulado de cada *batch*.

**Ejercicio**

1. Abrimos nc -lk -p 9999
2. Duplicamos Streaming1.ipynb a Streaming2.ipynb
3. Iniciamos
4. Mandamos dos frases 
    
    Esta es la **primera** frase
    
    Enviamos segunda linea **primera**
    

```markdown
-------------------------------------------
Batch: 2
-------------------------------------------
+--------+-----+
| palabra|count|
+--------+-----+
|   frase|    1|
| segunda|    1|
|      es|    1|
|   linea|    1|
|    Esta|    1|
|      la|    1|
|Enviamos|    1|
| primera|    1|
+--------+-----+
```

1. Modificamos el código del ejercicio, detenemos todo, volvemos a iniciar

```python
wordCountQuery = cantidadDF.writeStream \
    .format("console") \
    .outputMode("update") \
    .start()
```

1. Mandamos las mismas frases

```markdown
-------------------------------------------
Batch: 2
-------------------------------------------
+--------+-----+
| palabra|count|
+--------+-----+
| segunda|    1|
|   linea|    1|
|      la|    2|
|Enviamos|    1|
+--------+-----+

```

1. Modificamos el código del ejercicio, detenemos todo, volvemos a iniciar

```markdown
wordCountQuery = cantidadDF.writeStream \
    .format("console") \
    .outputMode("append") \
    .start()
```

Con este ejemplo, el modo *append* no tiene sentido (ya que para contar las palabras necesitamos las anteriores), y *Spark* es tan listo que cuando realizamos agregaciones no permite su uso y lanza una excepción del tipo `AnalysisException`:
`AnalysisException: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;`

En resumen, el modo append es sólo para inserciones, update para modificaciones e inserciones y finalmente complete sobrescribe los resultados previos.

Además, no todos los tipos de salida se pueden aplicar siempre, va a depender del tipo de operaciones que realicemos.

## Transformaciones

- **Sin estado (*stateless*)**: los datos de cada *micro-batch* son independientes de los anteriores, y por tanto, podemos realizar las transformaciones `select`, `filter`, `map`, `flatMap`, `explode`. Es importante destacar que estas transformaciones no soportan el modo de salida *complete*, por lo que sólo podemos utilizar los modos *append* o *update*.
- **Con estado (*stateful*)**: aquellas que implica realizar agrupaciones, agregaciones, *windowing* y/o *joins*, ya que mantienen el estado entre los diferentes *micro-batches*. Destacar que un abuso del estado puede causar problemas de falta de memoria, ya que el estado se almacena en la memoria de los ejecutores (*executors*). Por ello, *Spark* ofrece dos tipos de operaciones con estado:
    - Gestionadas (*managed*): *Spark* gestiona el estado y libera la memoria conforme sea necesario.
    - Sin gestionar (*unmanaged*): permite que el desarrollador defina las políticas de limpieza del estado (y su liberación de memoria), por ejemplo, a partir de políticas basadas en el tiempo. A día de hoy, las transformación sin gestionar sólo están disponibles mediante *Java* o *Scala*.

Además, hay que tener en cuenta que no todas las operaciones que realizamos con *DataFrames* están soportadas al trabajar en *streaming*, como pueden ser `show`, `describe`, `count` (aunque sí que podemos contar sobre agregaciones/funciones ventana), `limit`, `distinct`, `cube` o `sort` (podemos ordenar en algunos casos después de haber realizado una agregación), ya que los datos no están acotados y provocará una excepción del tipo `AnalysisException`.

## Triggers

Un [trigger](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers) define el intervalo (*timing*) temporal de procesamiento de los datos en *streaming*, indicando si la consulta se ejecutará como un *micro-batch* mediante un intervalo fijo o con una consulta con procesamiento continuo.

Así pues, un trigger es un mecanismo para que el motor de *Spark SQL* determine cuando ejecutar la computación en *streaming*.

Los posibles tipos son:

- ***Sin especificar***, de manera que cada micro-batch se va a ejecutar tan pronto como lleguen datos.
- ***Por intervalo de tiempo***, mediante la propiedad `processingTime`. Si indicamos un intervalo de un minuto, una vez finalizado un job, si no ha pasado un minuto, se esperará a ejecutarse. Si el micro-batch tardase más de un minuto, el siguiente se ejecutaría inmediatamente. Así pues, de esta manera, *Spark* permite colectar datos de entrada y procesarlos de manera conjunta (en vez de procesar individualmente cada registro de entrada).
- ***Un intervalo***, mediante la propiedad `once`, de manera que funciona como un proceso *batch* estándar, creando un único proceso *micro-batch*, o con la propiedad `availableNow` para leer todos los datos disponibles hasta el momento mediante múltiples *batches*.
- ***Continuo***, mediante la propiedad `continuous`, para permitir latencias del orden de milisegundos mediante [Continuous Processing](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing). Se trata de una opción experimental desde la versión 2.3 de Spark.

Los triggers se configuran al persistir el *DataFrame*, tras indicar el modo de salida mediante el método [trigger](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.trigger.html?highlight=trigger):

**Ejercicio**: Modifica Streaming2.ipynb

```python
wordCountQuery = cantidadDF.writeStream \
    .format("console") \
    .outputMode("complete") \
    .trigger(processingTime="1 minute") \
    .start()
```

# 3. Caso 2: Facturas

En este caso de uso vamos a poner en práctica algunos de los conceptos que acabamos de ver.

Vamos a suponer que tenemos un empresa compuesta de diferentes sucursales. Cada una de ellas, **cada 5 minutos** genera un fichero con los datos de las facturas ([*invoices.zip*](https://aitor-medrano.github.io/iabd/spark/resources/invoices.zip)) que han generado. Cada una de las facturas contiene una o más líneas de factura, las cuales queremos separar en facturas simples.

Así pues, vamos a partir de documentos JSON con la siguiente estructura:

![Untitled](<./UD05 8 Streaming fffe913de6c4819a9593edd001b23257/Untitled 1.png>)

Y a partir de él, generaremos 4 documentos (uno por cada línea de factura) con la siguiente estructura:

![Untitled](<./UD05 8 Streaming fffe913de6c4819a9593edd001b23257/Untitled 2.png>)

## Cargando los datos

[https://github.com/josepgarcia/datos/raw/main/invoices/invoices.zip](https://github.com/josepgarcia/datos/raw/main/invoices/invoices.zip)

Creamos el archivo invoices.ipynb

1. Iniciar sesión spark, leer todas las facturas (formato json).
2. Imprimir el schema (lo inferimos, no hace falta indicarlo).
- **SOL**
    
    ```python
    from pyspark.sql import SparkSession
    spark = SparkSession \
            .builder \
            .appName("Streaming de Ficheros") \
            .master("local[2]") \
            .config("spark.streaming.stopGracefullyOnShutdown", "true") \
            .config("spark.sql.shuffle.partitions", 3) \
            .config("spark.sql.streaming.schemaInference", "true") \
            .getOrCreate()
    
    raw_df = spark.readStream \
            .format("json") \
            .option("path", "PATH_CORRESPONDIENTE_INVOICES") \
            .load()
    
    raw_df.printSchema()
    # root
    #  |-- CESS: double (nullable = true)
    #  |-- CGST: double (nullable = true)
    #  |-- CashierID: string (nullable = true)
    #  |-- CreatedTime: long (nullable = true)
    #  |-- CustomerCardNo: string (nullable = true)
    #  |-- CustomerType: string (nullable = true)
    #  |-- DeliveryAddress: struct (nullable = true)
    #  |    |-- AddressLine: string (nullable = true)
    #  |    |-- City: string (nullable = true)
    #  |    |-- ContactNumber: string (nullable = true)
    #  |    |-- PinCode: string (nullable = true)
    #  |    |-- State: string (nullable = true)
    #  |-- DeliveryType: string (nullable = true)
    #  |-- InvoiceLineItems: array (nullable = true)
    #  |    |-- element: struct (containsNull = true)
    #  |    |    |-- ItemCode: string (nullable = true)
    #  |    |    |-- ItemDescription: string (nullable = true)
    #  |    |    |-- ItemPrice: double (nullable = true)
    #  |    |    |-- ItemQty: long (nullable = true)
    #  |    |    |-- TotalValue: double (nullable = true)
    #  |-- InvoiceNumber: string (nullable = true)
    #  |-- NumberOfItems: long (nullable = true)
    #  |-- PaymentMethod: string (nullable = true)
    #  |-- PosID: string (nullable = true)
    #  |-- SGST: double (nullable = true)
    #  |-- StoreID: string (nullable = true)
    #  |-- TaxableAmount: double (nullable = true)
    #  |-- TotalAmount: double (nullable = true)
    ```
    
    Aunque en este caso hemos realizado la inferencia de la estructura de los datos de entrada, lo normal es indicar el esquema de los datos.
    

## Proyectando

El siguiente paso es seleccionar los datos que nos interesan. Para ello, tras revisar la estructura de salida que deseamos, realizamos una selección de las columnas y utilizaremos la función `explode` para desenrollar el array de facturas `InvoiceLineItems`:

```python
explode_df = raw_df.selectExpr("InvoiceNumber", "CreatedTime", "StoreID",
                                 "PosID", "CustomerType",
                                 "PaymentMethod", "DeliveryType",
                                 "explode(InvoiceLineItems) as LineItem")
explode_df.printSchema()
```

Tras ello, vamos a renombrar los campos para quitar los campos anidados (creando columnas nuevas con el nombre deseando y eliminando la columna `LineItem`):

```python
from pyspark.sql.functions import expr
limpio_df = explode_df \
    .withColumn("ItemCode", expr("LineItem.ItemCode")) \
    .withColumn("ItemDescription", expr("LineItem.ItemDescription")) \
    .withColumn("ItemPrice", expr("LineItem.ItemPrice")) \
    .withColumn("ItemQty", expr("LineItem.ItemQty")) \
    .withColumn("TotalValue", expr("LineItem.TotalValue")) \
    .drop("LineItem")
limpio_df.printSchema()
```

## Guardando el resultado

Una vez tenemos el proceso de transformación de datos, sólo nos queda crear el *WriterQuery* para escribir el resultado del flujo de datos. En este caso, vamos a almacenarlo también como ficheros en la carpeta `salida` en formato *JSON* a intervalos de un minuto:

(writeStream)

.trigger(processingTime="1 minute")

- **SOL**
    
    ```python
    facturaWriterQuery = limpio_df.writeStream \
        .format("json") \
        .queryName("Facturas Writer") \
        .outputMode("append") \
        .option("path", "./salida") \
        .option("checkpointLocation", "chk-point-dir-caso2") \
        .trigger(processingTime="1 minute") \
        .start()
    ```
    

## Refinando el resultado

Una vez que vemos que todo funciona, podemos realizar unos ajustes de configuración.

Por ejemplo, vamos a configurar que sólo consuma un fichero cada vez. Para ello, en el *reader* configuramos la opción `maxFilesPerTrigger`, la cual permite limitar la cantidad de ficheros de cada micro-batch.

Otras opciones que se usan de manera conjunta son `cleanSource` y `sourceArchiveDir`, que permiten archivar los ficheros procesados de forma automática. La opción `cleanSource` puede tomar los valores `archive` o `delete`. Si decidimos archivar, mediante `sourceArchiveDir` indicamos el destino donde se moverán.

```python
raw_df = spark.readStream \
    .format("json") \
    .option("path", "../datos/invoices") \
    .option("maxFilesPerTrigger", 1) \
    .option("cleanSource", "delete") \
    .load()
```

Hay que tener en cuenta, que tanto archivar como eliminar van a impactar negativamente en el rendimiento de cada micro-batch. Nosotros hemos de limpiar el directorio de entrada, eso es un hecho. Si ejecutamos *micro-batch* largos, podemos usar la opción `cleanSource`. En cambio, si nuestros *batches* son muy cortos y el utilizar `cleanSource` no es factible por la demora que introduce, debemos crear un proceso de limpieza separado que se ejecute cada X horas y que limpie nuestro directorio de entrada.

## Monitorización

Una vez hemos realizado una consulta, podemos obtener [información](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#managing-streaming-queries) sobre la misma de forma programativa:

```python
facturaWriterQuery.explain() # muestra una explicación detalla del plan de ejecución
# == Physical Plan ==
# *(1) Project [InvoiceNumber#314, CreatedTime#308L, StoreID#319, PosID#317, CustomerType#310, PaymentMethod#316, DeliveryType#312, _extract_City#339 AS City#52, _extract_State#340 AS State#53, _extract_PinCode#341 AS PinCode#54, LineItem#55.ItemCode AS ItemCode#67, LineItem#55.ItemDescription AS ItemDescription#81, LineItem#55.ItemPrice AS ItemPrice#96, LineItem#55.ItemQty AS ItemQty#112L, LineItem#55.TotalValue AS TotalValue#129]
# ...
facturaWriterQuery.recentProgress # muestra una lista de los últimos progresos de la consulta
# [{'id': '3b6d37cf-6a3c-405e-a715-1dc787f34b00',
#   'runId': '3dc7c478-626a-4558-87ea-4912da55114d',
#   'name': 'Facturas Writer',
#   'timestamp': '2024-05-11T08:20:49.058Z',
#   'batchId': 0,
#   'numInputRows': 500,
#   'inputRowsPerSecond': 0.0,
#   'processedRowsPerSecond': 113.55893708834887,
#   'durationMs': {'addBatch': 2496,
# ...
facturaWriterQuery.lastProgress # muestra el último progreso
# {'id': '3b6d37cf-6a3c-405e-a715-1dc787f34b00',
#  'runId': '3dc7c478-626a-4558-87ea-4912da55114d',
#  'name': 'Facturas Writer',
#  'timestamp': '2024-05-11T08:33:00.001Z',
#  'batchId': 3,
#  'numInputRows': 0,
#  'inputRowsPerSecond': 0.0,
#  'processedRowsPerSecond': 0.0,
#  'durationMs': {'latestOffset': 5, 'triggerExecution': 8},
# ...
```

Estas mismas estadísticas las podemos obtener de forma gráfica. Al ejecutar procesos en Streaming, si accedemos a Spark UI, ahora podremos ver la pestaña *Structured Streaming* con información detallada de la cantidad datos de entrada, tiempo procesado y duración de los micro-batches:

![Untitled](<./UD05 8 Streaming fffe913de6c4819a9593edd001b23257/Untitled 3.png>)

Además, podemos iniciar tantas consultas como queramos en una única sesión de Spark, las cuales se ejecutarán de forma concurrente utilizando los recursos del clúster de Spark.

<aside>
✅ Para acceder a la webUI deberíamos de mapear los puertos tal y como lo hicimos en:

[UD05 4. PySpark. Contexto, sesión y RDDs.](<./UD05 4 PySpark Contexto, sesión y RDDs fffe913de6c48188a3bae07c2b6ff4e1.md>)

docker run -it -p 8888:8888 -p **4040:4040 -p 4041:4041 -p 4042:4042** -v $(pwd):/home/jovyan/work/projects/ jupyter/pyspark-notebook

</aside>

## Tolerancia a fallos

Un aplicación en *streaming* se espera que se ejecute de forma ininterrumpida mediante un bucle infinito de micro-batches.

Realmente, un escenario de ejecución infinita no es posible, ya que la aplicación se detendrá por:

- un fallo, ya sea por un dato mal formado o un error de red.
- mantenimiento del sistema, para actualizar la aplicación o el hardware donde corre.

Para tratar la tolerancia a fallos, existen tres escenarios posibles:

- Una vez como mucho (*at most once*): no se entrega más de una copia de un dato. Es decir, puede darse el caso de que no llegue, pero no habrá repetidos.
- Una vez al menos (*at least once*): en este caso no habrá pérdidas, pero un dato puede llegar más de una vez.
- Una vez exacta (*exactly once*): se garantiza que cada dato se entrega una única vez, sin pérdidas ni duplicados.

![Untitled](<./UD05 8 Streaming fffe913de6c4819a9593edd001b23257/Untitled 4.png>)

Por ello, una aplicación *Spark Streaming* se debe reiniciar de forma transparente para mantener la característica de ***exactly-once*** la cual implica que:

1. No se pierde ningún registro
2. No crea registros duplicados.

Para ello, *Spark Structured Streaming* mantiene el estado de los micro-batches mediante [*checkpoints*](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing) que se almacenan en la carpeta indicada por la opción `checkpointLocation`:

```python
facturaWriterQuery = limpio_df.writeStream \
    .format("json") \
    .queryName("Facturas Writer") \
    .outputMode("append") \
    .option("path", "salida") \
    **.option("checkpointLocation", "chk-point-dir-caso2") \**
    .trigger(processingTime="1 minute") \
    .start()
```

La localización de esta carpeta debería ser un sistema de archivo confiable y tolerante a fallos, como HDFS o Amazon S3.

Esta carpeta contiene dos elementos:

- Posición de lectura, que realiza la misma función que los *offset* en Kafka, y representa el inicio y el final del rango de datos procesados por el actual *micro-batch*, de manera que *Spark* conoce el progreso exacto del procesamiento. Una vez ha finalizado el *micro-batch*, *Spark* realiza un *commit* para indicar que se han procesado los datos de forma exitosa.
- Información del estado, que contiene los datos intermedios del *micro-batch*, como la cantidad total de palabras contadas.

De esta manera, *Spark* mantiene toda la información necesaria para reiniciar un micro-batch que no ha finalizado. Sin embargo, la capacidad de reiniciarse no tiene por qué garantizar la política *exactly-once*. Para ello, es necesario cumplir los siguientes requisitos:

1. Reiniciar la aplicación con el mismo `checkpointLocation`. Si se elimina la carpeta o se ejecuta la misma consulta sobre otro directorio de *checkpoint* es como si realizásemos una consulta desde 0.
2. Utilizar una fuente de datos que permita volver a leer los datos incompletos del *micro-batch*, por ejemplo, tanto los ficheros de texto como *Kafka* permiten volver a leer los datos desde un punto determinado. Sin embargo, los datos que provienen de un socket no permite volver a leerlos.
3. Asegurar que la lógica de aplicación, dados los mismos datos de entrada, produce siempre el mismo resultado (aplicación determinista). Si por ejemplo, nuestra lógica de aplicación utilizará alguna dependencia basada en fechas o el tiempo, ya no obtendríamos el mismo resultado.
4. El destino (*sink*) debe ser capaz de identificar los elementos duplicados e ignorarlos o actualizar la copia antigua con el mismo registro, es decir, son idempotentes.

# 4. Ejercicio Bizum

<aside>
✅ **Entregar AULES**

</aside>

El archivo bizums.zip contiene una simulación de datos de *bizum* que llegan a nuestra cuenta. 

[https://github.com/josepgarcia/datos/raw/main/bizums/bizums.zip](https://github.com/josepgarcia/datos/raw/main/bizums/bizums.zip)

1. Crea un *script* *Python* que, al ejecutarlo, se encargue de simular el envío de *bizums.*

```python
import os
import shutil
import shutil, os
from random import randint, uniform,random

import time

ruta = os.getcwd() + os.sep

#bizum = input('Bizum: ')

i = 1
while i <= 20:
	num_aleatorio = randint(0,20)
	origen = ruta + 'Sin_llegar' + os.sep + 'Bizum' +str(num_aleatorio) + '.csv'
	destino = ruta + 'recibidos' + os.sep + 'Bizum' +str(num_aleatorio) + '.csv'
	try:
	    shutil.copyfile(origen, destino)
	    print("Bizum recibido")
	    i=i+1
	except:
		print("Error al enviar bizum")
		i=i+1
	time.sleep(10)%
```

1. Crea una aplicación de *Spark Streaming* que muestre para cada persona, cual **es el *bizum* más alto**.

El formato de estos datos es CSV formado por el `Nombre;Cantidad;Concepto` (dos nombres en mayúsculas y en minúsculas son de la misma persona, convertir a lowercase). Un ejemplo de un *bizum* recibido sería similar a:

```bash
Aitor;25;Cena restaurante
```

Muestra el resultado completo por consola.