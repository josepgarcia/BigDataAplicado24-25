# Scala - Spark - Hadoop.

## Scala
https://scala-lang.org/
Scala es un lenguaje de programación moderno multi-paradigma diseñado para expresar patrones de programación comunes de una forma concisa, elegante, y con tipado seguro. Integra fácilmente características de lenguajes orientados a objetos y funcionales.

## Spark shell HDFS

Archivo utilizado: `puertos.csv`
```
https://github.com/josepgarcia/datos/blob/main/puertos.csv
```

Creamos un directorio en el sistema HDFS para guardar los ficheros de datos. Y subimos al directorio creado en HDFS “spark” el fichero facilitado en la práctica "puertos.csv" (datos de cargas de puertos marítimos) o lo creamos en la máquina y copiamos su contenido y guardamos.

```bash
hdfs dfs -mkdir /spark
hdfs dfs -put puerto.csv /spark
```

Creamos una sesión con "spark-shell" para conectar en el entorno Spark con la terminal Scala.

```bash
spark-shell
```

Vamos acceder a este fichero del sistema HDFS con Scala. Ya lo identifica a través de las variables ya definidas.

```scala
val vl = sc.textFile("/spark/puertos.csv")
vl.count()
```

> [!Info] Info
> El comando que utilizamos en el punto anterior, para cargar un fichero desde fuera de hadoop era:
	`scala> val fichero=sc.textFile("file:///opt/hadoop/spark/README.md")`

Vamos a hacer una búsqueda sobre el RDD buscando los puertos de Barcelona y guardarlo en otro RDD.

```bash
val v2 = vl.filter(line=>line.contains("Barcelona"))
v2.count()
```

## Spark scala → Hadoop

Ahora vamos a lanzar una aplicación SPARK con Scala contra YARN. El argumento más importante es el “—master yarn” que indica que vamos a ejecutarlo sobre Hadoop, no contra local.
**Nota: Ejecutar desde fuera del interprete de Scala/Python.**

spark-submit: Para lanzar un comando contra un claster hadoop
```
-class → Clase a utilizar
-master yarn → Ejecutamos el comando contra nuestro cluster hadoop
-deploy-mode cluster → Para que se ejecute en modo cluster, en todos los nodos
-name → para poder localizarla
.jar → fichero de ejemplos (el nuestro calcula el número PI)
```


```bash
:quit ## Salimos de spark-shell
```

```
spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster --name "apli1" /opt/hadoop/spark/examples/jars/spark-examples_2.12-3.3.0.jar 5
```

**Código de SparkPi**
![](<./images/spark1.png>)

Al lanzarlo se conecta al cluster, sube el fichero para trabajar con él…

Pasa a la cola → Se acepta ACCEPTED → RUNNING → FINISHED

Una vez finalizado el proceso, vamos a la web de monitorización y buscamos en el apartado de aplicaciones finalizadas con éxito:
http://IP_MAQUINA_HADOOP:8088/

Buscamos el apartado de los logs que ha generado la ejecución de la aplicación “apli1”. El fichero stdout tendremos la salida con el **resultado** de la ejecución.
![](<./images/spark2.png>)

** La aplicación se ha lanzado sobre el cluster, lo que significa que ha utilizado todos sus recursos disponibles (nodos, memoria, disco…)

## Spark python → Hadoop

Vamos a crear una aplicación con python y lanzarla contra Hadoop.
`contarPalabras.py`

Fichero a contar:
https://github.com/josepgarcia/datos/blob/main/el_quijote.txt

```python
import sys
try:
		# Permiten poner el entorno, contexto de spark (anteriormente contexto sc sparkcontext)
    from pyspark import SparkConf, SparkContext

    conf = SparkConf() # Creamos la configuración
    sc = SparkContext(conf=conf) # Creamos el contexto
    inputPath = sys.argv[1] # Parámetro 1 entrada, fichero
    outputPath = sys.argv[2] # Parámetro 2 entrada, directorio salida

		# Opciones para poder acceder a HDFS
    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path 
    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem
    Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration
    fs = FileSystem.get(Configuration())

    if(fs.exists(Path(inputPath)) == False):
        print("El fichero de entrada no existe")
    else:
        if(fs.exists(Path(outputPath))):
            fs.delete(Path(outputPath), True)
				# Divide el contenido del fichero en espacios en blanco y va contando (como wordcount de java)
				# El resultado lo deja en outputPath
        sc.textFile(inputPath).flatMap(lambda l: l.split(" ")).map(lambda w: (w, 1)).reduceByKey(lambda t, e: t + e).saveAsTextFile(outputPath)

    print ("Se han importado los modulos de Spark")

except ImportError as e:
    print ("No puedo importar los modulos de Spark", e)
    sys.exit(1)
```

Lo lanzamos con spark-submit
```bash
# Todo el comando en la misma línea
spark-submit --master yarn --deploy-mode cluster --name "ContarPalabras"
/home/hadoop/contarPalabras.py /tmp/el_quijote.txt
/salida_spark_wc
```

> [!ERROR] ERROR
> Dará un error, hay que solucionarlo.

![](<./images/spark3.png>)
![](<./images/spark4.png>)
**Error de ejecución, no encuentra el fichero del quijote**
![](<./images/spark5.png>)
## Spark UI

Cuando lanzamos `spark-shell` y creamos un context, automáticamente se levanta la UI de spark en el puerto 4040.
![](<./images/spark6.png>)
