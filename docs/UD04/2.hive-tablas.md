# Hive. Tablas.

## Tipos de tablas
### Tablas Internas

- Son gestionadas completamente por Hive.
- Los datos y la estructura están bajo el control total de Hive, incluyendo la eliminación de datos cuando se elimina la tabla.

```bash
CREATE TABLE empleados (
  id INT,
  nombre STRING,
  salario DOUBLE
);
```

### Tablas Externas

- Hive gestiona solo la estructura de metadatos, pero los datos residen fuera de Hive.
- Los datos **NO** se eliminan automáticamente cuando se elimina la tabla en Hive.

```bash
CREATE EXTERNAL TABLE empleados_externos (
  id INT,
  nombre STRING,
  salario DOUBLE
)
LOCATION '/ruta/en/hadoop/datos/empleados';
```

En ambos casos, estas tablas pueden ser consultadas y utilizadas en consultas SQL dentro de Hive, pero la diferencia clave radica en la gestión de los datos y la persistencia.

### ¿Cuáles utilizamos?

¿Por qué utilizar tablas internas? ¿en qué casos?

- Se necesita que Hive se encargue de garantizar la integridad y coherencia de datos.
- Gestión de recursos, con tablas internas se elimina el dato completamente.
- Hive puede optimizar consultas ya que tiene el control total sobre la distribución y organización de los datos.
- Proporcionan un extra de seguridad ya que se gestionan políticas de seguridad a nivel de Hive.

El uso de tablas internas en Hive es beneficioso cuando se busca un control más granular sobre la administración de datos, garantizando la consistencia, la seguridad y la eficiencia en los procesos analíticos y de negocio.

¿Por qué utilizar datas externas? ¿en qué casos?

- Cuando los datos son compartidos entre varios sistemas.
- Si los datos son generados por procesos externos a Hive.
- Cuando los datos están en formatos específicos (orc, avro…) y se quiere acceder a ellos sin tener que realizar una carga previa de los datos.
- Cuando los datos se encuentran en almacenamientos externos como Amazon S3, Azure y se desea acceder a ellos sin moverlos físicamente a HDFS.

Las tablas externas son ideales cuando se trata de acceder a datos que están fuera del control directo de Hive, proporcionando flexibilidad en el acceso a información compartida entre sistemas y permitiendo la lectura directa de datos en su ubicación original.

## Internas

Nos creamos un fichero plano con una nueva terminal.

```bash
cd /tmp/hadoop

vi empleados.txt
Rosa,50
Pedro,60
Raul,56
Maria,35
```

Vamos a la terminal conectada a Hive y la base de datos ejemplo. Creamos una nueva tabla empleados especificando que la cargaremos de fichero especificando que esta formateadas por filas y separadas por comas.

```sql
hive>
create table empleados
(
nombre string,
edad integer
)
row format delimited
fields terminated by ',';
load data local inpath '/tmp/hadoop/empleados.txt' into table empleados;
```

Volvemos a repetir la creación de otro fichero (p.e. copiando los datos del anterior) y lo cargamos en Hive. Veremos que Hive lo guarda todo como ficheros en el directorio correspondiente de hdfs.

```sql
load data local inpath '/tmp/hadoop/empleados2.txt' into table empleados; 
## (local especifica que busca el fichero en la maquina local)
```

Podemos realizar búsquedas:

```bash
select * from empleados where edad > 50;
```

** Accedemos a web HDFS para ver cómo ha importado los archivos) **

Si ahora borramos la tabla con Hive. Veremos cómo desaparece tanto la tabla como los ficheros en **HDFS**. Es el propio Hive quien gestiona los datos a todos los niveles.

```sql
drop table empleados;
```

## Externas

Vamos a subir el fichero de empleados a través de hdfs.

```bash
hdfs dfs -mkdir /prueba
hdfs dfs -put empleados.txt /prueba
```

Creamos la tabla como external y especificando la localización. Visualizaremos como crea automáticamente el directorio en el sistema hdfs. Luego cargamos la información del fichero.

```sql
create external table empleados
(
nombre string,
edad integer
)
row format delimited
fields terminated by ','
location '/user/hive/datos/empleados';
```

location: dónde vamos a tener los datos.

** Entrar en webdfs para ver que se ha creado un directorio llamado datos en la carpeta que hemos indicado.

Cargamos los datos:

```bash
load data inpath '/prueba/empleados.txt' into table empleados;
```

Ahora ya podemos ver el fichero desde webdfs.

Lo más importante de las tablas externas, es que cuando hacemos ahora un drop de la tabla, el fichero no desaparece en hdfs.

```sql
drop table empleados;
```

Podemos ver que aún existen el webdfs.
