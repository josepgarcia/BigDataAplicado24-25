# Procesamiento de datos en tiempo real

## Procesamiento en Lote vs. Procesamiento en Tiempo Real

**Procesamiento en lote (Batch Processing):**
	• Se envía un trabajo (por ejemplo, MapReduce, Spark o Hive) usando Yarn.
	• El sistema ejecuta el trabajo y devuelve una respuesta en pantalla o escribe la salida en HDFS.
	• Se ejecuta en intervalos regulares (cada hora, cada día, etc.).
	• Es ideal para comenzar con Big Data, pero no es adecuado cuando se requiere procesar datos en intervalos muy cortos.

**Procesamiento en tiempo real (Real-Time Processing):**
		• Se utiliza cuando hay necesidad de procesar los datos casi inmediatamente después de que llegan.
		• Requiere tecnologías y arquitecturas específicas para manejar flujos constantes de datos.


## Tecnologías Necesarias para Procesamiento en Tiempo Real
**Ingesta y almacenamiento temporal de datos:**
	• Los datos entrantes deben almacenarse temporalmente antes de ser procesados.
**Problema con HDFS:**
	• HDFS no es eficiente para leer eventos individuales (como una línea o un registro).
	• Está diseñado para leer bloques completos de datos.
	
**SOLUCIÓN Kafka:**
	• Kafka es un sistema de mensajería basado en el modelo **publicación-suscripción (pub-sub)**.
	• Actúa como un **log distribuido** que almacena eventos temporalmente en un buffer mientras esperan para ser procesados.

## Procesamiento de Eventos en Tiempo Real
Los servicios que procesan los datos deben estar en ejecución continua dentro del clúster de Hadoop y deben recuperar datos desde sistemas de colas como Kafka.
 **Diferentes enfoques para el procesamiento de eventos:**
• **Storm:** Procesa cada evento de manera individual.
• **Storm + Trident:** Permite agrupar eventos (batching) y procesar múltiples eventos a la vez.
• **Spark Streaming:** Utiliza un enfoque de **micro-lotes (micro-batching)**, agrupando eventos dentro de intervalos de tiempo cortos (por ejemplo, cada pocos segundos).

## Almacenamiento Persistente de Datos Procesados
Después de procesar los datos, deben almacenarse para consultas posteriores.

**Problema con HDFS:**
• No es adecuado para manejar muchos archivos pequeños.
• Una posible solución sería anexar eventos a un único archivo grande, pero esto hace que la lectura de un evento específico sea difícil.

**Soluciones ideales:**
• Utilizar almacenes de datos distribuidos como:
• **HBase:** Diseñado para escritura rápida y lectura eficiente cuando se conoce la clave de fila.
• **Accumulo:** Similar a HBase, pero con funcionalidades adicionales como control de acceso granular.
![](<./images/Pasted image 20250112190406.png>)

[https://aitor-medrano.github.io/iabd/dataflow/kafka1.html](https://aitor-medrano.github.io/iabd/dataflow/kafka1.html)
## Kafka

[Apache Kafka](https://kafka.apache.org/) es, en pocas palabras, un *middleware* de mensajería entre sistemas heterogéneos, el cual, mediante un sistema de colas (*topics*, para ser concreto) facilita la comunicación asíncrona, desacoplando los flujos de datos de los sistemas que los producen o consumen. Funciona como un *broker* de mensajes, encargado de enrutar los mensajes entre los clientes de un modo muy rápido.

Supongamos que tenemos múltiples generadores de datos, ya sean servidores web, de bases de datos, un servidor de chat y que todos ellos tienen que almacenar sus datos en múltiples destinos, como pueden ser logs, métricas de rendimiento y monitorización, el carrito de la compra o los fallos ocurridos, lo que puede provocar una serie de dependencias de unos con otros. Para evitarlo, *Kafka* viene al rescate conectando todos los generadores de datos (productores) a Kafka y a su vez, a todos los consumidores de estos datos.

En concreto, se trata de una plataforma *open source* **distribuida** de **transmisión de eventos/mensajes** en tiempo real con almacenamiento duradero y que proporciona de base un alto rendimiento (capaz de manejar billones de peticiones al día, con una latencia inferior a 10ms), tolerancia a fallos, disponibilidad y escalabilidad horizontal (mediante cientos de nodos).

Más del 80% de las 100 compañías más importantes de EEUU utilizan Kafka: Uber, Twitter, Netflix, Spotify, Blizzard, LinkedIn, Spotify, y PayPal procesan cada día sus mensajes con Kafka.

[https://kafka.apache.org/powered-by](https://kafka.apache.org/powered-by)

Como sistema de mensajes, sigue un modelo publicador-suscriptor. Su arquitectura tiene dos directivas claras:

- No bloquear los productores (para poder gestionar la [*back pressure*](https://youtu.be/K3axU2b0dDk), la cual sucede cuando un publicador produce más elementos de los que un suscriptor puede consumir).
- Aislar los productores y los consumidores, de manera que los productores y los consumidores no se conocen.

A día de hoy, *Apache Kafka* se utiliza, además de como un sistema de mensajería, para ingestar datos, realizar procesado de datos en streaming y analítica de datos en tiempo real, así como en arquitectura de microservicios y sistemas IOT.

### Publicador / Suscriptor (Productor / consumidor)

Antes de entrar en detalle sobre *Kafka*, hay que conocer el modelo publicador/suscriptor. Este patrón también se conoce como *publish / subscribe* o *productor / consumidor*.

Hay tres elementos que hay que tener realmente claros:

- **Publicador** (*publisher* / productor / emisor): genera un dato y lo coloca en un *topic* como un mensaje.
- **Topic** (tema): almacén temporal/duradero que guarda los mensajes funcionando como una cola.
- **Suscriptor** (*subscriber* / consumidor / receptor): recibe el mensaje.

Cabe destacar que un productor no se comunica nunca directamente con un consumidor, siempre lo hace a través de un *topic*:
![](<./images/kafka.png>)
[https://www.youtube.com/watch?v=wO6DCLU4uxE](https://www.youtube.com/watch?v=wO6DCLU4uxE)

Kafka garantiza que...
- Los mensajes se añaden a una partición/*topic* en el orden en el que se envían
- Los consumidores leen los mensajes en el orden en que se almacenaron en la partición/*topic*
- Con un factor de replicación N, los productores y consumidores pueden soportar que se caigan N-1 brokers.
   - Por ejemplo, con un factor de replicación de 3 (el cual es un valor muy apropiado), podemos tener un nodo detenido para mantenimiento y podemos permitirnos que otro de los nodos se caiga de forma inesperada.
- Mientras el número de particiones de un *topic* permanezca constante (no se hayan creado nuevas particiones), la misma clave implicará que los mensajes vayan a la misma partición.

## Zookeeper
Es un servicio para mantener la configuración, coordinación y aprovisionamiento de aplicaciones distribuidas dentro del ecosistema de *Apache*. No sólo se utiliza en *Hadoop*, pero es muy útil ya que elimina la complejidad de la gestión distribuida de la plataforma.

En el caso de *Kafka*, *Zookeeper*:
- gestiona los *brokers* (manteniendo una lista de ellos).
- ayuda en la elección de la partición líder
- envía notificaciones a *Kafka* cuando hay algún cambio (por ejemplo, se crea un *topic*, se cae un broker, se recupera un *broker*, al eliminar un *topic*, etc...).

Por todo ello, *Kafka* no puede funcionar sin *Zookeeper*.

En un entorno real, se instalan un número impar de servidores *Zookeeper* (3, 5, 7). Para su gestión, *Zookeeper* define un líder (gestiona las escrituras) y el resto de los servidores funcionan como réplicas de lectura.
![](<./images/kafka1.png>)

Pese a su dependencia, los productores y consumidores no interactúan nunca con *Zookeeper*, sólo lo hacen con *Kafka*.

Aunque podemos decir que todavía no es la opción más recomendable en producción, también podemos utilizar una instalación de Kafka sin Zookeeper haciendo uso de [Kraft](https://kafka.apache.org/documentation/#kraft), el cual ofrece un nuevo protocolo de consenso y evita tener una infraestructura extra para Zookeeper.

## Instalación

### En local

[https://kafka.apache.org/quickstart](https://kafka.apache.org/quickstart)

### Contenedores

[Apache Kafka con docker compose (producer y consumer)](https://www.youtube.com/watch?v=rhUuD0eA-EQ)

docker-compose.yml
```docker
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - 2181:2181

  kafka-broker-1:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-broker-1
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://broker:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
```

```bash
docker-compose up
```

Abrimos 2 terminales:

```bash
# Terminal 1
docker exec -it kafka-broker-1 bash
# Terminal 2
docker exec -it kafka-broker-1 bash

# Terminal 1, creamos topic test1
$ kafka-topics --bootstrap-server kafka-broker-1:9092 --create --topic test1
## (productor) Escribimos mensajes dentros de este topic
$ kafka-console-producer  --bootstrap-server kafka-broker-1:9092 --topic test1
> hola
> uno 
> dos 
> tres

# Terminal 2, creamos consumidor
## (consumidor)
kafka-console-consumer --bootstrap-server kafka-broker-1:9092 --topic test1 --from-beginning
```

Abrimos otro terminal

```bash
docker exec -it kafka-broker-1 bash

# Creamos nuevo topic
$ kafka-topics --bootstrap-server kafka-broker-1:9092 --create --topic test2

# Listamos los topics disponibles
$ kafka-topics --list --bootstrap-server kafka-broker-1:9092

# Mandamos mensajes al topic "test2"
kafka-console-producer  --bootstrap-server kafka-broker-1:9092 --topic test2

# No se muestran por la terminal2, ya que se encuentra escuchando
# otra "categoría"
```

## Ejemplos

Posibles ERRORES se solucionan con la versión *-ng :

```bash
pip install kafka-python-ng
```

### Producer desde python

```python
'''producer.py'''
'''pip install kafka-python'''

from kafka import KafkaProducer
import time

# Configuración del servidor Kafka y el tema al que enviaremos mensajes
bootstrap_servers = ['127.0.0.1:9092']
topic_name = 'test1'

# Crear un productor de Kafka
producer = KafkaProducer(bootstrap_servers=bootstrap_servers)

# Enviar mensajes al tema
for i in range(1, 6):
        mensaje = f"Mensaje {i}"
        producer.send(topic_name, mensaje.encode('utf-8'))
        print(f"Mensaje enviado: {mensaje}")
        time.sleep(1)  # Esperar 1 segundo entre cada mensaje

producer.close()

```

### Consumer desde python

```python
'''consumer.py'''
'''pip install kafka-python'''

from kafka import KafkaConsumer

# Configuración del servidor Kafka y el tema al que nos conectaremos
bootstrap_servers = ['localhost:9092']
topic_name = 'mi_tema'

# Crear un consumidor de Kafka y configurarlo para leer desde el inicio del tema
consumer = KafkaConsumer(topic_name,
                         group_id='grupo1',
                         bootstrap_servers=bootstrap_servers,
                         auto_offset_reset='earliest')

# Leer mensajes del tema desde el inicio
for mensaje in consumer:
    print(f"Mensaje recibido: {mensaje.value.decode('utf-8')}")
```

### Producer - Consumer desde node

```bash
npm init

npm install kafka-node
```

```jsx
const kafka = require('kafka-node');

const client = new kafka.KafkaClient({kafkaHost: '127.0.0.1:9092'})

/* Consumer */
var consumer = new kafka.Consumer(client, [{topic:'test1'}])
consumer.on('message', function(message) {
  console.log(message)
});

/* Producer */
var producer = new kafka.Producer(client);
producer.on('ready', function () {
    setInterval(function() {
            producer.send( [ { topic: "test1", messages: "Mensaje automático cada 5 seg." } ], function (err,data) {} );
          }, 5000);
});
```

```bash
node index.js

{
  topic: 'test1',
  value: 'hola',
  offset: 0,
  partition: 0,
  highWaterOffset: 7,
  key: null
}
{
  topic: 'test1',
  value: 'uno dos',
  offset: 1,
  partition: 0,
  highWaterOffset: 7,
  key: null
}
```

# Actividades

<aside>
✅ **Entregar AULES**

</aside>

A partir de un *topic* denominado `iabd-python-topic` y utilizando *Python* crea:
1. un productor que envíe datos de personas cada 10 segundos al *topic*. (5 datos por persona)
2. un consumidor que reciba las personas y, mediante *PyMongo*, las inserte en *MongoDB* en una colección llamada `kafka_personas`.


Utiliza la librería FAKER para generar los datos en python.


### Pipeline básico de Kafka y Spark Streaming para procesar datos en tiempo real

```
# Iniciar Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties
# Iniciar Kafka
bin/kafka-server-start.sh config/server.properties

2. **Crear un tema (topic):**
• Un **topic** es un canal donde Kafka almacena mensajes.
bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
• Usa el productor de Kafka para enviar mensajes.
bin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092


```

**Paso 2: Crear un Programa de Spark Streaming**  
Este programa en Python (usando PySpark) consumirá mensajes de Kafka y los procesará en micro-lotes.
```
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StringType, StructType

# Crear la sesión de Spark
spark = SparkSession.builder \
    .appName("KafkaSparkStreaming") \
    .getOrCreate()

# Leer datos en streaming desde Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "test-topic") \
    .load()

# Definir el esquema de los mensajes
schema = StructType().add("key", StringType()).add("value", StringType())

# Procesar los mensajes
messages = df.selectExpr("CAST(value AS STRING)").select(from_json(col("value"), schema).alias("data"))

# Escribir la salida en consola
query = messages.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()
```
**Paso 3: Enviar Datos al Pipeline**
• Usa el productor de Kafka para enviar mensajes.
```
bin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092
```
Escribe mensajes como:
{"key": "sensor1", "value": "temperature:25"}
{"key": "sensor2", "value": "temperature:30"}

• Observa cómo Spark Streaming procesa estos mensajes en la consola.

**Diagrama Arquitectónico: Pipeline de Procesamiento en Tiempo Real**
Data Sources (IoT devices, APIs, etc.)
      ↓
[ Kafka (buffer)]
      ↓
[ Spark Streaming (micro-batching) ]
      ↓
[ Processed Data Stored in HBase / Accumulo / Data Warehouse ]

1. **Kafka:** Actúa como un buffer que recibe y almacena temporalmente los datos.
2. **Spark Streaming:** Procesa los datos de Kafka en micro-lotes y ejecuta lógica de transformación.
3. **HBase/Accumulo:** Persiste los datos procesados para consultas rápidas.

