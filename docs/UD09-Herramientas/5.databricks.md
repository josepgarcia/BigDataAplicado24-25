# Databricks

Fuente:
[https://aitor-medrano.github.io/iabd/spark/spark.html#uso-en-la-nube](https://aitor-medrano.github.io/iabd/spark/spark.html#uso-en-la-nube)

*Databricks* es una plataforma analítica de datos basada en Apache Spark desarrollada por la compañía con el mismo nombre. La empresa, creada en el 2013 por los desarrolladores principales de *Spark*, permite realizar analítica Big Data e Inteligencia Artificial con *Spark* de una forma sencilla y colaborativa.

*Databricks* se integra de forma transparente con *AWS*, *Azure* y *Google Cloud*. En una [entrada del blog de la empresa de noviembre de 2021](https://databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html) anuncian un nuevo record de procesamiento que implica que su rendimiento es 3 veces superior a la competencia y con un coste menor.

Para poder trabajar con *Databricks* de forma gratuita, podemos hacer uso de [Databricks Community Edition](https://community.cloud.databricks.com/login.html), donde podemos crear nuestros propios cuadernos *Jupyter* y trabajar con *Spark* sin necesidad de instalar nada.

Para crear una cuenta gratuita, clickando sobre *Sign up*, tras rellenar los datos personales, antes de seleccionar el proveedor *cloud*, en la parte inferior derecha, hemos de pulsar sobre *Get started with Community Edition* (botón en negro):

## Creación clúster

El único paso inicial tras registrarnos, es crear un clúster básico (con 15.3GB de memoria y dos núcleos) desde la opción *Create* del menú de la izquierda:

![](<./images/databricks.png>)

Tras un par de minutos se habrá creado y lanzado el clúster, ya estaremos listos para crear un nuevo *notebook* y tener acceso a *Spark* directamente desde el objeto `spark`:

![](<./images/databricks1.png>)

Si queremos, podemos hacer público el cuaderno y compartirlo con la comunidad.

ℹ️ **INFO**
Por defecto, el navegador del sistema de archivos de DataBricks está oculto. Para facilitar el acceso a los datos y visualizar la estructura y ruta de los mismos, lo podemos activar desde el menú superior derecho: Admin Settings → Workspace Settings → Advanced, y ponemos la opción DBFS File Browser a Enabled.
## SparkUI en databricks

Para acceder a la herramienta de monitorización en Databricks, una vez creado un clúster, en la opción *Compute*/Calcular podremos seleccionar el clúster creado y en la pestaña *IU de Spark* acceder al mismo interfaz gráfico:

![](<./images/databricks2.png>)
## Trabajando con *Databricks*[¶](https://aitor-medrano.github.io/iabd/spark/dataframeAPI.html#trabajando-con-databricks)

[https://aitor-medrano.github.io/iabd/spark/dataframeAPI.html#trabajando-con-databricks](https://aitor-medrano.github.io/iabd/spark/dataframeAPI.html#trabajando-con-databricks)

Una vez creado de nuevo el cluster, vamos a cargar los datos mediante la opción Data, subiendo el archivo `pdi_sales_small.csv`

![](<./images/databricks3.png>)

Una vez cargado el archivo, pulsamos sobre el botón Create table in notebook de manera que nos crea un cuaderno Jupyter donde podemos consultar los datos y crear una vista temporal:

![](<./images/databricks4.png>)

Para que funcione correctamente con nuestro datos, vamos a modificar el código:

```python
infer_schema = "true"
first_row_is_header = "true"
delimiter = ";"
```

Y tras cargar el *dataset*, antes de crear la vista, vamos a limpiar los países:

```python
from pyspark.sql.functions import trim
df = df.withColumn("Country", trim(df.Country))
```

## Datos visuales[¶](https://aitor-medrano.github.io/iabd/spark/dataframeAPI.html#datos-visuales)

Si volvemos a ejecutar el cuaderno, ahora sí que cargará correctamente los datos. Si nos vamos a la celda que realiza una consulta sobre todos los datos, podemos ver en la parte superior derecha como el lenguaje empleado en la celda es SQL, por ello la primera línea comienza con %sql, y a continuación ya podemos introducir directamente código SQL, teniendo la opción de visualizar los datos tanto en modo texto como mediante gráficos:

![https://aitor-medrano.github.io/iabd/spark/images/02spark-databricks-sql.gif](https://aitor-medrano.github.io/iabd/spark/images/02spark-databricks-sql.gif)

CONTINUAR…..

[https://aitor-medrano.github.io/iabd/spark/dataframeAPI.html#cuadro-de-mandos](https://aitor-medrano.github.io/iabd/spark/dataframeAPI.html#cuadro-de-mandos)

Además, con las tablas y/o gráficos que generamos dentro de *Databricks*, podemos generar un sencillo cuadro de mandos.

Vamos a crear un par de consultas, una para obtener las ventas medias por país:

```python
%sql
select Country, avg(Revenue) as ventas
from pdi_sales_small_csv
group by Country
order by ventas desc
```

Y otra para las unidas pedidas por cada país:

```python
%sql
select Country, sum(Units) as pedidos
from pdi_sales_small_csv
group by Country
order by pedidos desc
```

Si pulsamos sobre el icono del gráfico de barras de la esquina superior derecha de una celda SQL, podemos añadir el resultado de la celda a un *dashboard*:

![https://aitor-medrano.github.io/iabd/spark/images/02spark-databricks-dashboard.gif](https://aitor-medrano.github.io/iabd/spark/images/02spark-databricks-dashboard.gif)

Una vez creado, sólo tenemos que seleccionar las celdas que queramos, e ir añadiéndolas al cuadro de mandos creado. Posteriormente, podemos abrirlo, resituar los elementos y visualizarlo:

![https://aitor-medrano.github.io/iabd/spark/images/02spark-databricks-dashboard2.gif](https://aitor-medrano.github.io/iabd/spark/images/02spark-databricks-dashboard2.gif)